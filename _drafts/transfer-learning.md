---
layout: default
title:  Transfer learning in the context of deep learning
date:   2016-10-25 16:42:25 +0530
categories: deep-learning transfer-learning
---

# Transfer learning in the context of deep learning #

## [CNN Features off-the-shelf: an Astounding Baseline for Recognition][RIT'13] ##

This paper explores the potential of features generated by Convolutional Neural Network by using the features learned by Overfeat model for object classification task in ILSVRC13.  
The features are appllied for various tasks such as *object classification, fine-grained recognition, scene recognition, attribute detection (for example attributes of pedestrians from a cam feed that is if they are wearing a hat, shoe, trouser, jeans etc.) and scene retrieval*.   

### Object recognition ###

Is the task of assigning labels (possibly more than one) to a given image. 
Since the features used are learned over a similar task, better results are expected. 
They tried on PASCAL VOM and MIT indoor dataset both of which are considered harder than ImageNet.  
The mean Average Precision(mAP) over all the classes on PASCAL VOM is 77.2, which is 7 point more than second best. 
Thing to note is that, Oquab et.al. fixes all the layers trained on ImageNet and then it adds and optimized two fully-connected layers on VOC dataset with **77.7** mAP. 

### Object Detection ###

This is about recognising and also putting a box around the object.
With off-the-shelf features: **46.2** mAP (Girshick et.al.) and when fine-tuned on PASCAL VOC it is **53.1**.

The graph below summarizes the contribution of this paper.

![Comparison of CNN features with other baselines][feature-transfer]

## [DeCAF][decaf] ##

This work is quite similar to the above one. 
The features are extracted from the [AlexNet][alexnet] which contains 5 convolutional layers (not inluding pooling) followed b7y two fully connnected layers.  
Features are extracted from the output of 5th, 6th and 7th layer; notice that 7th is last but one layer and 5th has only convolutional layers. 
AlexNet has dropout in layers 6 and 7. 
Experiments were carried on the tasks of Object detection, fine-grained recognition, domain adaptation and scene recognition.  

### Object Recognition ###

It is found in their experiments that logistic Regression or linear SVM on these features already beat methods that involve task-specific features with SVM using multiple kernels by around 2-3% on mean accuracy.
Things to note, the mean accuracy was around 33% when the training set contains only one example per class. 
*The performance of the system on 7th level features is slightly less (by 2%) when compared to 6th level feature.*
Dropout on layers: 6/7 improves perf. by 0-2%.
This method beats two-layer convolution network trained on only the task-specific training data by 20%. 

## [Frustratingly Easy Domain Adaptation][easyadapt] ##

This is a 2007 paper that propses a simple pre-processing trick for easy adaptation. 
The DeCAF paper reports best results in the task of domain adaptation from Amazon images -> webcam images using the features from AlexNet and this method. 
That interested me to study this further.
Let me start with the simple code.  

```perl
#source: http://hal3.name/easyadapt.pl.gz
for ($i=0; $i<@ARGV; $i++) {
    open F, $ARGV[$i];
    while (<F>) {
        chomp;
        ($y,@x) = split;
        print $y;
        map { print " *$_ $i$_" } @x;
        print "\n";
    }
}
```

Consider the case where there is a loads of data in one domain but is different from the domain that we want to use it in. 
For example, we might want a PoS tagger trained on NewsWire to work on hardware blogs. 
That is, we have data from source domain and some amount of data from target domain; there are sevearal standard ways to adapt in such a case. 
According to the author, these approaches are surprisingly hard to beat.
   
  * SRCOnly: A single model is trained on only data from source data alone ignoring target data.
  * TGTOnly: Model trained only on target data alone
  * ALL: A model is trained on both the source and target domain. 
	Can wash away the effects of target data since it is small relatively.
	For this reason, the source data eaxamples are weighed down by that factor to bring balance. 
	WEIGHT is our next baseline which chooses the scaling factor of source examples by CV.
  * PRED: Use the output of model trained on SRC data as feature for training on target domain.
  * LinINT: Linearly interpolate the predictions of src and target models while the interpolation params are from dev. data.
  
  Two models are found to have beat these: 

[RIT'13]: https://arxiv.org/pdf/1403.6382v3.pdf
[feature-transfer]: /assets/images/feature-transfer13.png
[decaf]: https://arxiv.org/pdf/1310.1531v1.pdf

[alexnet]: https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf "ImageNet classification with deep convolutional neural networks. "

[easyadapt]: http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf "Frustratingly Easy Domain Adaptation"


