<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Literature Survey on Adaptive Learning in the context of neural networks</title>
  <meta name="description" content="Introduction">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/deep-learning/speaker-adaption/adaptive-neural-networks/2017/03/15/NN_adaptation.html">
  <link rel="alternate" type="application/rss+xml" title="Vihari Piratla blog" href="http://localhost:4000/feed.xml">
</head>


  <body>
    <script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">Vihari Piratla blog</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <h1 id="introduction">Introduction</h1>

<p>Adaptation of neural network models in the context of speech, speaker adaptation, is well researched. 
Acoustic models have evolved starting from GMM or HMM models to hybrid models that pushed in to the neural network regime, making way for deep neural networks to LSTM based RNN models.
The problem of low performance due to train and test domain differences is acute in speech and is well addressed in the past even before the introduction of neural networks.
In this post, I will discuss the domain adaptation problem only in the context of neural nets.</p>

<h2 id="speaker-adaptation-of-hybrid-nnhmm-based-on-speaker-codesspeaker-code"><a href="http://ieeexplore.ieee.org/document/6639211/" title="Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code">Speaker Adaptation of hybrid NN/HMM based on speaker codes</a></h2>

<p>In a NN/HMM model, the GMM component that is used to estimate the emission probabilities of each state is replaced by neural network which when fed with the feature vector, outputs the posterior over HMM state labels.</p>

<p><img src="/assets/images/speaker-code-fig1.png" alt="speaker-code-fig1" /></p>

<p>As shown in the image above, the proposed adaptation method relies on learning adaptation NN and speaker codes.
All the layers in adaptation NN are standard fully connected layers.
The transformed feature vector should have the same dimension as the input feature vector.
During the adaptation phase, both the weights of adaptation NN and speaker need to be learned.</p>

<h3 id="training">Training</h3>

<p>The training of speaker independent model and adaptive NN along with speaker code is carried in two separate steps.
Speaker Independent (SI) model is learned as if adapt NN does not exist, that is standard NN-HMM model is trained without using any speaker specific information.<br />
<strong>Once the SI model is trained, its weights are freezed and speaker code and adaptNN weights are learned jointly with back-propagation to optimize frame-wise classification performance.</strong><br />
Acknowledges that there are several other plausible ways of training the network, but do not provide any rationale as to why this method was choosen as such.</p>
<ul>
  <li>it is possible to tweak the weights of SI model when optimizing for speaker codes and adaptNN weights.</li>
  <li>Learn all the three parameters jointly. “However, this may result in two inseparable NNs and they eventually become one large deep NN with only a number of lower layers receiving a speaker code.”</li>
  <li>Another possibility is to learn SI model over the features transformed with an already trained adaptNN, that is to flip the order in which we train.</li>
</ul>

<h3 id="adaptation">Adaptation</h3>

<p>During this phase, only the speaker code is to be learned for the new speaker over the small amount of data available for adaptation.
This is one of the strong points of this work that the speaker code can be arbitrarily made small or large depending on how much data is available for adaptation.
We do the same, BP, but adaptNN weights are freezed as well.</p>

<h3 id="interesting-experiments">Interesting experiments</h3>

<p><img src="/assets/images/speaker-code-fig2.png" alt="fig2" /></p>

<p>The figure above shows the effect of number of examples used for estimating speaker code on performance.<br />
The experiment was conducted on a 462 speaker training set and 24-speaker test set.
The test set each contain eight utterances per user.
The learning rate, context window are al fixed, hidden layer width (1000), speaker code size (50), 183 target class labels and feature vector dimension (??) are all fixed.</p>

<p>Note</p>
<ul>
  <li>“Dummy” is when no speaker code, but only dummy layers – does not affect the performance meaning that the perf. improvement is not just increased model complexity.</li>
  <li>using zero adaptation has some positve effect.</li>
  <li>Even when exposed to one utterance, the perf. improvement is not bad.</li>
</ul>

<h2 id="using-i-vector-inputs-to-improve-speaker-independence"><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853591" title="IMPROVING DNN SPEAKER INDEPENDENCE WITH I-VECTOR INPUTS">Using I-Vector inputs to improve speaker independence</a></h2>

<p>Leveraging utterance-level features as inputs to DNN to facilitate speaker, channel and background normalization.</p>

<h3 id="i-vectors-or-identity-vectors">i-Vectors or identity vectors</h3>

<p><strong>“i-vectors encode precisely those effects to which we want our ASR system to be invariant: speaker, channel and background noise.”</strong>
These vectors are generally used in speaker recognition and verification</p>

<h3 id="adapting-with-i-vectors">Adapting with i-vectors</h3>

<p><img src="/assets/images/google-ivector-fig1.png" alt="google-ivec-fig1" /></p>

<p>As shown in the image above, the idea is to provide the input with characterisation of the speaker, which could enable it to normalise the signal with respect to speaker specific nuances and thus leading to a better Speaker Independent model.</p>

<h3 id="experiments">Experiments</h3>

<p>The training and dev set performance differed when the input is compounded with the 300 dimensional i-vector. 
This could mean that the network is over-fitting the i-vectors or it could also be that the computing a 300-dim vector from short utterances is not relaiable.</p>

<p>Reducing the i-vector dimension, to say 20, along with l2 regularization helped.</p>

<p>The dataset contains 80 speakers with an equivalent of 10 minutes of utterance per user.
The input augmentation with 20-dimensional i-vector model along with re-training on the adaptation set with l2 reg. coeff of 0.01 improved the results further.</p>

<p><img src="/assets/images/ivec-adapt-results.png" alt="ivec-adapt-results" /></p>

<p>This work claims that when the network with input augmented with i-vectors is also adapted over the user-data, it can lead to better performance as shown in the figure above.
It is not mentioned if any weights are fixed while adapting, it is probably that none of the weights are fixed and are all jointly optimized over various passes on the user data.
The baseline model is a normal feed-forward neural network.</p>

<h2 id="speaker-adaptive-deep-neural-networks"><a href="https://www.cs.cmu.edu/~ymiao/pub/tasl_sat.pdf" title="Towards Speaker Adaptive Training of Deep Neural Network Acoustic Models">Speaker adaptive deep neural networks</a></h2>

<p>Two different model architectures are tried with: AdaptNN and iVecNN.</p>

<p><img src="/assets/images/ivecNN.png" alt="models" /></p>

<p>iVecNN works by producing a linear feature shift which is added to the original feature vector and is activated with a linear activation function.<br />
<script type="math/tex">a_t = o_t+f(i_s)</script><br />
The weights of iVecNN are estimated using BP while the weights of the initial DNN are estimated and fixed.</p>

<p>The strong point of this method is its relevance to CNNs.</p>

<p>In the figure above, <script type="math/tex">z_t</script> is the element-wise sum of <script type="math/tex">o_t</script> and <script type="math/tex">y_s^{-1}</script>.
For two speakers in the training set, two principal components from PCA are plotted as shown in the image below. 
Observe that the non-overlapping regions has shrunk for the case of <script type="math/tex">z_t</script> when compared to <script type="math/tex">o_t</script> implying that adding a linear shift to the original vector is actually making the speaker independent.</p>

<p><img src="/assets/images/speaker-indep-ivecNN.png" alt="speaker independence" /></p>

<p>The training pipeline of the system is shown below.</p>

<p><img src="/assets/images/ivecNN-pipeline.png" alt="pipeline" /></p>

<p>They did show that the model is better than DNN+i-vector, that is the augmented input, the first model in this article, and concluded that this model is better than the other.
However, the experiment was not set right, the i-vectors are not normalized and i-vector size is not varied.
I am not including results because I did not like them. (I have some issues with how the experiment was set-up)</p>

<h1 id="adaptation-in-the-context-of-hand-writing-recognition">Adaptation in the context of hand writing recognition</h1>

<p>I have not come across any work that adapts hand writing recognition to a user. 
There is some interest in recognizing what is called as unconstrained hand writing recognition task which is recognizing hand written characters with no restrictions imposed on their style, size, position and medium.</p>

<h2 id="generating-sequences-with-recurrent-neural-networksgen-hw"><a href="https://arxiv.org/pdf/1308.0850v5.pdf" title="Generating Sequences With Recurrent Neural Networks">Generating Sequences With Recurrent Neural Networks</a></h2>

<p>This work is an interesting read, although it does not explicitly make user modeling.
The model can generate hand writen sentences that resemble the ones written by human.
Also interesting is that it is possible to tune the style of the generated sentence instead of randomly choosing one.
This work demonstrates that it is possible to generate such writings one point at a time with RNNs that are also consistent with a style.</p>

<p>Unlike others, no pre-processing of the data (online data) is made.
According to them, pre-processing will normalize and removes variance in the input and will lead an output that is more synthetic.</p>

<h2 id="a-novel-connectionist-system-for-unconstrained-handwriting-recognitiongarves-09"><a href="http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531750" title="A Novel Connectionist System for Unconstrained Handwriting Recognition">A Novel Connectionist System for Unconstrained Handwriting Recognition</a></h2>

<h2 id="meta-learning-with-memory-augmented-neural-networksmemaug-one-shot"><a href="http://jmlr.org/proceedings/papers/v48/santoro16.pdf" title="Meta-Learning with Memory-Augmented Neural Networks">Meta-Learning with Memory-Augmented Neural Networks</a></h2>
<p>Given a small amopunt of data, a straightforward gradient based solution is to completely relearn the parameters from the data available, which can lead to poor learning due to interference.
One-shot learning is quite hard because of the interference effects due to the training params learned over much larger data.
<code class="highlighter-rouge">Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models.</code></p>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">Vihari Piratla blog</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>Vihari Piratla blog</li>
          <li><a href="mailto:viharipiratla[at]GMAIL">viharipiratla[at]GMAIL</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/vihari"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">vihari</span></a>

          </li>
          

          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>By a researcher in computer science.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
