<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Transfer learning in the context of deep learning</title>
  <meta name="description" content="Transfer learning in the context of deep learning">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/deep-learning/transfer-learning/2016/10/25/transfer-learning.html">
  <link rel="alternate" type="application/rss+xml" title="My notes" href="http://localhost:4000/feed.xml">
</head>


  <body>
    <script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">My notes</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <h1 id="transfer-learning-in-the-context-of-deep-learning">Transfer learning in the context of deep learning</h1>

<h2 id="cnn-features-off-the-shelf-an-astounding-baseline-for-recognitionrit13"><a href="https://arxiv.org/pdf/1403.6382v3.pdf">CNN Features off-the-shelf: an Astounding Baseline for Recognition</a></h2>

<p>This paper explores the potential of features generated by Convolutional Neural Network by using the features learned by Overfeat model for object classification task in ILSVRC13.<br />
The features are appllied for various tasks such as <em>object classification, fine-grained recognition, scene recognition, attribute detection (for example attributes of pedestrians from a cam feed that is if they are wearing a hat, shoe, trouser, jeans etc.) and scene retrieval</em>.</p>

<h3 id="object-recognition">Object recognition</h3>

<p>Is the task of assigning labels (possibly more than one) to a given image. 
Since the features used are learned over a similar task, better results are expected. 
They tried on PASCAL VOM and MIT indoor dataset both of which are considered harder than ImageNet.<br />
The mean Average Precision(mAP) over all the classes on PASCAL VOM is 77.2, which is 7 point more than second best. 
Thing to note is that, Oquab et.al. fixes all the layers trained on ImageNet and then it adds and optimized two fully-connected layers on VOC dataset with <strong>77.7</strong> mAP.</p>

<h3 id="object-detection">Object Detection</h3>

<p>This is about recognising and also putting a box around the object.
With off-the-shelf features: <strong>46.2</strong> mAP (Girshick et.al.) and when fine-tuned on PASCAL VOC it is <strong>53.1</strong>.</p>

<p>The graph below summarizes the contribution of this paper.</p>

<p><img src="/assets/images/feature-transfer13.png" alt="Comparison of CNN features with other baselines" /></p>

<h2 id="decafdecaf"><a href="https://arxiv.org/pdf/1310.1531v1.pdf">DeCAF</a></h2>

<p>This work is quite similar to the above one. 
The features are extracted from the <a href="https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf" title="ImageNet classification with deep convolutional neural networks. ">AlexNet</a> which contains 5 convolutional layers (not inluding pooling) followed b7y two fully connnected layers.<br />
Features are extracted from the output of 5th, 6th and 7th layer; notice that 7th is last but one layer and 5th has only convolutional layers. 
AlexNet has dropout in layers 6 and 7. 
Experiments were carried on the tasks of Object detection, fine-grained recognition, domain adaptation and scene recognition.</p>

<h3 id="object-recognition-1">Object Recognition</h3>

<p>It is found in their experiments that logistic Regression or linear SVM on these features already beat methods that involve task-specific features with SVM using multiple kernels by around 2-3% on mean accuracy.
Things to note, the mean accuracy was around 33% when the training set contains only one example per class. 
<em>The performance of the system on 7th level features is slightly less (by 2%) when compared to 6th level feature.</em>
Dropout on layers: 6/7 improves perf. by 0-2%.
This method beats two-layer convolution network trained on only the task-specific training data by 20%.</p>

<h3 id="domain-adaptation">Domain Adaptation</h3>

<p>This entire work is a kind of domain adapatation. 
This section explores domain apaptability of featutures extracted from Conv. layers.
In this task, the features are compared over their perf. on multi-class accuracy on ffice dataset collected from Amamzon, webcam and camera snapshots.
The ConvNet features beat SURF features by a large margin which means that this hand-engineered feature do not represent all the knowledge required to classify as well as convNet layers. 
The fact that logistic regression over source, target or combined data did not perform well and Daume III (next section) and SVM did well means that there are several features that are specific to a certain domain and feature reweightinghelps to increase the performance from <strong>75.30</strong> for best performing log. regr. model to Daume III.</p>

<p>The table below is an interesting read. 
Observe that the in the case of Dslr<script type="math/tex">\rightarrow</script>webcam, the performance of log. regression model when trained on source data is better than when trained on target data. 
This indicates that the domains are not very different which is understandable since they only differ in resolution. 
Due to this, Daume III did not do any better than logistic regression on source and target data combined.</p>

<p><img src="/assets/images/decaf-domain-adapt.png" alt="Domain Adaptation with AlexNet features" /></p>

<h2 id="how-transferable-are-features-in-deep-neural-networkstransfer-bengio-14"><a href="https://arxiv.org/abs/1411.1792">How transferable are features in deep neural networks?</a></h2>

<p>Unlike other papers that measure transfer of features from ImageNet to various image-processing task, this work takes a different approach. 
Transfer learning is tested on mutually exludive splits of ImageNet data containing 1000 classes. 
The 1000 class collection is either split randomly that is both the splits still contain 1000 classes but the examples in each class will be shared equally between them.</p>

<p>In a different experiment, the data is split to form two sets that are very different from other.
The 1000 classes are split into almost equal sets based on if it is man-made or natural, we will refer to this split as non-random split.</p>

<p>The effect of coadaptation when using features from different layers is isolated, thet is when we are using features from say a 4th or 5th layer in a seven layer netweork then, the activations of this layer might have co-adapted with the next layer.</p>

<h2 id="frustratingly-easy-domain-adaptationeasyadapt"><a href="http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf" title="Frustratingly Easy Domain Adaptation">Frustratingly Easy Domain Adaptation</a></h2>

<p>This is a 2007 paper that propses a simple pre-processing trick for easy adaptation. 
The DeCAF paper reports best results in the task of domain adaptation from Amazon images -&gt; webcam images using the features from AlexNet and this method. 
That interested me to study this further.
Let me start with the simple code.</p>

<div class="language-perl highlighter-rouge"><pre class="highlight"><code><span class="c1">#source: http://hal3.name/easyadapt.pl.gz</span>
<span class="k">for</span> <span class="p">(</span><span class="nv">$i</span><span class="o">=</span><span class="mi">0</span><span class="p">;</span> <span class="nv">$i</span><span class="o">&lt;</span><span class="nv">@ARGV</span><span class="p">;</span> <span class="nv">$i</span><span class="o">++</span><span class="p">)</span> <span class="p">{</span>
    <span class="nb">open</span> <span class="nv">F</span><span class="p">,</span> <span class="nv">$ARGV</span><span class="p">[</span><span class="nv">$i</span><span class="p">];</span>
    <span class="k">while</span> <span class="p">(</span><span class="sr">&lt;F&gt;</span><span class="p">)</span> <span class="p">{</span>
        <span class="nb">chomp</span><span class="p">;</span>
        <span class="p">(</span><span class="nv">$y</span><span class="p">,</span><span class="nv">@x</span><span class="p">)</span> <span class="o">=</span> <span class="nb">split</span><span class="p">;</span>
        <span class="k">print</span> <span class="nv">$y</span><span class="p">;</span>
        <span class="nb">map</span> <span class="p">{</span> <span class="k">print</span> <span class="s">" *$_ $i$_"</span> <span class="p">}</span> <span class="nv">@x</span><span class="p">;</span>
        <span class="k">print</span> <span class="s">"\n"</span><span class="p">;</span>
    <span class="p">}</span>
<span class="p">}</span>
</code></pre>
</div>

<p>The snippet above has the following input-output characteristics.</p>
<div class="language-text highlighter-rouge"><pre class="highlight"><code>====File 1====
x a v f
y a c d
===File 2===
x d f
y g d f
===Output===
x *a 0a *v 0v *f 0f
y *a 0a *c 0c *d 0d
x *d 1d *f 1f
y *g 1g *d 1d *f 1f
</code></pre>
</div>
<p>The output needs to be further processed before it is used, more specifically any symbol in input that starts with * is general and starting with ‘%d’ belongs to that number slot in the input.</p>

<p>Consider the case where there is a loads of data in one domain but is different from the domain that we want to use it in. 
For example, we might want a PoS tagger trained on NewsWire to work on hardware blogs. 
That is, we have data from source domain and some amount of data from target domain; there are sevearal standard ways to adapt in such a case. 
According to the author, these approaches are surprisingly hard to beat.</p>

<ul>
  <li>SRCOnly: A single model is trained on only data from source data alone ignoring target data.</li>
  <li>TGTOnly: Model trained only on target data alone</li>
  <li>ALL: A model is trained on both the source and target domain. 
Can wash away the effects of target data since it is small relatively.
For this reason, the source data eaxamples are weighed down by that factor to bring balance. 
WEIGHT is our next baseline which chooses the scaling factor of source examples by CV.</li>
  <li>PRED: Use the output of model trained on SRC data as feature for training on target domain.</li>
  <li>LinINT: Linearly interpolate the predictions of src and target models while the interpolation params are from dev. data.</li>
</ul>

<p>Two models are found to have beat these</p>
<ul>
  <li>PRIOR: train a model on target data with priors from model trained on source data. 
It is about just adding a regularization term like this: <script type="math/tex">\lambda{\lvert\lvert{w-w_s}\rvert\rvert _2}^2</script></li>
  <li>By the very author, key idea is to learn three different models: one for each of source, target and general. 
The idea is very similar to the work presented in this paper, distinction is made for each example: if it is source specific or general or target-specific or general.
The EM algo. presented is quite slow, so this paper is redemtion of sorts.</li>
</ul>

<p>This work presents a simple augmentation of data that transforms source and target data differently with <script type="math/tex">\phi^s(\mathbf{x})</script>=<script type="math/tex">% <![CDATA[
<\mathbf x,\mathbf x,0> %]]></script>, <script type="math/tex">\phi^t(\mathbf{x})</script>=<script type="math/tex">% <![CDATA[
<\mathbf x,0,\mathbf x> %]]></script>.
The intuition is that any by replicating the feature space thrice one for general, source-specific and target-specific, the features that are specific to say target or source will remain in their respective dimensions and common ones in the general dimension.</p>

<p>An interesting takeaway is that things did not go south because of teh increased dimensionality of feature space. 
The methodfailed to perform in cases where the source and target domains did not differ by much.</p>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>My notes</li>
          <li><a href="mailto:viharipiratla[at]GMAIL">viharipiratla[at]GMAIL</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jekyll"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jekyll</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/jekyllrb"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jekyllrb</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
