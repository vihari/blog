<!DOCTYPE html>
<html>

  <head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1">

  <title>Notes from my independent study of deep learning</title>
  <meta name="description" content="Practical aspects of Training">

  <link rel="stylesheet" href="/css/main.css">
  <link rel="canonical" href="http://localhost:4000/jekyll/update/2016/10/25/study.html">
  <link rel="alternate" type="application/rss+xml" title="My notes" href="http://localhost:4000/feed.xml">
</head>


  <body>
    <script type="text/javascript"
	    src="http://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML">
    </script>
    <header class="site-header">

  <div class="wrapper">

    <a class="site-title" href="/">My notes</a>

    <nav class="site-nav">
      <a href="#" class="menu-icon">
        <svg viewBox="0 0 18 15">
          <path fill="#424242" d="M18,1.484c0,0.82-0.665,1.484-1.484,1.484H1.484C0.665,2.969,0,2.304,0,1.484l0,0C0,0.665,0.665,0,1.484,0 h15.031C17.335,0,18,0.665,18,1.484L18,1.484z"/>
          <path fill="#424242" d="M18,7.516C18,8.335,17.335,9,16.516,9H1.484C0.665,9,0,8.335,0,7.516l0,0c0-0.82,0.665-1.484,1.484-1.484 h15.031C17.335,6.031,18,6.696,18,7.516L18,7.516z"/>
          <path fill="#424242" d="M18,13.516C18,14.335,17.335,15,16.516,15H1.484C0.665,15,0,14.335,0,13.516l0,0 c0-0.82,0.665-1.484,1.484-1.484h15.031C17.335,12.031,18,12.696,18,13.516L18,13.516z"/>
        </svg>
      </a>

      <div class="trigger">
        
          
          <a class="page-link" href="/about/">About</a>
          
        
          
        
          
        
          
        
      </div>
    </nav>

  </div>

</header>


    <div class="page-content">
      <div class="wrapper">
        <h2 id="practical-aspects-of-training">Practical aspects of Training</h2>

<ul>
  <li>
    <p><a href="https://arxiv.org/pdf/1206.5533v2.pdf" title="Practical Recommendations for Gradient-Based Training of Deep Architectures">Recommendations by Y. Bengio</a><br />
  The recommendations are not definitive and should be challenged.
  They can work as a good starting point.<br />
  It has been shown that use of computer clusters for hyper-parameter selection can have an important effect on results. 
  The value of sopme hyper-parameters can be selected based on its performance on training data, but most cannot. For any hyper-parameter that affects the capacity of the learner, it makes more sense to use an out-of-sample data to select the params, hence the validation set.<br />
  	&gt; different researchers and research groups do not always agree on the practice of training neural networks</p>

    <ul>
      <li><strong>Initial Learning rate (<script type="math/tex">\epsilon_0</script>)</strong> is the single most important hyp. param. according to him. <code class="highlighter-rouge">If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.</code> Typical values for neural networks is: 1E-6 to 1 that is if the input is properly normalized to lie between 0 and 1. A default value of 0.01 typically works for multi-layer neural network.</li>
      <li>
        <p><strong>Learning rate schedule</strong>: The general strategy is to keep the learning rate constant until some iterations as shown in the equation below. In many cases choosing other than the default value for <script type="math/tex">\tau\rightarrow \infty</script> has very little effect. 
There are suggestions to decrease the learning rate less steeply than linear after <script type="math/tex">\tau</script> time steps that is <script type="math/tex">% <![CDATA[
O(1/t^\alpha), \alpha<1 %]]></script> depending on the convex behaviour of the function being optimized.
Adaptive strategies for learning rate exist.</p>

        <script type="math/tex; mode=display">\epsilon_t=\frac{\epsilon_0 \tau}{max(t,\tau)}</script>
      </li>
      <li><strong>Mini-batch size (B)</strong>: In theory, this hyp. param. should only affect the training time and not performance. Typically, B is a value between 1 and a few hundred. A value of 32 is good because it can take implementational advantages that matrix-matrix product are more efficient than matrix-vector and is not too high to require a long training time. 
B can be optimized independently of others. Once a value is choosen, it can be fixed and is better to re-optimize in the end since it weakly interacts with other hyp. params.</li>
      <li><strong>Number of Training iterations (T)</strong>: To be choosen by early-stopping criteria. During the analysis stage where we try and compare different models, stopping early can have evening out effect. That is we cannot make-out between an over-fitting or under-fitting model. For this reason, it is best to turn it off during analysis. Another param called <em>patience</em> is generally defined that tells how long the model should wait after it observed the minimum validation error, the param is defined in terms of minimum number of examples to be seen before stopping.</li>
      <li><strong>Momenum (<script type="math/tex">\beta</script>)</strong>: <script type="math/tex">\hat{g}\leftarrow (1-\beta)\hat{g}+\beta g</script> where <script type="math/tex">\hat{g}</script> is the smoothed gradient. The default value of <script type="math/tex">\beta=1</script> works for most cases, but is found to help in some cases of unsupervised learning. “The idea is that it removes some of the noise and oscillations that gradient descent has, in particular in the directions of high curvature of the loss function”
 In some rare cases, layer-specific hyper paramater optimization is employed. This makes sense when the number of hidden units vary large between layers. Generally employed in layer-wise unsupervised pre-training.</li>
    </ul>

    <p>The parameters discussed above are related to SGD (Stochastic Gradient Descent), what follows are the params related to the neural network.</p>

    <ul>
      <li><strong>Number of hidden units <script type="math/tex">n_h</script></strong> Because of early stopping and possiblty other regularizers such as weight decay, it is mostly important to choose <script type="math/tex">n_h</script> large enough. In general, it is observed that seeting all the layers to equal number of units works better or the same as when setting the hidden unit widths in decreasing or increasing order (pyramidal or reverse pyramidal), but this could be data-dependent (<a href="http://deeplearning.cs.cmu.edu/pdfs/1111/jmlr10_larochelle.pdf" title="Exploring Strategies for Training Deep Neural Networks">Larochelle et.al. 2014</a>). 
 An over-complete (larger than the input vector) is better than an under-complete one.
 A more validated observation is that optimal <script type="math/tex">n_h</script> value when training with unsupervised pre-training in a supervised neural network. Typically from 100 to 1000. This could be because unsupervised pre-training could hold lot more information that is not relevant to the task and hence require large hidden layers to make sure relevant information is captured.</li>
      <li><strong>Weight decay:</strong> This article makes a very inetersting note about thsi parameter. As we know, weight decay is used to avoid over-fitting by limiting capacity of the learner. L2 or L1 regularization correspond to the penalties: <script type="math/tex">\lambda \sum_i {\theta_i}^2</script> and <script type="math/tex">\lambda \sum_i{\theta_i}</script>, both the terms can be included. In the case of batch-wise handling of data, the param <script type="math/tex">\lambda</script> that we optimize is actually <script type="math/tex">\frac{\lambda'*B}{T}</script> where B is batch size and T is the size of training data.
L2 regularization corresponds to a guassian prior (over weights) <script type="math/tex">\propto exp^{\frac{-1 \theta^2}{2 \sigma^2}}</script> with <script type="math/tex">\sigma^2=1/2\lambda</script>.
<em>Note that there is a connection between L2 regularization and early stopping with one basically playing the same role as other</em>. L1 regularization is different and sometimes act as feature selectors by making sure the parameters that are not really very useful go to 0. L1 corresponds to laplace density prior <script type="math/tex">\propto e^{-\frac{|\theta|}{s}}</script> with <script type="math/tex">s=\frac{1}{\lambda}</script><br />
 It is sufficient to regularize just the output weights in order to constrain the capacity. (we use the input weights and output weights to denote weights corresponding to the first and last layer. The input weights is also often referred to as <em>filters</em> because of analogies with signal processing techniques)<br />
 Using an L1 regularizer helps to make the input filters cleaner and easier to interpret. 
 We may draw that L1 cleans the input weights and L2 the output weights. 
 When we introduce both the penalties into our optimization, then it is required to tune the coeffs for L1 and L2 independently. In particluar, input and output weights are treated different.<br />
 <em>In the limit case of the number of hidden layers going to infinity, L2 regularization corresponds to SVMs and L1 to Boosting (<a href="https://papers.nips.cc/paper/2800-convex-neural-networks.pdf" title="Convex Neural Networks">Bengio et.al. 2006</a>)</em><br />
 One of the reason why we cannot rely only on early stopping criteria and treat input and output weights differently from hidden units is because they may be sparse. For example, some input features could be 0 more frequently and others non-zero more frequently. A similar situation may arise when target variable is sparse i.e. trying to predict a rare event. In both cases, the effective number of meaningful update (active feature or rare event) seen by these params is less than the actual number of updates. The parameters (weights outgoing from the corresponding input) of such sparse examples should be more regularized, that is to scale the reg. coeff. of these params by one over effective number of updated seen by the parameter. This presents an alteranaate way to deal with imbalanced data or anamolies(?).</li>
      <li>There are several approaches that aim to minimize the sparsity of hidden layers (note that sparsity is very different from L1 norm)</li>
      <li><strong>Non-linear activation functions</strong>: Popular choices are: sigmoid <script type="math/tex">\frac{1}{1+e^{-a}}</script>, the hyperbolic tangent <script type="math/tex">\frac{e^a-e^{-a}}{e^a+e^{-a}}</script>, rectifier max; max(0,a) and hard tanh.<br />
Sigmoid was shown to yield serious optimization difficulties when used as the top hidden layer of a deep supervised network without unsupervised pre-training. 
<code class="highlighter-rouge">For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error. For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error</code> 
The general trick is to use linear output and squared error for Gaussian output model, cross-entropy and sigmoid output for binomial output model and log output[target class] with softmax outputs to correspond to multinomial output variables (that is take softmax for over all the neuron outpus and score by considering only target label required).</li>
      <li><strong>Weights initiazation and scaling coefficient</strong>: The weights should be initialized randomly inorder to break symmetry and bias can be initialized to 0. 
If not all the neurons initially will produce the same output and hence receive the same gradient, wasting the capacity.
The scaling factor controls how small or big the initial weights are. Units with large input (fan-in of the unit) should have smaller weights (I have first-hand experience with problems that arise when this is not done with one of my NN assignments. The initialization that worked smaller number of hidden units just over-shooted due to explosive gradients. That is because the output diverged to a large value when I did this)<br />
The recommendation made is either to sample <em>Uniform(-r,r)</em>. 
Where r is <script type="math/tex">\sqrt{\frac{6}{fan-in+fan-out}}</script> for hyperbolic tangent units and  <script type="math/tex">4*\sqrt{\frac{6}{fan-in+fan-out}}</script>. fan-in and fan-out are the input and output dimension of a hidden layer.</li>
    </ul>
  </li>
</ul>

<p>General advice on finding the best model.<br />
  Numerical hyper-parameters need to be grid-searched in order to find one. 
  It is not sufficient to conclude the best value based on comparison with less than 5 other values. <br />
  Scale of values considered is often an important decision to make, this is the starting interval in which the values will be looked up. 
  It makes more sense to sample values uniformly in the log space of such interval than to blindly evaluate at every value because the perf. at say 0.01 and 0.011 is likley to remain the same.<br />
  Strategies for hyper-param selection: Coordinate descent and Multi-resolution search.
  Coordinate descent: Make changes to the each hyper-param one at a time, find the best value for the param and move on to the next one. 
  Mult-resolution search: There is no point in fine-tuning or high-resolution search over large intervals. Do a low-resolution search over several settings and then high-res search over best configurations.</p>

<h2 id="limitations-of-deep-learning">Limitations of Deep Learning</h2>
<ul>
  <li><a href="https://arxiv.org/pdf/1412.1897.pdf">DNNs are easy to fool</a> Surprisingly, DNNs can be easily fooled by adding adverse perturbations to an image. For example, adding noise that is imperceptable to humans to an image that looks like <em>panda</em> and recognized as one with confidence of ~56% will lead to an image that is wrongly labeled but with very high confidence. This paper details about very interesting case-study of where the state-of-art AlexNet utterly fails. For code and images <a href="http://www.evolvingai.org/fooling" title="Deep neural networks are easily fooled: High confidence predictions for unrecognizable images ">see</a>. There has also been some effort at explaining this phenomena. In the paper: <a href="https://arxiv.org/pdf/1412.6572v3.pdf" title="Explaining and Harnessing Adversial Examples">Explaining and Harnessing Adversial Examples</a> by Ian Goodfellow et. al., they make a case that such a thing happens majorly because the DNNs are linear in nature.</li>
</ul>

<h2 id="wisdom-from-random-sources">Wisdom from random sources</h2>

<h3 id="hinton-at-stanfordgeoffrey-stanford"><a href="https://www.youtube.com/watch?v=VIRCybGgHts" title="Can the Brain do back-propagation?">Hinton at Stanford</a></h3>

<ul>
  <li>Big data is good (something that frequentist statisticians suggest)
    <ul>
      <li>For any given size of model, its better to have more data</li>
      <li>But it’s a bad idea to try to make the data look big by making the model small</li>
    </ul>
  </li>
  <li>Big models are good (Something that statisticians do not believe but true)
    <ul>
      <li>For any given size of data, the bigger the model, the better it generalized, provided you regularize it well.</li>
      <li>This is obviously true id your model is an ensemble of smaller models. Adding extra models to the ensemble alwqays helps.</li>
      <li>It’s a <strong>good idea</strong> to try to make the data look small by using a big model.</li>
    </ul>
  </li>
  <li>Dropout enables the all the models in an ensemble to share knowledge. If there is only one layer in the network with with H, then the model with dropout is an ensemble of 2^H models and softmax over the output layer is geometric mean of all the models in ensemble.</li>
  <li>Dropout can be seen as bernoulli noise, we do not change the expected value because a neuron either emits zero or twice the value. It is noted that any other kind of noise can work just as well. Gaussian noise and Possion noise are tested to give same performance if not better. In these cases a multiplicative noise with standard deviation equal to the activity. The point is that neurons do not share real values but spikes and that is a lot better than trhe actuaol values.</li>
  <li>In this lecture, Hinton goes on lengths elaborating why brains cannot do exact back-propagation and explains possible other ways in which it could be learning the weights. He argues that neurons in a feed-back loop can do away with the need to back-propagate by considering the difference between the inout at this instance and previous one (plasticity of brain, Spike-time dependent plasticity)</li>
</ul>

<h2 id="debugging-in-tensorflow">Debugging in Tensorflow</h2>
<p>Debugging or even understanding a tensorflow code can be a daunting task. 
Here in this section, I will share some tricks of trade.</p>

<h3 id="a-practical-guide-to-debugging-tensorflow-codestf-debug"><a href="https://wookayin.github.io/TensorflowKR-2016-talk-debugging" title="A practical guide to debugging tensorflow codes">A practical guide to debugging tensorflow codes</a></h3>
<ul>
  <li>Use <code class="highlighter-rouge">Session.run()</code> on tensorflow variables.</li>
  <li>Use off-the-shelf tensorboard to compute the variable summaries.</li>
  <li><code class="highlighter-rouge">tf.print()</code>: some control can be execrised through the paramaters of this method such as <code class="highlighter-rouge">first_n</code>, number of lines to print before breaking, message and summarizer. Behaves like an identity operation with the side-effect of printing.</li>
  <li><code class="highlighter-rouge">tf.Assert()</code> is also on the same lines, print when a condition is met. In the end, this need to be evaluated with <code class="highlighter-rouge">session.run()</code>. One trick is to add all asserts to a collection with: <code class="highlighter-rouge">tf.add_to_collection('Asserts', tf.Assert(...))</code> and in the end: <code class="highlighter-rouge">assert_op = tf.group(*tf.get_collections('Asserts'))</code> and <code class="highlighter-rouge">session.run(assert_op)</code>.</li>
  <li>How about python debuggers: <code class="highlighter-rouge">pdb</code>, <code class="highlighter-rouge">ipdb</code>, <code class="highlighter-rouge">pudb</code>.</li>
</ul>

<p>Some usefule Tensorflow APIs:</p>
<ul>
  <li><code class="highlighter-rouge">tf.get_default_graph()</code> Get the current (default) graph</li>
  <li><code class="highlighter-rouge">G.get_operations, G.get_operations_by_name(name),G.get_tensor_by_name(name), tf.get_collection(tf.GraphKeys.~~)</code> gets all operations (sometimes by name) or tensors or collection</li>
  <li><code class="highlighter-rouge">tf.trainable_variables()</code> list all the trainable variables and likewise <code class="highlighter-rouge">tf.global_variables()</code></li>
  <li><code class="highlighter-rouge">[t = tf.verify_tensor_all_finite(t, msg)][tf-doc-controlflow]</code>, <code class="highlighter-rouge">tf.add_check_numerics_ops()</code></li>
  <li>Naming all the tensors and ops can help in pointing where the problem is when looking at stacktrace.</li>
</ul>

<p><strong>Advanced</strong></p>
<ul>
  <li>It is possible to interpose any python code in computation graph by wrapping a tensorflow operation around the function with: <code class="highlighter-rouge">tf.py_func()</code>. 
Consider the following example (https://wookayin.github.io/TensorflowKR-2016-talk-debugging/#57):</li>
  <li>https://wookayin.github.io/TensorflowKR-2016-talk-debugging/#75</li>
</ul>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="k">def</span> <span class="nf">multilayer_perceptron</span><span class="p">(</span><span class="n">x</span><span class="p">):</span>
    <span class="n">fc1</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s">'fc1'</span><span class="p">)</span>
    <span class="n">fc2</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">fc1</span><span class="p">,</span> <span class="mi">256</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="n">tf</span><span class="o">.</span><span class="n">nn</span><span class="o">.</span><span class="n">relu</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s">'fc2'</span><span class="p">)</span>
    <span class="n">out</span> <span class="o">=</span> <span class="n">layers</span><span class="o">.</span><span class="n">fully_connected</span><span class="p">(</span><span class="n">fc2</span><span class="p">,</span> <span class="mi">10</span><span class="p">,</span> <span class="n">activation_fn</span><span class="o">=</span><span class="bp">None</span><span class="p">,</span> <span class="n">scope</span><span class="o">=</span><span class="s">'out'</span><span class="p">)</span>
    <span class="k">def</span> <span class="nf">_debug_print_func</span><span class="p">(</span><span class="n">fc1_val</span><span class="p">,</span> <span class="n">fc2_val</span><span class="p">):</span>
        <span class="k">print</span> <span class="s">'FC1 : {}, FC2 : {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fc1_val</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">fc2_val</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">print</span> <span class="s">'min, max of FC2 = {}, {}'</span><span class="o">.</span><span class="n">format</span><span class="p">(</span><span class="n">fc2_val</span><span class="o">.</span><span class="nb">min</span><span class="p">(),</span> <span class="n">fc2_val</span><span class="o">.</span><span class="nb">max</span><span class="p">())</span>
        <span class="k">return</span> <span class="bp">False</span>
    <span class="n">debug_print_op</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">py_func</span><span class="p">(</span><span class="n">_debug_print_func</span><span class="p">,</span> <span class="p">[</span><span class="n">fc1</span><span class="p">,</span> <span class="n">fc2</span><span class="p">],</span> <span class="p">[</span><span class="n">tf</span><span class="o">.</span><span class="nb">bool</span><span class="p">])</span>
    <span class="k">with</span> <span class="n">tf</span><span class="o">.</span><span class="n">control_dependencies</span><span class="p">(</span><span class="n">debug_print_op</span><span class="p">):</span>
        <span class="n">out</span> <span class="o">=</span> <span class="n">tf</span><span class="o">.</span><span class="n">identity</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">name</span><span class="o">=</span><span class="s">'out'</span><span class="p">)</span>
    <span class="k">return</span> <span class="n">out</span>
</code></pre>
</div>
<ul>
  <li>The <a href="https://github.com/ericjang/tdb">tdb</a> library</li>
  <li><code class="highlighter-rouge">tfdbg</code>: the official debugger from TF. [Dec. 2016]</li>
</ul>

<h2 id="tfdbgtfdbg"><a href="https://www.tensorflow.org/versions/master/how_tos/debugger/" title="TensorFlow Debugger (tfdbg) Command-Line-Interface Tutorial: MNIST">tfdbg</a></h2>
<p>The debugger from tensorflow can be handy.</p>

<p>It can be used to see the values of the nodes in the computation graph at runtime.
It can also be used to quicly figure out where a bug is: such as a variable not initialized, inf or nan values or shape mismatch in matrix multiplication with pre-fedined filters: <code class="highlighter-rouge">uninitialized_variable</code>, <code class="highlighter-rouge">has_inf_or_nan</code>, <code class="highlighter-rouge">shape_filter</code> respectively.</p>

<p>It is possible to wrap the tensorflow session to debug along with filters to quicly figure where things are going wrong.
It works a gdb session and the interactive session supports: mouse clicks,  which can be turned off with <code class="highlighter-rouge">mouse off</code>, history upon up-arrow and tab completions.</p>

<p>Include the following linesr to enable interactive debugging (the tool is more helpful if all the ops and variables are properly named)</p>

<div class="language-python highlighter-rouge"><pre class="highlight"><code><span class="kn">from</span> <span class="nn">tensorflow.python</span> <span class="kn">import</span> <span class="n">debug</span> <span class="k">as</span> <span class="n">tf_debug</span>

<span class="k">if</span> <span class="n">FLAGS</span><span class="o">.</span><span class="n">debug</span><span class="p">:</span>
        <span class="n">session</span> <span class="o">=</span> <span class="n">tf_debug</span><span class="o">.</span><span class="n">LocalCLIDebugWrapperSession</span><span class="p">(</span><span class="n">session</span><span class="p">)</span>
        <span class="n">session</span><span class="o">.</span><span class="n">add_tensor_filter</span><span class="p">(</span><span class="s">"has_inf_or_nan"</span><span class="p">,</span> <span class="n">tf_debug</span><span class="o">.</span><span class="n">has_inf_or_nan</span><span class="p">)</span>
</code></pre>
</div>

<p>At the time of writing this, the module is experimental and buggy. 
For example, it failed on BasicLSTMCell with the error: <code class="highlighter-rouge">AttributeError: 'LSTMStateTuple' object has no attribute 'name'</code>.
I don’t get it completely, but it looks like it is trying to find name attribute of LSTMCell which is not defined by default.</p>


      </div>
    </div>

    <footer class="site-footer">

  <div class="wrapper">

    <h2 class="footer-heading">My notes</h2>

    <div class="footer-col-wrapper">
      <div class="footer-col footer-col-1">
        <ul class="contact-list">
          <li>My notes</li>
          <li><a href="mailto:viharipiratla[at]GMAIL">viharipiratla[at]GMAIL</a></li>
        </ul>
      </div>

      <div class="footer-col footer-col-2">
        <ul class="social-media-list">
          
          <li>
            <a href="https://github.com/jekyll"><span class="icon icon--github"><svg viewBox="0 0 16 16"><path fill="#828282" d="M7.999,0.431c-4.285,0-7.76,3.474-7.76,7.761 c0,3.428,2.223,6.337,5.307,7.363c0.388,0.071,0.53-0.168,0.53-0.374c0-0.184-0.007-0.672-0.01-1.32 c-2.159,0.469-2.614-1.04-2.614-1.04c-0.353-0.896-0.862-1.135-0.862-1.135c-0.705-0.481,0.053-0.472,0.053-0.472 c0.779,0.055,1.189,0.8,1.189,0.8c0.692,1.186,1.816,0.843,2.258,0.645c0.071-0.502,0.271-0.843,0.493-1.037 C4.86,11.425,3.049,10.76,3.049,7.786c0-0.847,0.302-1.54,0.799-2.082C3.768,5.507,3.501,4.718,3.924,3.65 c0,0,0.652-0.209,2.134,0.796C6.677,4.273,7.34,4.187,8,4.184c0.659,0.003,1.323,0.089,1.943,0.261 c1.482-1.004,2.132-0.796,2.132-0.796c0.423,1.068,0.157,1.857,0.077,2.054c0.497,0.542,0.798,1.235,0.798,2.082 c0,2.981-1.814,3.637-3.543,3.829c0.279,0.24,0.527,0.713,0.527,1.437c0,1.037-0.01,1.874-0.01,2.129 c0,0.208,0.14,0.449,0.534,0.373c3.081-1.028,5.302-3.935,5.302-7.362C15.76,3.906,12.285,0.431,7.999,0.431z"/></svg>
</span><span class="username">jekyll</span></a>

          </li>
          

          
          <li>
            <a href="https://twitter.com/jekyllrb"><span class="icon icon--twitter"><svg viewBox="0 0 16 16"><path fill="#828282" d="M15.969,3.058c-0.586,0.26-1.217,0.436-1.878,0.515c0.675-0.405,1.194-1.045,1.438-1.809c-0.632,0.375-1.332,0.647-2.076,0.793c-0.596-0.636-1.446-1.033-2.387-1.033c-1.806,0-3.27,1.464-3.27,3.27 c0,0.256,0.029,0.506,0.085,0.745C5.163,5.404,2.753,4.102,1.14,2.124C0.859,2.607,0.698,3.168,0.698,3.767 c0,1.134,0.577,2.135,1.455,2.722C1.616,6.472,1.112,6.325,0.671,6.08c0,0.014,0,0.027,0,0.041c0,1.584,1.127,2.906,2.623,3.206 C3.02,9.402,2.731,9.442,2.433,9.442c-0.211,0-0.416-0.021-0.615-0.059c0.416,1.299,1.624,2.245,3.055,2.271 c-1.119,0.877-2.529,1.4-4.061,1.4c-0.264,0-0.524-0.015-0.78-0.046c1.447,0.928,3.166,1.469,5.013,1.469 c6.015,0,9.304-4.983,9.304-9.304c0-0.142-0.003-0.283-0.009-0.423C14.976,4.29,15.531,3.714,15.969,3.058z"/></svg>
</span><span class="username">jekyllrb</span></a>

          </li>
          
        </ul>
      </div>

      <div class="footer-col footer-col-3">
        <p>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</p>
      </div>
    </div>

  </div>

</footer>


  </body>

</html>
