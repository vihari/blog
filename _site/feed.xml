<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Vihari Piratla's blog</title>
    <description>This is my research blog; that includes my notes and pointers.</description>
    <link>http://localhost:4000/</link>
    <atom:link href="http://localhost:4000/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Sat, 18 Mar 2017 00:15:35 +0530</pubDate>
    <lastBuildDate>Sat, 18 Mar 2017 00:15:35 +0530</lastBuildDate>
    <generator>Jekyll v3.3.0</generator>
    
      <item>
        <title>Summary of two papers from ICLR'17</title>
        <description>&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#introduction&quot; id=&quot;markdown-toc-introduction&quot;&gt;Introduction&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#introduction-continued&quot; id=&quot;markdown-toc-introduction-continued&quot;&gt;Introduction [continued…]&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#tricks-to-avoid-over-fitting&quot; id=&quot;markdown-toc-tricks-to-avoid-over-fitting&quot;&gt;Tricks to avoid over-fitting&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#common-image-classification-datasets&quot; id=&quot;markdown-toc-common-image-classification-datasets&quot;&gt;Common Image classification datasets&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#imagenethttpcsstanfordedupeoplekarpathycnnembedcnnembed1kjpg&quot; id=&quot;markdown-toc-imagenethttpcsstanfordedupeoplekarpathycnnembedcnnembed1kjpg&quot;&gt;&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg&quot;&gt;Imagenet&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#popular-neural-nets-for-image-classification&quot; id=&quot;markdown-toc-popular-neural-nets-for-image-classification&quot;&gt;Popular neural nets for Image Classification&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima-1&quot; id=&quot;markdown-toc-on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima-1&quot;&gt;On Large-Batch Training For Deep Learning: Generalization Gap and Sharp Minima [1]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#why-not-just-use-small-batch-methods&quot; id=&quot;markdown-toc-why-not-just-use-small-batch-methods&quot;&gt;Why not just use Small Batch methods?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#why-is-large-batch-bad&quot; id=&quot;markdown-toc-why-is-large-batch-bad&quot;&gt;Why is Large Batch bad?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#guess&quot; id=&quot;markdown-toc-guess&quot;&gt;Guess&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-are-sharp-and-flat-minimizers&quot; id=&quot;markdown-toc-what-are-sharp-and-flat-minimizers&quot;&gt;What are sharp and flat minimizers?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-are-sharp-and-flat-minimizers-continued&quot; id=&quot;markdown-toc-what-are-sharp-and-flat-minimizers-continued&quot;&gt;What are sharp and flat minimizers? [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#targeted-experiments&quot; id=&quot;markdown-toc-targeted-experiments&quot;&gt;Targeted experiments&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#targeted-experiments-continued&quot; id=&quot;markdown-toc-targeted-experiments-continued&quot;&gt;Targeted experiments [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#targeted-experiments-continued-1&quot; id=&quot;markdown-toc-targeted-experiments-continued-1&quot;&gt;Targeted experiments [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#recognizing-the-problem&quot; id=&quot;markdown-toc-recognizing-the-problem&quot;&gt;Recognizing the problem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#over-fitting-is-not-the-problem&quot; id=&quot;markdown-toc-over-fitting-is-not-the-problem&quot;&gt;Over-fitting is not the problem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-evidence-for-if-the-sharpness-of-minima-is-a-problem&quot; id=&quot;markdown-toc-an-evidence-for-if-the-sharpness-of-minima-is-a-problem&quot;&gt;An evidence for if the sharpness of minima is a problem&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#an-evidence-for-if-the-sharpness-of-minima-is-a-problem-continued&quot; id=&quot;markdown-toc-an-evidence-for-if-the-sharpness-of-minima-is-a-problem-continued&quot;&gt;An evidence for if the sharpness of minima is a problem [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#sharpness-metric&quot; id=&quot;markdown-toc-sharpness-metric&quot;&gt;Sharpness metric&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#what-does-the-metric-say&quot; id=&quot;markdown-toc-what-does-the-metric-say&quot;&gt;What does the metric say?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#how-is-sb-avoiding-this-solution&quot; id=&quot;markdown-toc-how-is-sb-avoiding-this-solution&quot;&gt;How is SB avoiding this solution?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#how-is-sb-avoiding-this-solution-continued&quot; id=&quot;markdown-toc-how-is-sb-avoiding-this-solution-continued&quot;&gt;How is SB avoiding this solution? [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#can-we-patch-the-lb-method&quot; id=&quot;markdown-toc-can-we-patch-the-lb-method&quot;&gt;Can we patch the LB method?&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#does-not-amount-to-reducing-the-sharpness-of-the-solution&quot; id=&quot;markdown-toc-does-not-amount-to-reducing-the-sharpness-of-the-solution&quot;&gt;Does not amount to reducing the sharpness of the solution&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#can-we-patch-the-lb-method-continued&quot; id=&quot;markdown-toc-can-we-patch-the-lb-method-continued&quot;&gt;Can we patch the LB method? [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#whats-next&quot; id=&quot;markdown-toc-whats-next&quot;&gt;What’s next?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#understanding-deep-learning-requires-rethinking-generalization-best-paper-award-iclr-2017-4&quot; id=&quot;markdown-toc-understanding-deep-learning-requires-rethinking-generalization-best-paper-award-iclr-2017-4&quot;&gt;UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION (Best Paper Award ICLR 2017) [4]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#the-big-question&quot; id=&quot;markdown-toc-the-big-question&quot;&gt;The big question&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#randomization-tests&quot; id=&quot;markdown-toc-randomization-tests&quot;&gt;Randomization tests&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#can-the-traditional-approaches-provide-a-generalization-bound&quot; id=&quot;markdown-toc-can-the-traditional-approaches-provide-a-generalization-bound&quot;&gt;Can the traditional approaches provide a generalization bound?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#are-regularizers-responsible-for-generalization&quot; id=&quot;markdown-toc-are-regularizers-responsible-for-generalization&quot;&gt;Are Regularizers responsible for Generalization?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#are-regularizers-responsible-for-generalization-continued&quot; id=&quot;markdown-toc-are-regularizers-responsible-for-generalization-continued&quot;&gt;Are Regularizers responsible for Generalization? [continued…]&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#implicit-regularizers&quot; id=&quot;markdown-toc-implicit-regularizers&quot;&gt;Implicit Regularizers?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#concluding-remarks-about-regularization&quot; id=&quot;markdown-toc-concluding-remarks-about-regularization&quot;&gt;Concluding remarks about regularization&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#expressivity-of-networks&quot; id=&quot;markdown-toc-expressivity-of-networks&quot;&gt;Expressivity of networks&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#is-sgd-responsible-for-generalization&quot; id=&quot;markdown-toc-is-sgd-responsible-for-generalization&quot;&gt;Is SGD responsible for generalization?&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#conclusion&quot; id=&quot;markdown-toc-conclusion&quot;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#a-case-study&quot; id=&quot;markdown-toc-a-case-study&quot;&gt;A Case study&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#alexnet-on-randomized-cifar10&quot; id=&quot;markdown-toc-alexnet-on-randomized-cifar10&quot;&gt;AlexNet on Randomized CIFAR10&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#it-did-memorize-the-dataset&quot; id=&quot;markdown-toc-it-did-memorize-the-dataset&quot;&gt;It did memorize the dataset&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#characteristics-of-the-solution-learned&quot; id=&quot;markdown-toc-characteristics-of-the-solution-learned&quot;&gt;Characteristics of the solution learned&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#model-learned-on-true-data&quot; id=&quot;markdown-toc-model-learned-on-true-data&quot;&gt;Model learned on true data&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#model-learned-on-randomized-pixels&quot; id=&quot;markdown-toc-model-learned-on-randomized-pixels&quot;&gt;Model learned on randomized pixels&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#interesting-remarkscritics-from-the-audience&quot; id=&quot;markdown-toc-interesting-remarkscritics-from-the-audience&quot;&gt;Interesting remarks/critics from the audience&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#references&quot; id=&quot;markdown-toc-references&quot;&gt;References&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#proof-of-theorem-extra-slide&quot; id=&quot;markdown-toc-proof-of-theorem-extra-slide&quot;&gt;Proof of theorem (Extra slide)&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#contact&quot; id=&quot;markdown-toc-contact&quot;&gt;Contact&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;
&lt;p&gt;Things you should know before we proceed&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Gradient descent optimization procedures – weight updates proportional to the gradient. Several optimizers such as Rmsprop, SGD, momentum, ADAM exist. They are all slight variations that generally only affect the convergence rate and can sometimes find a better solution.&lt;/li&gt;
  &lt;li&gt;Optimization parameter: batch size. Some variants of optimization consider the errors on all the data available (full batch learning), some consider a (relatively) small batch (mini-batch) or only one example in each step (stochastic).&lt;/li&gt;
&lt;/ul&gt;

&lt;h4 id=&quot;introduction-continued&quot;&gt;Introduction [continued…]&lt;/h4&gt;

&lt;ul&gt;
  &lt;li&gt;Generalization error: The difference between the error on test and train datasets.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tricks-to-avoid-over-fitting&quot;&gt;Tricks to avoid over-fitting&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Explicit regularization: minimizing &lt;script type=&quot;math/tex&quot;&gt;l_2&lt;/script&gt; or &lt;script type=&quot;math/tex&quot;&gt;l_1&lt;/script&gt; norm on weights. Dropout. Input augmentation for example, adding noise to the input or random transformations over the input like random crops&lt;/li&gt;
  &lt;li&gt;Implicit regularization: batch norm and early stopping&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;For further reference, see these &lt;a href=&quot;http://vision.stanford.edu/teaching/cs231b_spring1415/slides/alexnet_tugce_kyunghee.pdf&quot;&gt;slides&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;Note: We recognize over-fitting when the error on validation set starts increasing.&lt;br /&gt;
We only care about ReLU (Rectified Linear Units) activation function for this material.&lt;/p&gt;

&lt;h2 id=&quot;common-image-classification-datasets&quot;&gt;Common Image classification datasets&lt;/h2&gt;
&lt;p&gt;&lt;a href=&quot;https://www.cs.toronto.edu/~kriz/cifar.html&quot;&gt;CIFAR10&lt;/a&gt;: 60,000 images, 10 classes, 32x32 resolution, 6,000 images per class&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/cifar10.png&quot; alt=&quot;CIFAR10&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;imagenethttpcsstanfordedupeoplekarpathycnnembedcnnembed1kjpg&quot;&gt;&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg&quot;&gt;Imagenet&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;1.2 million images, 1000 classes, ~ 300x300 resolution&lt;/p&gt;

&lt;p&gt;&lt;img alt=&quot;Imagenet&quot; src=&quot;/assets/images/slides/imagenet.jpg&quot; style=&quot;width:500px&quot; /&gt;&lt;/p&gt;

&lt;p&gt;(&lt;a href=&quot;http://cs.stanford.edu/people/karpathy/cnnembed/cnn_embed_1k.jpg&quot;&gt;Image courtesy&lt;/a&gt;)&lt;/p&gt;

&lt;h2 id=&quot;popular-neural-nets-for-image-classification&quot;&gt;Popular neural nets for Image Classification&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;AlexNet&lt;/strong&gt; and &lt;strong&gt;Inception&lt;/strong&gt; are two networks that are designed for performance on the ImageNet.&lt;/p&gt;

&lt;p&gt;AlexNet:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;7 hidden weight layers&lt;/li&gt;
  &lt;li&gt;650K neurons&lt;/li&gt;
  &lt;li&gt;60M parameters&lt;/li&gt;
  &lt;li&gt;630M connections&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Inception is much bigger and deeper.&lt;/p&gt;

&lt;p&gt;For more information, consult this &lt;a href=&quot;https://culurciello.github.io/tech/2016/06/04/nets.html&quot;&gt;blog&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;on-large-batch-training-for-deep-learning-generalization-gap-and-sharp-minima-1&quot;&gt;On Large-Batch Training For Deep Learning: Generalization Gap and Sharp Minima [1]&lt;/h2&gt;
&lt;p&gt;Understand why Small Batch (SB) methods generally find a better solution than Large Batch (LB) ones.&lt;/p&gt;

&lt;h2 id=&quot;why-not-just-use-small-batch-methods&quot;&gt;Why not just use Small Batch methods?&lt;/h2&gt;

&lt;p&gt;Large batches are typically in the order of thousands and batch sizes less than 512 are considered small.&lt;/p&gt;

&lt;p&gt;From Yann LeCun’s “Efficient Backprop”[2]&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;Advantages of Stochastic Learning&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Stochastic learning is usually much faster than batch learning.&lt;/li&gt;
    &lt;li&gt;Stochastic learning also often results in better solutions.&lt;/li&gt;
    &lt;li&gt;Stochastic learning can be used for tracking changes.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Notes:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;It is an established fact that stochastic learning gives a better solution.&lt;/li&gt;
  &lt;li&gt;(The 3rd advantage above:) Stochastic learning can deal with the case when the function being modelled changes with time. When we average over all the data, the changes go undetected. Online learning if “operate properly” can give good approximation results.&lt;/li&gt;
&lt;/ul&gt;

&lt;blockquote&gt;
  &lt;p&gt;Advantages of Batch Learning&lt;/p&gt;

  &lt;ol&gt;
    &lt;li&gt;Conditions of convergence are well understood.&lt;/li&gt;
    &lt;li&gt;Many acceleration techniques (e.g. Conjugate gradient) only operate in batch learning.&lt;/li&gt;
    &lt;li&gt;Theoretical analysis of the weight dynamics and convergence rates are simpler.&lt;/li&gt;
  &lt;/ol&gt;
&lt;/blockquote&gt;

&lt;p&gt;Notes:
The very reason why SGD is useful, noise, also works to the advantage of batch learning.&lt;br /&gt;
The main disadvantage that is emphasized in this paper is the difficulty in parallelizing the SGD update because (1) sequential nature of the updates (2) the update step does not contain enough computation that it reaps the benefit of parallelization.&lt;br /&gt;
By reducing the time per update step, it is possible to reduce the computation time since the number of update steps required in a batch learning is lesser.&lt;/p&gt;

&lt;h2 id=&quot;why-is-large-batch-bad&quot;&gt;Why is Large Batch bad?&lt;/h2&gt;

&lt;p&gt;Although, optimization with large and small batch lead to similar performance on train set, the solution obtained by large batch suffers on the test set.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Possible reasons:
    &lt;ul&gt;
      &lt;li&gt;LB methods over-fit the model; &lt;!-- .element: class=&quot;fragment&quot; --&gt;&lt;/li&gt;
      &lt;li&gt;LB methods are attracted to saddle points; &lt;!-- .element: class=&quot;fragment&quot; --&gt;&lt;/li&gt;
      &lt;li&gt;LB methods lack the explorative properties of SB methods and tend to zoom-in on the minimizer closest to the initial point; &lt;!-- .element: class=&quot;fragment&quot; --&gt;&lt;/li&gt;
      &lt;li&gt;SB and LB methods converge to qualitatively different minimizers with differing generalization properties. &lt;!-- .element: class=&quot;fragment&quot; --&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;The data presented in this paper supports the last two causes. &lt;!-- .element: class=&quot;fragment&quot; --&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;guess&quot;&gt;Guess&lt;/h2&gt;

&lt;p&gt;It is conjectured that the LB methods lack explorative properties and settle at a sharp minimizer which may perform well on training data, but may fail on test data.&lt;/p&gt;

&lt;h2 id=&quot;what-are-sharp-and-flat-minimizers&quot;&gt;What are sharp and flat minimizers?&lt;/h2&gt;

&lt;blockquote&gt;
  &lt;p&gt;A flat minimizer &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt; is one for which the function varies slowly in a relatively large neighborhood of &lt;script type=&quot;math/tex&quot;&gt;\bar{x}&lt;/script&gt;. In contrast, a sharp minimizer &lt;script type=&quot;math/tex&quot;&gt;\hat{x}&lt;/script&gt; is such that the function increases rapidly in a small neighborhood of &lt;script type=&quot;math/tex&quot;&gt;\hat{x }&lt;/script&gt;.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;what-are-sharp-and-flat-minimizers-continued&quot;&gt;What are sharp and flat minimizers? [continued…]&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/sharp_vs_flat_minimizer.png&quot; alt=&quot;Sharp vs Flat minimizer&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;targeted-experiments&quot;&gt;Targeted experiments&lt;/h2&gt;

&lt;p&gt;Six multi-class classification network configurations are considered.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/network_configs.png&quot; alt=&quot;Network configurations&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;targeted-experiments-continued&quot;&gt;Targeted experiments [continued…]&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;For LB methods, 10% of the training data is chosen as the batch size (which means the batch size varied from 5,000 on MNIST and ~72,000 on TIMIT dataset). For SB, the batch size is set to 256 for all the experiments.&lt;/li&gt;
  &lt;li&gt;Experiments with any of the optimizers: ADAM, ADGRAD, SGD, adaQN gave similar results, all the results reported are with ADAM optimizer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;targeted-experiments-continued-1&quot;&gt;Targeted experiments [continued…]&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/dataset_sizes.png&quot; alt=&quot;Dataset sizes&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;recognizing-the-problem&quot;&gt;Recognizing the problem&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/train_and_test_acc.png&quot; alt=&quot;Train and test accuracy&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: 
The numbers in the table are written in “mean+standard deviation” format summarized across five trails.&lt;br /&gt;
Observe that the difference between LB and SB is starker in test accuracy than in train.&lt;/p&gt;

&lt;h2 id=&quot;over-fitting-is-not-the-problem&quot;&gt;Over-fitting is not the problem&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/not_overfitting.png&quot; alt=&quot;Overfitting not a problem&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: In both SB and LB cases, the network is trained so as not to deteriorate on validation data.&lt;/p&gt;

&lt;h2 id=&quot;an-evidence-for-if-the-sharpness-of-minima-is-a-problem&quot;&gt;An evidence for if the sharpness of minima is a problem&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The nature of minima can be visualized with something called parametric 1-D plots.&lt;/li&gt;
  &lt;li&gt;If &lt;script type=&quot;math/tex&quot;&gt;{x_l}^{\ast}&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;{x_s}^{\ast}&lt;/script&gt; are the solutions (weights) corresponding to the large and small batch methods, then the 1-D parametric plots look at the nature of all solutions in 
&lt;script type=&quot;math/tex&quot;&gt;\alpha\*{x_l}^{\ast}+(1-\alpha)\*{x_s}^{\ast}&lt;/script&gt;.&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha \in [-1, 2]&lt;/script&gt; for the sake of experiment.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;=1 corresponds to the solution of large batch and &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt;=0 to the small batch.&lt;/p&gt;

&lt;h2 id=&quot;an-evidence-for-if-the-sharpness-of-minima-is-a-problem-continued&quot;&gt;An evidence for if the sharpness of minima is a problem [continued…]&lt;/h2&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/param_plots.png&quot; alt=&quot;1-D Parametric plots&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: 
There is an interesting correlation between the table 2 and this figure that I cannot help, but notice.&lt;br /&gt;
The network configurations when arranged according to the difference in test accuracy with LB and SB methods will fall into: &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
F_1&lt;C_2&lt;C_1&lt;C_3&lt;F_2&lt;C_4 %]]&gt;&lt;/script&gt;&lt;br /&gt;
Now, take a look at the solid blue line in each of the figures.
&lt;script type=&quot;math/tex&quot;&gt;F_1&lt;/script&gt; is the most flat of all at &lt;script type=&quot;math/tex&quot;&gt;\alpha=1&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;C_4&lt;/script&gt; at &lt;script type=&quot;math/tex&quot;&gt;\alpha=1&lt;/script&gt; is a valley with long walls on both the sides.&lt;/p&gt;

&lt;h2 id=&quot;sharpness-metric&quot;&gt;Sharpness metric&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\phi_{x,f}(\epsilon, A)=\frac{(max_{y \in C_\epsilon} f(x+Ay))-f(x)}{1+f(x)}\*100&lt;/script&gt;

&lt;ul&gt;
  &lt;li&gt;&lt;em&gt;x&lt;/em&gt; is the weight matrix, &lt;em&gt;f&lt;/em&gt; is a loss function that is &lt;em&gt;f(x)&lt;/em&gt; is the loss corresponding to the weight &lt;em&gt;x&lt;/em&gt;.&lt;/li&gt;
  &lt;li&gt;To keep things simple, consider this: A is an identity matrix, &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt; is a parameter that defines the size of neighbourhood, &lt;script type=&quot;math/tex&quot;&gt;C_{\epsilon}&lt;/script&gt; is the set of all points defined by &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: 
In the paper, so as to not to be misled by the case when the maximum value of f occurs in a very small sub-space around x, experiments are reported for when &lt;em&gt;A&lt;/em&gt; is an Identity matrix and for a random &lt;em&gt;nxp&lt;/em&gt; matrix, where &lt;em&gt;p&lt;/em&gt; is the dimension of the manifold.&lt;br /&gt;
When &lt;em&gt;A&lt;/em&gt; is not an identity matrix, but nxp matrix, then the points, &lt;em&gt;y&lt;/em&gt; are sampled in the random manifold.
That way both the values in full space around &lt;em&gt;x&lt;/em&gt; and the sub-space spanned by the random manifolds are explored.&lt;/p&gt;

&lt;h2 id=&quot;what-does-the-metric-say&quot;&gt;What does the metric say?&lt;/h2&gt;

&lt;p&gt;As expected, the number assigned by the metric is high in the case of LB as shown in the table below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/metric_lb_sb.png&quot; alt=&quot;Sharpness Metric on LB and SB solution&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-is-sb-avoiding-this-solution&quot;&gt;How is SB avoiding this solution?&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/lb_vs_sb/sharpness_batch_size.png&quot; alt=&quot;Sharpness with batch size&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The blue lines in the plot above is the change in testing accuracy (The vertical axis to the left) as the batch size increases (X axis).&lt;/li&gt;
  &lt;li&gt;The strokes in red capture the change in sharpness.&lt;/li&gt;
  &lt;li&gt;Observe the sudden fall in testing accuracy in both the plots, at around a batch size of 1500 for plot on left and 500 for plot on the right, meaning that the noise in the gradient computation is no longer enough to escape the attraction from sharp minimizer.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;how-is-sb-avoiding-this-solution-continued&quot;&gt;How is SB avoiding this solution? [continued…]&lt;/h2&gt;

&lt;p&gt;Answer: Noise.&lt;/p&gt;
&lt;blockquote&gt;
  &lt;p&gt;From the results reported in the previous section[slide], it appears that noise in the gradient pushes the iterates out of the basin of attraction of sharp minimizers and encourages movement towards a flatter minimizer where noise will not cause exit from that basin&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;can-we-patch-the-lb-method&quot;&gt;Can we patch the LB method?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Dynamic Sampling:&lt;/strong&gt; Start with a small batch and increase the batch size. It is shown to work, but what’s the schedule?&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;does-not-amount-to-reducing-the-sharpness-of-the-solution&quot;&gt;Does not amount to reducing the sharpness of the solution&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Data augmentation: Random transformations over the images: random rotations, translations, horizontal reflections, etc.; Basically, adding noise to the weight updates.&lt;/li&gt;
  &lt;li&gt;Conservative training: Make good use of data in a given batch before moving on to the next batch, this involves more than one update on a single batch.&lt;/li&gt;
  &lt;li&gt;Robust training: Classical approaches search for a lowest point in the valley, while these approaches attempt to lower an &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;-disc down the loss surface. Surprisingly, this method affected neither test accuracy nor sharpness.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-we-patch-the-lb-method-continued&quot;&gt;Can we patch the LB method? [continued…]&lt;/h2&gt;
&lt;p&gt;Can we add noise to the gradients computed in LB method which will perhaps make it more explorative?
The authors answered one such question asked by a reviewer [3]&lt;/p&gt;
&lt;pre&gt;
Thank you for your review. 
We experimented with additive random Gaussian noise (both in gradients 
and in iterates), noisy labels and noisy input-data. 
However, despite significant tuning of the hyperparameters of the random noise, we did not observe any consistent improvements in testing error.  
Overall, our feeling is that this needs deeper investigation and that LB methods may need to be modified in a more fundamental way to achieve good generalization.
&lt;/pre&gt;

&lt;p&gt;Note: There is a difference between noise and intelligent guess.&lt;/p&gt;

&lt;h2 id=&quot;whats-next&quot;&gt;What’s next?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Can it be analytically proved that the LB methods generally converge to the sharp minimizers of the training functions?&lt;/li&gt;
  &lt;li&gt;How best to patch LB methods to avoid this problem (better weights initialization, neural network architecture, optimization algorithm or regulatory means)?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;understanding-deep-learning-requires-rethinking-generalization-best-paper-award-iclr-2017-4&quot;&gt;UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION (Best Paper Award ICLR 2017) [4]&lt;/h2&gt;

&lt;p&gt;How are deep networks able to generalize so well?&lt;/p&gt;

&lt;h2 id=&quot;the-big-question&quot;&gt;The big question&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The state-of-art networks that did well on CIFAR10 and ImageNet datasets when trained on the same dataset with randomized labels or pixels, converged to zero training error.&lt;/li&gt;
  &lt;li&gt;This means that such networks have enough capacity to remember the data-points that they are trained on.&lt;/li&gt;
  &lt;li&gt;Yet, they do not. In spite of having a perfectly valid solution (training loss 0), the one corresponding to remembering all the data, the optimization procedure unfailingly finds a better solution (the one with low generalization error). 
What is causing this?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;randomization-tests&quot;&gt;Randomization tests&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/nnet_gen_how/random_tests.png&quot; alt=&quot;Randomization tests&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Average loss of training data goes to zero irrespective of the data transformations like: random labels, random/shuffled pixels.&lt;/li&gt;
  &lt;li&gt;Results for random transformations is reported only for the case when there is no other explicit regularization.&lt;/li&gt;
  &lt;li&gt;The results shown above are for the CIFAR10 dataset (which is smaller than Imagenet). On CIFAR10, smaller versions of the networks such as Inception, Alexnet, MLPs, that are designed for Imagenet task, are used. On Imagenet, the training error did not converge to 100%, but only 95.2 top-1 accuracy (which is still very good for million labels).&lt;/li&gt;
  &lt;li&gt;The case of random labels take longer to converge than the case of random pixels which involves more randomization. This could be because in the case of random pixels, the images are well separated; in the case of random labels, the images are still correlated.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;can-the-traditional-approaches-provide-a-generalization-bound&quot;&gt;Can the traditional approaches provide a generalization bound?&lt;/h2&gt;
&lt;p&gt;&lt;strong&gt;Rademacher Complexity (RC) and VC-dimension&lt;/strong&gt;
&lt;script type=&quot;math/tex&quot;&gt;\hat{\Re}_n(H)=E_{\sigma}[\sup_{h \in H} \frac{1}{n} \sum_{i=1}^{n}{\sigma_ih(x_i)}] \sigma_1...\sigma_n \in {\pm 1}&lt;/script&gt; i.i.d. uniform random binary labels.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;RC measures the ability of a given hypothesis space, &lt;em&gt;H&lt;/em&gt;, to fit random binary labels, &lt;script type=&quot;math/tex&quot;&gt;\pm 1&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;Since the networks are able to fit random labels perfectly, the RC measure would close on its upper bound, &lt;script type=&quot;math/tex&quot;&gt;\Re(H)\approx 1&lt;/script&gt;, and hence may fail to provide any reasonable generalization bound.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;We need generalization bound to identify solutions that generalize to ones that do not.&lt;/li&gt;
  &lt;li&gt;The existing methods that bound VC-dimension or its continuous analog, fat-shattering dimension do not seem to explain the generalization behavior.&lt;/li&gt;
  &lt;li&gt;The paper also mentions about weaker notions of uniform stability, but concludes that it is difficult to utilize these effectively. 
They measure the sensitivity to replacing one of the data points.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This slide basically concludes that none of the existing complexity measures can give a reasonable generalization bound.&lt;br /&gt;
You can skip this slide, and still understand the rest of them.&lt;/p&gt;

&lt;h2 id=&quot;are-regularizers-responsible-for-generalization&quot;&gt;Are Regularizers responsible for Generalization?&lt;/h2&gt;
&lt;p&gt;Experimented with three commonly used regularizers&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Data augmentation&lt;/strong&gt;: Transformations on the image like: random cropping, random perturbation of brightness, saturation, hue and contrast&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Weight decay&lt;/strong&gt;: an &lt;script type=&quot;math/tex&quot;&gt;l_2&lt;/script&gt; regularizer on the weights.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Dropout&lt;/strong&gt;: randomly dropping the output of a layer with a given probability.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;are-regularizers-responsible-for-generalization-continued&quot;&gt;Are Regularizers responsible for Generalization? [continued…]&lt;/h2&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/nnet_gen_how/reg_in_gen.png&quot; alt=&quot;Do Regularizers help in Generalization&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: The networks generalize fine with no regularizers (we all know that, though).&lt;br /&gt;
The point is to rule out regularizers as “the” reason for generalization.&lt;/p&gt;

&lt;h2 id=&quot;implicit-regularizers&quot;&gt;Implicit Regularizers?&lt;/h2&gt;
&lt;p&gt;Two commonly used implicit regularizers are (a) early stopping (b) batch normalization.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/nnet_gen_how/imp_reg.png&quot; alt=&quot;Implicit Regularizers&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The shaded area is what could have been gained if stopped early.&lt;/li&gt;
  &lt;li&gt;“Although not explicitly designed for regularization, batch normalization is usually found to improve the generalization performance”. The impact of batch norm is only 3-4% (figure 2b)
In later sections, they show that SGD also does implicit regularization. It is specially handled.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;concluding-remarks-about-regularization&quot;&gt;Concluding remarks about regularization&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;Explicit and implicit regularizers when properly tuned have improved the generalization performance consistently.&lt;/li&gt;
  &lt;li&gt;However, it is unlikely that they are fundamental for generalization.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;expressivity-of-networks&quot;&gt;Expressivity of networks&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The existing methods to compute expressivity only consider what functions over the domain can be represented irrespective of the sample size.&lt;/li&gt;
  &lt;li&gt;This work proves a lower bound on size of the networks that can perform on a finite sample size.&lt;/li&gt;
  &lt;li&gt;Theorem: There exists a two-layer neural network with ReLU activations and 2n + d weights that can represent any function on a sample of size n in d dimensions.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: Expressivity of a network provides insight into what functions over the domain a network is capable of representing. 
It is a fancy word for model complexity.&lt;/p&gt;

&lt;p&gt;For example, AlexNet which is trained on ImageNet (1.2 million training images) has 60M parameters and more than one layer. 
According to the theorem above, it is capable of representing any function over ImageNet including random labeling (effectively memorizing the data).&lt;/p&gt;

&lt;p&gt;For the proof, they constructed a neural net with one hidden layer which has a width of &lt;em&gt;n&lt;/em&gt;. 
Since &lt;em&gt;n&lt;/em&gt; can be very large, they also provide a construction such that the network has width O(n/k) and k layers.&lt;/p&gt;

&lt;h2 id=&quot;is-sgd-responsible-for-generalization&quot;&gt;Is SGD responsible for generalization?&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The solution obtained by SGD in the linear case is looked at, to better understand the behavior of its solution.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;In the linear case&lt;/p&gt;

    &lt;script type=&quot;math/tex; mode=display&quot;&gt;min_{w \in \mathbb{R}^d} \frac{1}{n} \sum_{i=1}^nloss(w^Tx_i, y_i)&lt;/script&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;If &lt;script type=&quot;math/tex&quot;&gt;d\geq n&lt;/script&gt;, there are several solutions. Does SGD find a generalizable solution in the face of several possible solutions?&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;The updates of SGD at each step are of the form &lt;script type=&quot;math/tex&quot;&gt;w_{t+1} \leftarrow w_{t}-\eta e_tx_{i_t}&lt;/script&gt;. The final solution can be written as &lt;script type=&quot;math/tex&quot;&gt;w=X^T\alpha&lt;/script&gt;. 
This reduces to
&lt;script type=&quot;math/tex&quot;&gt;XX^T\alpha = y&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;&lt;script type=&quot;math/tex&quot;&gt;XX^T&lt;/script&gt; is the kernel gram matrix.&lt;/li&gt;
  &lt;li&gt;The equation above can be solved exactly for at least small datasets.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;The solution found by solving the equation above for MNIST and CIFAR10 dataset have an error rate (best) of 1.2% and 15% respectively.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;It can be proved that the solution obtained by SGD (or the equation) is of minimum norm. 
That is of all the solutions that exactly fit the data, SGD will often converge to the solution with minimum norm.
By doing so, SGD behaves like an implicit regularizer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Minimum norm isn’t always a good thing&lt;/strong&gt;: “On MNIST data, the l2-norm of the minimum norm solution with no preprocessing is approximately 220. With wavelet preprocessing, the norm jumps to 390. Yet the test error drops by a factor of 2”&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Openreview has some interesting discussion on this section [5]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;conclusion&quot;&gt;Conclusion&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;The effective capacity of several networks is large enough to shatter the training data, and yet they do fine on test data. 
It is still a missing piece of the puzzle as to what the agent is responsible.&lt;/li&gt;
  &lt;li&gt;Yet to discover formal measures under which these enormous models are simple and finally explain the generalization.&lt;/li&gt;
  &lt;li&gt;A key takeaway is that none of the regularization methods or model capacity is responsible for the generalization behavior and, perhaps, it is the dynamics of the optimization procedure that is causing this.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;a-case-study&quot;&gt;A Case study&lt;/h2&gt;
&lt;p&gt;An implementation of the scaled-down AlexNet for CIFAR10 described in the second paper is available as a &lt;a href=&quot;https://github.com/tensorflow/models/blob/master/slim/nets/cifarnet.py&quot;&gt;tensorflow model&lt;/a&gt; (perhaps implemented by this team itself).&lt;/p&gt;

&lt;p&gt;I trained this network on the true CIFAR10 dataset and with pixels randomized in CIFAR10.
The parameter configuration is as follows:&lt;/p&gt;
&lt;pre&gt;&lt;code class=&quot;language-txt&quot;&gt;optimizer: SGD
momentum: 0.9
learning rate: 0.1
learning rate decay factor: 0.9
dropout: None
weight_decay: 0
input augmentation: None
&lt;/code&gt;&lt;/pre&gt;

&lt;h2 id=&quot;alexnet-on-randomized-cifar10&quot;&gt;AlexNet on Randomized CIFAR10&lt;/h2&gt;
&lt;p&gt;I found that the convergence on this dataset is extremely sensitive to regularization.
Failed to converge in each of the following cases:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;learning rate of 0.1 rather than the suggested 0.01&lt;/li&gt;
  &lt;li&gt;weight decay: 0.004&lt;/li&gt;
  &lt;li&gt;dropout: 0.5&lt;/li&gt;
  &lt;li&gt;any bit of data augmentation&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Note: My claims are to be taken with a pinch of salt. 
These are only my observations, but not tested with rigor.&lt;/p&gt;

&lt;h3 id=&quot;it-did-memorize-the-dataset&quot;&gt;It did memorize the dataset&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/slides/expt/cifar10_rnd_90K.png&quot; alt=&quot;Loss when SGD is used&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: This is when the model is trained on CIFAR10 with randomized labels.&lt;/p&gt;

&lt;p&gt;In the case when I missed the first line in the image preprocessing, the loss behavior is shown in the image below.&lt;/p&gt;
&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;/&lt;/span&gt;&lt;span class=&quot;mf&quot;&gt;255.&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;per_image_standardization&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;image&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;&lt;img src=&quot;/assets/images/slides/expt/cifar_rnd_300K.png&quot; alt=&quot;When not properly normalized&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Note: Just did not converge even after 300K iterations.&lt;/p&gt;

&lt;h3 id=&quot;characteristics-of-the-solution-learned&quot;&gt;Characteristics of the solution learned&lt;/h3&gt;

&lt;h4 id=&quot;model-learned-on-true-data&quot;&gt;Model learned on true data&lt;/h4&gt;

&lt;p&gt;Smoothness metric: 29.821 &lt;script type=&quot;math/tex&quot;&gt;\pm&lt;/script&gt; 0.250&lt;/p&gt;

&lt;h4 id=&quot;model-learned-on-randomized-pixels&quot;&gt;Model learned on randomized pixels&lt;/h4&gt;

&lt;p&gt;Smoothness metric: 263.928 &lt;script type=&quot;math/tex&quot;&gt;\pm&lt;/script&gt; 3.141&lt;/p&gt;

&lt;p&gt;Note: * The reported smoothness metric is averaged over three runs (the number to the right of this value is the standard deviation), with parameters: 100 neighbors considered in hyper-sphere given by &lt;script type=&quot;math/tex&quot;&gt;\epsilon&lt;/script&gt;=e-3. &lt;em&gt;Cross Entropy loss&lt;/em&gt; is used in the place of the function, f.&lt;/p&gt;

&lt;p&gt;The value assigned by the smoothness metric [1] is orders of magnitude bigger than in the case of model learned with randomized pixels.&lt;/p&gt;

&lt;p&gt;It is also shown in [1] that SGD updates have noise that will keep it away from such valleys. 
I strongly believe this to be one of the reasons why the solution found by SGD in the case of true data generalizes well; Because SGD cannot precisely navigate down the valley to find the solution that corresponds to over-fitting [1]. 
In the case of random pixels, since there is no better solution, SGD manages to find it (the over-fitting solution), perhaps.&lt;/p&gt;

&lt;p&gt;Note: The implicit assumption I made here is that the smoothness metric of the solutions that corresponds to over-fitting solution on true data will have similar smoothness metric as the solution found in the case of randomized labels (the over-fit)&lt;/p&gt;

&lt;h2 id=&quot;interesting-remarkscritics-from-the-audience&quot;&gt;Interesting remarks/critics from the audience&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;l_2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;l_1&lt;/script&gt; norm should be used with caution. They both penalize weights such that those that do not affect the loss much, remain low. Although, it may reduce the model complexity; It is not guaranteed that such regularizations can find a better solution. Consider the case when it is desired that some parameters are unbounded or take a finite non-zero value. In a nutshell, such weight penalties should not be used as a one-size-fits-all manner.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;When a model that has 100% accuracy on the train set, but only 84% on the test set; A model with 16% generalization error is considered an over-fit, at least for the traditional Machine Learning models. Why is that not the case here?&lt;br /&gt;
 I agree that the solution with 16% generalization error may not be the best and can be improved, but it is not considered an overfit, at least not according to the authors. The authors consider over-fitting the case when the performance on test set is no better than chance.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Why are sharp minimizers bad for generalization?&lt;/p&gt;
    &lt;ul&gt;
      &lt;li&gt;There are several explanations and links provided in section 2.1 of [1].
According to the minimum description length (MDL) theory, the models that require fewer bits to describe, generalize better. Since flat minima can be specified with lower precision, they have lower MDL.&lt;/li&gt;
      &lt;li&gt;An alternate explanation is provided in [6]. In [6], the authors have proposed a variant of SGD called Entropy-SGD which is specially designed to find flat minimas. The following explanation is provided.&lt;br /&gt;
If we consider a prior on the parameters, say a Gaussian prior with fixed variance, the marginal likelihood of data would be higher in the case of flatter minima. 
&lt;script type=&quot;math/tex&quot;&gt;P(x) = P(x/\theta)P(\theta)&lt;/script&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;references&quot;&gt;References&lt;/h2&gt;

&lt;ol&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=H1oyRlYgg&quot;&gt;ON LARGE-BATCH TRAINING FOR DEEP LEARNING: GENERALIZATION GAP AND SHARP MINIMA&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;Yann LeCun’s Efficient BackProp.&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=H1oyRlYgg&amp;amp;noteId=H1oyRlYgg&quot;&gt;Discussion on Openreview for 1 above&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/pdf?id=Sy8gdB9xx&quot;&gt;UNDERSTANDING DEEP LEARNING REQUIRES RETHINKING GENERALIZATION&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://openreview.net/forum?id=Sy8gdB9xx&amp;amp;noteId=Sy8gdB9xx&quot;&gt;Discussion on Openreview for above&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1611.01838.pdf&quot;&gt;ENTROPY-SGD: BIASING GRADIENT DESCENT INTO WIDE VALLEYS&lt;/a&gt;&lt;/li&gt;
&lt;/ol&gt;

&lt;h2 id=&quot;proof-of-theorem-extra-slide&quot;&gt;Proof of theorem (Extra slide)&lt;/h2&gt;
&lt;p&gt;Theorem: &lt;em&gt;There exists a two-layer neural network with ReLU activations and 2n + d weights that can represent any function on a sample of size n in d dimensions.&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;Sketchy proof: For weight vector w, b &lt;script type=&quot;math/tex&quot;&gt;\in \mathbb{R}^n&lt;/script&gt; and a &lt;script type=&quot;math/tex&quot;&gt;\in \mathbb{R}^d&lt;/script&gt;, consider the function to learn: c: &lt;script type=&quot;math/tex&quot;&gt;\mathbb{R}^n\rightarrow \mathbb{R}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c(x) = \sum_{j=1}{w_j max(\langle a,x\rangle-b_j, 0)}&lt;/script&gt;

&lt;p&gt;The weights from input to the layer are shared: &lt;em&gt;a&lt;/em&gt;.
The activations from the layer are combined with the vector: &lt;em&gt;w&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Note: For proof, look at [4]; I am only interested in an intuitive explanation.&lt;/p&gt;

&lt;p&gt;Basically, the plan is to make a different number of neurons to activate (classic trick).
That is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
b_1&lt;x_1&lt;...b_n&lt;x_n %]]&gt;&lt;/script&gt;, which means the number of activated neurons for the input &lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt; is i.&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;x_i&lt;/script&gt;s are the inputs to the first layer that is &lt;script type=&quot;math/tex&quot;&gt;\langle a,z_i\rangle&lt;/script&gt;.
Since &lt;em&gt;a&lt;/em&gt; and &lt;em&gt;b&lt;/em&gt; are both unknowns, choose a value for a and for each of the distinct values of &lt;script type=&quot;math/tex&quot;&gt;\langle a,z_i\rangle&lt;/script&gt;, choose the value for &lt;em&gt;&lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt;&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;Finally, we have &lt;script type=&quot;math/tex&quot;&gt;y=Aw&lt;/script&gt; where A is &lt;script type=&quot;math/tex&quot;&gt;max(\langle a,x\rangle-b_j, 0)&lt;/script&gt;. 
The construction is such that A is full rank, hence &lt;em&gt;w&lt;/em&gt; is solvable.&lt;/p&gt;

&lt;p&gt;Note: The weights learned by a simple network on XOR data would show the same behavior, &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
b_1&lt;x_1...b_n&lt;x_n %]]&gt;&lt;/script&gt;; That is why I call it the classic trick.&lt;/p&gt;

&lt;h2 id=&quot;contact&quot;&gt;Contact&lt;/h2&gt;

&lt;p&gt;Vihari Piratla&lt;/p&gt;

&lt;p&gt;IIT Bombay&lt;/p&gt;

&lt;p&gt;viharipiratla@gmail.com&lt;/p&gt;
</description>
        <pubDate>Fri, 17 Mar 2017 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/deep-learning/training/2017/03/17/mlr-iitb.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep-learning/training/2017/03/17/mlr-iitb.html</guid>
        
        
        <category>deep-learning</category>
        
        <category>training</category>
        
      </item>
    
      <item>
        <title>Notes from Geoffrey Hinton's Neural Networks course on Coursera</title>
        <description>&lt;h1 id=&quot;my-notes-from-geoffrey-hintons-neural-networks-course-on-coursera&quot;&gt;My notes from Geoffrey Hinton’s Neural Networks course on Coursera&lt;/h1&gt;

&lt;ul id=&quot;markdown-toc&quot;&gt;
  &lt;li&gt;&lt;a href=&quot;#my-notes-from-geoffrey-hintons-neural-networks-course-on-coursera&quot; id=&quot;markdown-toc-my-notes-from-geoffrey-hintons-neural-networks-course-on-coursera&quot;&gt;My notes from Geoffrey Hinton’s Neural Networks course on Coursera&lt;/a&gt;&lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-2-an-overview-of-the-main-types-of-neural-network-architecture&quot; id=&quot;markdown-toc-week-2-an-overview-of-the-main-types-of-neural-network-architecture&quot;&gt;Week 2 (An overview of the main types of neural network architecture)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#perceptron-learning&quot; id=&quot;markdown-toc-perceptron-learning&quot;&gt;Perceptron learning&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-3-the-backpropagation-learning-proccedure&quot; id=&quot;markdown-toc-week-3-the-backpropagation-learning-proccedure&quot;&gt;Week 3 (The backpropagation learning proccedure)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#multi-layer-learning&quot; id=&quot;markdown-toc-multi-layer-learning&quot;&gt;Multi-layer learning&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-learning-internal-representations-by-error-propagation-link1980backprop&quot; id=&quot;markdown-toc-reading-assignment-learning-internal-representations-by-error-propagation-link1980backprop&quot;&gt;Reading assignment (Learning internal representations by Error Propagation) &lt;a href=&quot;http://www.cnbc.cmu.edu/~plaut/IntroPDP/papers/RumelhartETAL86.backprop.pdf&quot;&gt;link&lt;/a&gt;&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#parity-problem&quot; id=&quot;markdown-toc-parity-problem&quot;&gt;Parity Problem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#symmetry-problem&quot; id=&quot;markdown-toc-symmetry-problem&quot;&gt;Symmetry Problem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#addition-problem&quot; id=&quot;markdown-toc-addition-problem&quot;&gt;Addition Problem&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#negation&quot; id=&quot;markdown-toc-negation&quot;&gt;Negation&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#t-c-classification&quot; id=&quot;markdown-toc-t-c-classification&quot;&gt;T-C classification&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#in-the-case-of-recurrent-nets&quot; id=&quot;markdown-toc-in-the-case-of-recurrent-nets&quot;&gt;In the case of recurrent nets&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-4-learning-to-predict-the-next-word&quot; id=&quot;markdown-toc-week-4-learning-to-predict-the-next-word&quot;&gt;Week 4 (Learning to predict the next word)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-y-bengios-neural-language-model-linkbengio-language&quot; id=&quot;markdown-toc-reading-assignment-y-bengios-neural-language-model-linkbengio-language&quot;&gt;Reading Assignment (Y. Bengio’s Neural Language Model) &lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot; title=&quot;A Neural Probabilistic Language Model&quot;&gt;link&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-5-why-object-recognition-is-difficult&quot; id=&quot;markdown-toc-week-5-why-object-recognition-is-difficult&quot;&gt;Week 5 (Why object recognition is difficult.)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment---1-gradient-based-learning-applied-to-document-recognition-linklecun-docrec-yet-to-be-read&quot; id=&quot;markdown-toc-reading-assignment---1-gradient-based-learning-applied-to-document-recognition-linklecun-docrec-yet-to-be-read&quot;&gt;Reading Assignment - 1 (Gradient based learning applied to document recognition) &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf&quot; title=&quot;Gradient based learning applied to document recognition&quot;&gt;link&lt;/a&gt; [Yet to be read]&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment---2-convolutional-networks-for-image-speech-and-time-series-linkconv-nets&quot; id=&quot;markdown-toc-reading-assignment---2-convolutional-networks-for-image-speech-and-time-series-linkconv-nets&quot;&gt;Reading Assignment - 2 (Convolutional Networks for Image, Speech and Time-Series) &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf&quot; title=&quot;Convolutional Networks for Image, Speech and Time-Series&quot;&gt;link&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#slides&quot; id=&quot;markdown-toc-slides&quot;&gt;Slides&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#good-questions-from-quiz&quot; id=&quot;markdown-toc-good-questions-from-quiz&quot;&gt;Good questions from quiz&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#programming-assignment&quot; id=&quot;markdown-toc-programming-assignment&quot;&gt;Programming assignment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-6-mini-batch-gradient-descent-and-learning-rate&quot; id=&quot;markdown-toc-week-6-mini-batch-gradient-descent-and-learning-rate&quot;&gt;Week 6 (Mini-batch gradient descent and learning rate)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#tricks-to-speed-up-training&quot; id=&quot;markdown-toc-tricks-to-speed-up-training&quot;&gt;Tricks to speed up training&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-7-training-recurrent-neural-networks&quot; id=&quot;markdown-toc-week-7-training-recurrent-neural-networks&quot;&gt;Week 7 (Training Recurrent Neural Networks)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#training-rnn----backprop&quot; id=&quot;markdown-toc-training-rnn----backprop&quot;&gt;Training RNN – backprop&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#why-is-training-rnn-difficult&quot; id=&quot;markdown-toc-why-is-training-rnn-difficult&quot;&gt;Why is training RNN difficult&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#long-term-short-term-memory&quot; id=&quot;markdown-toc-long-term-short-term-memory&quot;&gt;Long term short term memory&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#interesting-questions-from-quiz&quot; id=&quot;markdown-toc-interesting-questions-from-quiz&quot;&gt;Interesting questions from Quiz&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-8-more-rnns&quot; id=&quot;markdown-toc-week-8-more-rnns&quot;&gt;Week 8 (More RNNs)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#echo-state-networks&quot; id=&quot;markdown-toc-echo-state-networks&quot;&gt;Echo state networks&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-generating-text-with-recurrent-neural-networks&quot; id=&quot;markdown-toc-reading-assignment-generating-text-with-recurrent-neural-networks&quot;&gt;Reading Assignment (Generating Text with Recurrent Neural Networks)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#hessian-free-optimization-optional-lecture-material&quot; id=&quot;markdown-toc-hessian-free-optimization-optional-lecture-material&quot;&gt;Hessian-free optimization (optional lecture material)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#regularization-in-rnns&quot; id=&quot;markdown-toc-regularization-in-rnns&quot;&gt;Regularization in RNNs&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-9-ways-to-make-neural-networks-generalize-better&quot; id=&quot;markdown-toc-week-9-ways-to-make-neural-networks-generalize-better&quot;&gt;Week 9 (Ways to make neural networks generalize better)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#overview-of-ways-to-improve-generalizaton&quot; id=&quot;markdown-toc-overview-of-ways-to-improve-generalizaton&quot;&gt;Overview of ways to improve generalizaton&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#get-more-data&quot; id=&quot;markdown-toc-get-more-data&quot;&gt;Get more data&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#use-a-model-that-has-the-right-capacity&quot; id=&quot;markdown-toc-use-a-model-that-has-the-right-capacity&quot;&gt;Use a model that has the right capacity&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#average-many-different-models&quot; id=&quot;markdown-toc-average-many-different-models&quot;&gt;Average many different models&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bayesisan-approach&quot; id=&quot;markdown-toc-bayesisan-approach&quot;&gt;Bayesisan approach&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#weight-decay&quot; id=&quot;markdown-toc-weight-decay&quot;&gt;Weight decay&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#using-noise-as-regularizer&quot; id=&quot;markdown-toc-using-noise-as-regularizer&quot;&gt;Using noise as regularizer&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#bayesian-approach-to-regularization&quot; id=&quot;markdown-toc-bayesian-approach-to-regularization&quot;&gt;Bayesian approach to regularization&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#good-questions-from-quiz-1&quot; id=&quot;markdown-toc-good-questions-from-quiz-1&quot;&gt;Good questions from quiz&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#programming-assignment-1&quot; id=&quot;markdown-toc-programming-assignment-1&quot;&gt;Programming Assignment&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-10-combining-multiple-neural-networks-to-improve-generalization&quot; id=&quot;markdown-toc-week-10-combining-multiple-neural-networks-to-improve-generalization&quot;&gt;Week 10 (Combining multiple neural networks to improve generalization)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#mixture-of-experts-developed-in-1990s&quot; id=&quot;markdown-toc-mixture-of-experts-developed-in-1990s&quot;&gt;Mixture of experts (developed in 1990s)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#full-bayesian-learning-revisited&quot; id=&quot;markdown-toc-full-bayesian-learning-revisited&quot;&gt;Full Bayesian learning revisited&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#making-it-practical-with-mcmc&quot; id=&quot;markdown-toc-making-it-practical-with-mcmc&quot;&gt;Making it practical with MCMC&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#dropout----averaging-without-using-several-models&quot; id=&quot;markdown-toc-dropout----averaging-without-using-several-models&quot;&gt;Dropout – Averaging without using several models&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-1-adaptive-mixtures-of-local-experts-linkexpert-mixture&quot; id=&quot;markdown-toc-reading-assignment-1-adaptive-mixtures-of-local-experts-linkexpert-mixture&quot;&gt;Reading Assignment 1 (Adaptive Mixtures of Local Experts) &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf&quot; title=&quot;Adaptive Mixtures of Local Experts&quot;&gt;link&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-2-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors-linkdropout&quot; id=&quot;markdown-toc-reading-assignment-2-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors-linkdropout&quot;&gt;Reading Assignment 2 (Improving neural networks by preventing co-adaptation of feature detectors) &lt;a href=&quot;https://arxiv.org/pdf/1207.0580.pdf&quot; title=&quot;Improving neural networks by preventing co-adaptation of feature detectors&quot;&gt;link&lt;/a&gt;&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-11-hopfield-nets-and-boltzmann-machines&quot; id=&quot;markdown-toc-week-11-hopfield-nets-and-boltzmann-machines&quot;&gt;Week 11 (Hopfield Nets and Boltzmann machines)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#the-memory-capacity-of-hopfield-nets-and-methods-to-improve-it&quot; id=&quot;markdown-toc-the-memory-capacity-of-hopfield-nets-and-methods-to-improve-it&quot;&gt;The memory capacity of hopfield nets and methods to improve it&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#using-hopfield-energy-states-for-a-good-interpretation-of-inputs-with-the-underlying-hidden-states&quot; id=&quot;markdown-toc-using-hopfield-energy-states-for-a-good-interpretation-of-inputs-with-the-underlying-hidden-states&quot;&gt;Using Hopfield energy states for a good interpretation of inputs with the underlying hidden states&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#simulated-annealing-and-thermal-equilibrium&quot; id=&quot;markdown-toc-simulated-annealing-and-thermal-equilibrium&quot;&gt;Simulated annealing and thermal equilibrium&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#boltzmann-machines-aka-stochastic-hopfield-networks&quot; id=&quot;markdown-toc-boltzmann-machines-aka-stochastic-hopfield-networks&quot;&gt;Boltzmann Machines aka stochastic hopfield networks&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-12-restricted-boltzmann-machines----rbms&quot; id=&quot;markdown-toc-week-12-restricted-boltzmann-machines----rbms&quot;&gt;Week 12 (Restricted Boltzmann Machines – RBMs)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#boltzmann-machine-learning-algorithm&quot; id=&quot;markdown-toc-boltzmann-machine-learning-algorithm&quot;&gt;Boltzmann Machine Learning algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#more-efficient-ways-to-train-bms&quot; id=&quot;markdown-toc-more-efficient-ways-to-train-bms&quot;&gt;More efficient ways to train BMs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#restricted-boltzmann-machines-rbms&quot; id=&quot;markdown-toc-restricted-boltzmann-machines-rbms&quot;&gt;Restricted Boltzmann Machines (RBMs)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#rbms-for-collaborative-filtering&quot; id=&quot;markdown-toc-rbms-for-collaborative-filtering&quot;&gt;RBMs for collaborative filtering&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#good-questions-from-quiz-2&quot; id=&quot;markdown-toc-good-questions-from-quiz-2&quot;&gt;Good questions from quiz&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-13-belief-nets&quot; id=&quot;markdown-toc-week-13-belief-nets&quot;&gt;Week 13 (Belief nets)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#a-brief-history-of-backpropagation&quot; id=&quot;markdown-toc-a-brief-history-of-backpropagation&quot;&gt;A brief history of backpropagation&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#belief-nets&quot; id=&quot;markdown-toc-belief-nets&quot;&gt;Belief nets&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#wake-sleep-algorithm&quot; id=&quot;markdown-toc-wake-sleep-algorithm&quot;&gt;Wake-sleep algorithm&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#reading-assignment-1-the-wake-sleep-algorithm-for-unsupervised-neural-networkswake-sleep&quot; id=&quot;markdown-toc-reading-assignment-1-the-wake-sleep-algorithm-for-unsupervised-neural-networkswake-sleep&quot;&gt;Reading Assignment 1 (&lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~Dayan/papers/hdfn95.pdf&quot; title=&quot;The wake-sleep algorithm for unsupervised neural networks&quot;&gt;The wake-sleep algorithm for unsupervised neural networks&lt;/a&gt;)&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#programming-assignment-2&quot; id=&quot;markdown-toc-programming-assignment-2&quot;&gt;Programming assignment&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#cheat-sheet&quot; id=&quot;markdown-toc-cheat-sheet&quot;&gt;Cheat sheet&lt;/a&gt;        &lt;ul&gt;
          &lt;li&gt;&lt;a href=&quot;#bm&quot; id=&quot;markdown-toc-bm&quot;&gt;BM&lt;/a&gt;&lt;/li&gt;
          &lt;li&gt;&lt;a href=&quot;#rbm&quot; id=&quot;markdown-toc-rbm&quot;&gt;RBM&lt;/a&gt;&lt;/li&gt;
        &lt;/ul&gt;
      &lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-14-deep-neural-nets-with-generative-pre-training&quot; id=&quot;markdown-toc-week-14-deep-neural-nets-with-generative-pre-training&quot;&gt;Week 14 (Deep neural nets with generative pre-training)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#layers-of-features-by-stacking-rbms&quot; id=&quot;markdown-toc-layers-of-features-by-stacking-rbms&quot;&gt;Layers of features by stacking RBMs&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#modeling-of-real-valued-data&quot; id=&quot;markdown-toc-modeling-of-real-valued-data&quot;&gt;Modeling of real-valued data&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#guassian-binary-rbm&quot; id=&quot;markdown-toc-guassian-binary-rbm&quot;&gt;Guassian Binary RBM&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#equivalence-of-rbm-and-sigmoid-belief-net&quot; id=&quot;markdown-toc-equivalence-of-rbm-and-sigmoid-belief-net&quot;&gt;Equivalence of RBM and sigmoid belief net&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#week-15-auto-encoders-and-semantic-hashing----modeling-hierarchical-structure-with-neural-nets&quot; id=&quot;markdown-toc-week-15-auto-encoders-and-semantic-hashing----modeling-hierarchical-structure-with-neural-nets&quot;&gt;Week 15 (Auto-encoders and semantic hashing – Modeling hierarchical structure with neural nets)&lt;/a&gt;    &lt;ul&gt;
      &lt;li&gt;&lt;a href=&quot;#from-pca-to-autoencoder&quot; id=&quot;markdown-toc-from-pca-to-autoencoder&quot;&gt;From PCA to Autoencoder&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#semantic-hashing&quot; id=&quot;markdown-toc-semantic-hashing&quot;&gt;Semantic Hashing&lt;/a&gt;&lt;/li&gt;
      &lt;li&gt;&lt;a href=&quot;#using-auto-encoders-for-pre-training&quot; id=&quot;markdown-toc-using-auto-encoders-for-pre-training&quot;&gt;Using Auto-encoders for Pre-training&lt;/a&gt;&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;&lt;a href=&quot;#good-questions-from-final-quiz&quot; id=&quot;markdown-toc-good-questions-from-final-quiz&quot;&gt;Good questions from final quiz&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;week-2-an-overview-of-the-main-types-of-neural-network-architecture&quot;&gt;Week 2 (An overview of the main types of neural network architecture)&lt;/h1&gt;

&lt;h2 id=&quot;perceptron-learning&quot;&gt;Perceptron learning&lt;/h2&gt;

&lt;p&gt;In a perceptron learning case, the updates to the weight vector are as follows:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;If the current weight vector correctly classifies a vector &lt;strong&gt;x&lt;/strong&gt;, then it is passed on&lt;/li&gt;
  &lt;li&gt;when misclassified then update the weight with &lt;script type=&quot;math/tex&quot;&gt;\eta * (t-y)*x&lt;/script&gt; where t and y are expected and found output, &lt;script type=&quot;math/tex&quot;&gt;\eta&lt;/script&gt; is the learning rate.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Few points to note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Because, we look at each example at a time, the update can throw the learning procedure off and hence may lead to more number of classification errors. 
This can happen irrespective of if the problem is linearly separable or not.
A smoother transition can be obtained by a smaller learning rate.&lt;/li&gt;
  &lt;li&gt;The similarity of the weight vector with generously feasible weight vector should continuosly increase and infact it can be proved that perceptron algorithm works because it gets closer to the desired vector.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;One way to make sense of the update equation is to analytically see that the update is gradient of distance between current weight vector and feasible vectors. 
(This is what makes the difference between Perceptron learning and general delta-rule, although they may both lead to same update equation in the case of single linear neuron)
This can also be seen geometrically, see the images below:&lt;/p&gt;

&lt;p&gt;Update in positive case:&lt;br /&gt;
&lt;img src=&quot;/assets/images/nn-notes/positive-perceptron.png&quot; alt=&quot;Update in positive case&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Update in negative case:&lt;br /&gt;
&lt;img src=&quot;/assets/images/nn-notes/negative-perceptron.png&quot; alt=&quot;Update in negative case&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;week-3-the-backpropagation-learning-proccedure&quot;&gt;Week 3 (The backpropagation learning proccedure)&lt;/h1&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/back-prop.png&quot; alt=&quot;Backprop slide&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above captures everything you need to know about back-prop. Observe that there are two things which are almost separate. One is the recurrence relation of error with y for each layer (the first two equations). Other is weight update calculated by the outer product of error differential with respect to input of next layer and output of this layer.&lt;/p&gt;

&lt;p&gt;Stacking several linear neuron layers does not add to capacity. In the end, the entire stack can be replaced by a single linear neuron.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;A feed forward neural network is defined as a network with no cycles and with no other added constraints. For example, an input layer can have connections to any layer of the subsequent layers as long as they do not form a loop. It is not necessary for every neuron to connect to only the next layer. To enforce the constraint of no cycle, every neuron should send output to layer above and receive input from layer below.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;h2 id=&quot;multi-layer-learning&quot;&gt;Multi-layer learning&lt;/h2&gt;

&lt;p&gt;In perceptron learning procedure, the weights got closer to true weights with every iteration, but in NN we try to get the outputs closer to expected ones.&lt;/p&gt;

&lt;p&gt;It’s not hard to see that perceptrons are specialization of learning through back-propagation. 
The update for weights in that case is just cross product of error in the last layer and output of the input layer, which means adding proportion of input to the weights every time.
The perceptron algorithm cannot directly be extended to neural networks. 
In perceptron space, the average of two good solutions is again a solution (convex).&lt;br /&gt;
The delta rule for weight update with linear activation is &lt;script type=&quot;math/tex&quot;&gt;\sum_{n}{\eta_i * x_n * (t_n-y_n)}&lt;/script&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;By making the error rate small enough we can get as close as we want to the best answer.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;How quickly do weights converge to their correct values?&lt;/strong&gt; 
It can be very slow if two input dimensions are highly correlated. 
An analogy to weight learning is explained with the example: if you were not told about the prices of each of the commodities that you buy but only the final price, then after some purchases you can figure out the price. 
If you always buy same quantity of two things then it will take a long time to figure their individual prices.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Cases of Over-fitting:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Target values may be unreliable (bayesian error). This is usually a minor worry.&lt;/li&gt;
  &lt;li&gt;Sampling error: There will be accidental regularities just because of particular training cases that were chosen. 
For example, in the hand-writing case, we collected data from a person who write ‘A’ very differently. 
If the model is very flexible it can model the sampling error really well which can be disastrous.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Although, back propagation wins by a huge margin in finding weights, there are other possible ways to find the weights (although none of the following beats backprop in performance or exactness, it’s good to take note).&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Change the weights randomly to see how they effect the performance. 
This is not scalable as there can be a lot of params and for each change a full forward pass has to be made inorder to calculate.&lt;/li&gt;
  &lt;li&gt;Randomly perturb all the weights to find the best set of weight values that seem to do the best. 
Again requires a large number of trials.&lt;/li&gt;
  &lt;li&gt;Randomly change the activations instead of weights and then figure out how the weights should change. 
Better because lesser neurons than weights but worse than backprop (wins by factor of number of neurons).&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;reading-assignment-learning-internal-representations-by-error-propagation-link1980backprop&quot;&gt;Reading assignment (Learning internal representations by Error Propagation) &lt;a href=&quot;http://www.cnbc.cmu.edu/~plaut/IntroPDP/papers/RumelhartETAL86.backprop.pdf&quot;&gt;link&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Neural networks are feature estimators on steroids. 
They work by trying and adding more features to the existing features. 
For example, consider XOR problem.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input&lt;/th&gt;
      &lt;th&gt;output&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;00&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;01&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;11&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The input patterns that differ the most are the ones that have the same output and hence it is hard to learn. 
By adding an additional feature to the input, we can hope to a better job at classification.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;input&lt;/th&gt;
      &lt;th&gt;output&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;000&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;010&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;111&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;The feature added is the disjunction of the first two dimensions and now we can see a pattern on input that maps to the same output.&lt;/p&gt;

&lt;p&gt;Minsky and Pappert (1969) apart from showing the limits of perceptron learning, they also showed that an input if when recoded with an internal representation (hidden layer) then an input-output mapping can be learned which could not have been possible without recoding. 
They also pointed that there is no general algorithm to learn internal representation at that time. 
Note that the genaeralized delta rule does not work with activation functions that are not differentiable.&lt;/p&gt;

&lt;p&gt;This chpater of a book discusses various other interesting problems that explain how neural networks work.&lt;/p&gt;

&lt;h3 id=&quot;parity-problem&quot;&gt;Parity Problem&lt;/h3&gt;

&lt;p&gt;This problem is discussed in Minky and Pappert and is solved here using generalized delta rule.
This class of problems are very similar to the XOR problems. 
The input patterns that look least similar share an output. 
One such problem is mapping of one for inputs with even/odd number of ones in it.&lt;br /&gt;
The networks learned by NN with backprop look like the image shown.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/parity-solution.png&quot; alt=&quot;parity-solution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As can be seen, the internal encoding contain bits that turn on if any input is on in the input, at least two inputs are on in the input and so on… The weights from hidden to output are varying in signs (positive, negative). In the image shown solid lines are positive and dotted are negative. This allows the network to recognize inputs with odd number of ones.&lt;/p&gt;

&lt;h3 id=&quot;symmetry-problem&quot;&gt;Symmetry Problem&lt;/h3&gt;

&lt;p&gt;The problem is to recognize if a string of fixed size is symmetric around the mid-point.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/symmetry-solution.png&quot; alt=&quot;symmetry-solution&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The network is beautiful that it barely needs an explanation. 
Observe that the weights for the above and below hidden units are mirror images (lateral inversion). 
The weights for the same hidden unit are alternate positive and negative. 
Also observe that the weights for the same hidden unit are in the ratio of 1:2:4 this helps to make sure that the correct bits are being compared. 
Finally, if you are wondering if a single hidden unit could solve the problem, it is not possible, at least not with the binary threshold activation function used. 
For example, for input: 101100 only the below unit responds and for “001101” the unit above responds.&lt;/p&gt;

&lt;h3 id=&quot;addition-problem&quot;&gt;Addition Problem&lt;/h3&gt;

&lt;p&gt;Given two 2-bit numbers, the output is a 3-bit number which is the addition of the two. 
The minimal network contains two hidden units that map to three output units. 
Each of hidden units function as carry-on bits; in general, for adding two n bit numbers that map to a n+1 bit output, n hidden units are required.
Learning over network with only two hidden units seldom leads (reliably) to a local minima solution that fails one or more cases. 
The authors attribute this to the latent ordering over the input which is that the output of middle bit depends on the carry on from the previous bits.
The second output bit depends on the first output bit and hence follows learning of first bit which means it does not influence the first bit learning. 
Because of which middle bit gets messed up more often due to loosing of carry on information.
The case of local solution occurs when the network untilises the hidden unit corresponding to higher bits when adding lower-order bits which makes the lower order hidden unit ignorant of carry-on. 
This causes error like “11”+”11”=”100” instead of “110”&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/minimal-add.png&quot; alt=&quot;minimal-add&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The problem can be avoided by using three units instead, but one looses the interpretability of the hidden output.&lt;/p&gt;

&lt;h3 id=&quot;negation&quot;&gt;Negation&lt;/h3&gt;

&lt;p&gt;Transformation of input such that all the bits are inverted if the sign bit is negative and will remain the same if the sign bit is positive. 
“011”-&amp;gt;”11”; “111”-&amp;gt;”00”
The minimal solution of the problem should adopt XOR for every bit with the sign bit and the solution acheives that.&lt;/p&gt;

&lt;h3 id=&quot;t-c-classification&quot;&gt;T-C classification&lt;/h3&gt;

&lt;p&gt;The task is to classify the shapes T and C.
This problem is again from Minsky and Pappert, the two shapes only differ in one cell and cannot be distinguished by only considering pairs of cells and requires consideration of triplets hence are of order 3.&lt;/p&gt;

&lt;p&gt;The text goes on to describe convolution networks, weight sharing concept, translational invariance (30 year old paper?! So ahead of its time).
Various detectors learned by the model are presented which exploit the compactness of C and protrusion difference between the networks.&lt;/p&gt;

&lt;p&gt;We really have waited for 30 years just so that computers can handle the computation.&lt;/p&gt;

&lt;h2 id=&quot;in-the-case-of-recurrent-nets&quot;&gt;In the case of recurrent nets&lt;/h2&gt;

&lt;p&gt;The chapter further discusses the relevance in the case of recurrent nets. 
Minsky and Pappert (again) showed that for any recurrent network there is an equivalent feed-forward network that has the same behavior in a finite amount of time.
In RNets, supervision is through periodic comparison of the outputs to the desired values.&lt;/p&gt;

&lt;p&gt;They have tried the weight learning on&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Shift Registers: where the input should be shifted by two units in the register after two time units. 
Let us say the input is a 8 bit vector with few bits on and it is expected that the on bits are moved by two units after two time units. 
Observe that the intermediate representation is not specified. 
For this reason, the intial biases are all set to 0 so that the intermediate transitions are not complex.
The system does learns to move input by one unit every time step and showed a strong affliation with left neighbour although initially connections between any of the output units is allowed.&lt;/li&gt;
  &lt;li&gt;Learning to predict next in the sequence: the task is to predict the next four numbers given first two characters. 
All the characters are correlated to codes they appear with, so a character deterministically gives the two digit numeric code. 
The specific test has 5 chars and 3 numbers. 
one of the two chars in the begin are shown hence input dimension: 5, output dimension: 3 and 30 hidden layers. 
They could successfully train RNets to do well over this task.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This heavily cited paper was an answer to the Minsky and Pappert critic.&lt;/p&gt;

&lt;h1 id=&quot;week-4-learning-to-predict-the-next-word&quot;&gt;Week 4 (Learning to predict the next word)&lt;/h1&gt;

&lt;p&gt;Q: &lt;em&gt;Why do we adopt one-hot encoding given that it is O(N) storage, but not a binary encoding which only takes log N bits?&lt;/em&gt;&lt;br /&gt;
A: Because then subset of input is linearly separable from any other disjoint set over the input.
Another reason is that binary coding mis-represents the input space and puts some input closer than others while in reality we have not apriori information.&lt;/p&gt;

&lt;hr /&gt;
&lt;p&gt;An interesting application of relational learning is discussed in the lecture which is to infer new or unspecified relations given a family tree. 
For example if the family tree mentions two people as the children with the same parents then they are also &lt;em&gt;brothers&lt;/em&gt; which is unspecified. 
It is hard to infer such relations through deduction over several rules.
Neural networks to rescue, if we recast the problem as trying to predict person2 given person1 and relationship.
In practice, we predict the distributional encoding of person2 given such encoding for person1 and relationship.&lt;br /&gt;
Learning of distributional encoding is where neural nets come in.
They learn to encode objects so that they represent useful features such as &lt;em&gt;nationality, generation, branch of the family tree&lt;/em&gt;.&lt;/p&gt;

&lt;p&gt;A large scale example: given a large database of facts (such tuples) find the ones that are unlikely.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;&lt;strong&gt;Debate on how concept is encoded in the brain (Cognitive Science)&lt;/strong&gt;&lt;br /&gt;
&lt;strong&gt;Feature Theory&lt;/strong&gt; A concept is encoded as vector of semantic features&lt;br /&gt;
&lt;strong&gt;Structuralist Theory&lt;/strong&gt; The meaning of a concept lies in its relationships to other concepts.&lt;br /&gt;
Geff argues that a feature of semantic vector representation can be used to implement a relational graph. 
&lt;strong&gt;The right way to implement relational knowledge in a neural net is still an open problem.&lt;/strong&gt;
Typically, a neauron can be involved in many concepts and every concept involved many neurons.&lt;/p&gt;

&lt;hr /&gt;

&lt;p&gt;Say we are dealing with logistic units with sequared error measure, in that case if the actual output is 1 in a million and desired output is 1, there is hardly any gradient because &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial E}{\partial z}=y*(1-y)*&lt;/script&gt;(finite number).
We are depriving the network of the world knowledge that the outputs should sum to one hence the softmax.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial y_i}{\partial z_i} = y_i*(1-y_i)&lt;/script&gt;

&lt;p&gt;This problem can be avoided by using right cost function: cross entropy &lt;script type=&quot;math/tex&quot;&gt;-\sum_{j}{t_j*log y_j}&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial C}{\partial z_i}=\sum_j \frac{\partial C}{\partial y_j}\frac{\partial y_j}{\partial z_i} = y_i-t_i&lt;/script&gt;

&lt;hr /&gt;

&lt;h2 id=&quot;reading-assignment-y-bengios-neural-language-model-linkbengio-language&quot;&gt;Reading Assignment (Y. Bengio’s Neural Language Model) &lt;a href=&quot;http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf&quot; title=&quot;A Neural Probabilistic Language Model&quot;&gt;link&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;A neural language model is presented that jointly learns to predict next word in the sequence with distributional representation of words. 
The insight is that the problem of unseen sequence is acute and can only be handled if a sentence that is already looked at can be generalized to exponentially many similar sentences.
“The cat got squashed in the garden on friday” is equivalent to “The dog flattened in the yard on monday”&lt;/p&gt;

&lt;p&gt;In this paper, a neural network with one hidden layer and skip layer connection to output from input that formulates a representation for each word such that it predicts the next word.
Softmax is used in the last layer to output probabilities which means summing over the entire vocabulary.
This is the reason why the system works best with tri-gram models so that the probabilities are only summed over candidates proposed by the n-gram model.&lt;/p&gt;

&lt;p&gt;Another method that is worth mentioning is Collobert and Weston, 2008 where they try to distinguish if a middle word looks right or random.
By just classifying if a word is right or random they have reformulated the problem as a binary classification one which is much easier to deal with.&lt;/p&gt;

&lt;h1 id=&quot;week-5-why-object-recognition-is-difficult&quot;&gt;Week 5 (Why object recognition is difficult.)&lt;/h1&gt;
&lt;h2 id=&quot;reading-assignment---1-gradient-based-learning-applied-to-document-recognition-linklecun-docrec-yet-to-be-read&quot;&gt;Reading Assignment - 1 (Gradient based learning applied to document recognition) &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf&quot; title=&quot;Gradient based learning applied to document recognition&quot;&gt;link&lt;/a&gt; [Yet to be read]&lt;/h2&gt;

&lt;p&gt;The performance difference between the train and test sets can be expressed as&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E_{test} - E_{train} = k(h/P)^\alpha&lt;/script&gt;

&lt;p&gt;where P is the number of training samples, h is the effective capacity and &lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is a real number between 0.5 and 1; k is a constant.
Note that ‘h’ is the effective capacity and when a network with a given capacity is trained with a regularizer, its effectove capactity increases smoothly.&lt;/p&gt;

&lt;h2 id=&quot;reading-assignment---2-convolutional-networks-for-image-speech-and-time-series-linkconv-nets&quot;&gt;Reading Assignment - 2 (Convolutional Networks for Image, Speech and Time-Series) &lt;a href=&quot;http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf&quot; title=&quot;Convolutional Networks for Image, Speech and Time-Series&quot;&gt;link&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Convolutional networks show some degree of shift and distortion invariance by local receptive fields.
They enable weight sharing (weight replication), and spatial or temporal sub-sampling.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;With local receptive fields, neurons can extract elementary visual features such as corners, edges, end-points.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Fixed size convolutional networks that share weights along a single temporal dimension are known as Time-Delay Neural Networks used in phoneme, spoken word recognition and online hand writing task.&lt;/p&gt;

&lt;h2 id=&quot;slides&quot;&gt;Slides&lt;/h2&gt;

&lt;p&gt;Object recognition is hard because there can be viewpoint changes, object in the image can be transformed in non-affine ways, pixel intensities may change and finally, viewpoint changes (translational variance); change in viewpoint makes the object appear in different locations&lt;/p&gt;

&lt;p&gt;Viewpoint variation can be handled either through&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;invariant features – features that are redundant under transformations. (SIFT, SURF and so on…)&lt;/li&gt;
  &lt;li&gt;Put a box around the object – normalization approach.&lt;/li&gt;
  &lt;li&gt;Feature sharing followed by averaging. Convolution + Pooling&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Convolution nets can also be replicated across scale and orientation (not just position) but that is more complicated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Replicated feature detectors do not achieve translational invariance but equivariance.&lt;/strong&gt;
We are acheiving equivariance in neural activity and invariance in knowledge. 
That is the neural activity is same no matter where in the image the object appears (equivariance) and the feature is detected irrespective of where it appeared (knowledge invariance)&lt;/p&gt;

&lt;p&gt;We can get a small amount of translational invariance by averaging over four neighbouring replicated detectors.
That is, the neural activity or the feature map will remain the same in the entire region of pooling/averaging.
Because of pooling we loose precise spatial relationships between high-level parts, that is we may recognize that the image contains few eyes, nose and mouth etc. but to recognize whose face, precise spatial locations of the individual objects is required.&lt;/p&gt;

&lt;p&gt;Ways of including prior knowledge:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Neural connectivity&lt;/li&gt;
  &lt;li&gt;Weight constraints&lt;/li&gt;
  &lt;li&gt;Neuron activation functions&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Forces the network to learn what we have in mind (prejudice); Alternatively we can use our prior knowledge to create whole lot more training data. (Hofman &amp;amp; Tresp ‘93).
Lenet uses local connectivity, weight sharing and pooling to inform knowledge about invariance. 
LeNet 80 errors on 10,000; Ranzatop 2008 40 errors by including lot of transforms over input.
Ciresan et.al. 2010 by lots of synthetic data =&amp;gt; can use larger network on GPU without over-fitting; 35 errors. with model averaging 25 errors.&lt;br /&gt;
McNemar test for comparing image classification methods.&lt;/p&gt;

&lt;p&gt;AlexNet entry for ImageNet 2012 had a much smaller error rate 16% compared to the next best, 26%. 
The rest of the entries in ImageNet came from Computer Vision labs.
AlexNet had 7 layers (not including convolution layers), the early layers were all convolutional.
The last two layers were globally connected. 
The last layers look for combination of features generated in earlier layers and because there are combinatorially many such, they contain a large number of parameters.
It used ReLU activation units – train much faster and more expressive than logistic.&lt;/p&gt;

&lt;p&gt;Competitive Normalization is another technique that is generally used to help with variations in intensity.
Suppresses hidden activities when nearby units have stronger activities. 
For example, a blurry edge in the midst of high intensity - feature rich neighbourhood. 
Trained on 224*224 patches in 256*256 images, 4 corner patches+central patch + patches from left-right reflection =&amp;gt; 10 patches per example. 
Dropout regularization.&lt;/p&gt;

&lt;p&gt;Dimension hopping is why we need conv. layers. 
A dataset is said to be doing this if the input or the features hop the sub-space and still have the same input. 
For example, consider the case where we want to predict the risk of heart attack given age, weight, family history etc. 
If the weight and age fields are swapped for some input entries, we don’t expect them to have same label as the entries that are not swapped (?!)&lt;br /&gt;
Consider if we are trying to recognize a pattern in an image or wave-form, then the ink bloat or wave amplitudes can shift/translate and still have the same label. 
This is dimension hopping.&lt;/p&gt;

&lt;h2 id=&quot;good-questions-from-quiz&quot;&gt;Good questions from quiz&lt;/h2&gt;

&lt;p&gt;Brian mangled the digits dataset and cannot be repaired, but the damage is caused with the same pattern on both the train and test. 
For example, pixel i swapped with pixel j (and for several other pixels) in both.
This puts Conv. Nets with small weight windows at disadvantage because they may not be able to capture any regularities such as loop or stroke or edge when the data is mangled like this.
Note that conv. nets that span the entire image-size or fully connected layers may stll solve the problem.&lt;/p&gt;

&lt;h2 id=&quot;programming-assignment&quot;&gt;Programming assignment&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Do not assume that when the initial weights are all set to zero, then they are going to remian there.
They will remain all equal but not zero. 
The gradients will flow through bias one iteration at a time.
In the first iteration, the bias in the last layer will move away from 0 because it is just the sum of error derviative in the next layer over the entire batch.
That makes the state of last but one layer non-zero in the next iteration which will make the gradients move.&lt;/li&gt;
  &lt;li&gt;&lt;del&gt;The model trained in this assignment unlike CBOW or skip-gram, does not put words in similar context closer&lt;/del&gt;
Two words are put closer if one can be replaced by other and still form a valid 4-gram.
Tail words that appear less than 10 times, say, barely get any update and hence will be closer to their initialized values. This may lead to wrong inference that they are similar.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;With 50 dimensional embedding and 200 width hidden layer, after 5 epochs&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;momentum&lt;/th&gt;
      &lt;th&gt;train CE&lt;/th&gt;
      &lt;th&gt;validation CE&lt;/th&gt;
      &lt;th&gt;test CE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;3.986&lt;/td&gt;
      &lt;td&gt;3.944&lt;/td&gt;
      &lt;td&gt;3.947&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;3.328&lt;/td&gt;
      &lt;td&gt;3.254&lt;/td&gt;
      &lt;td&gt;3.252&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.714&lt;/td&gt;
      &lt;td&gt;2.716&lt;/td&gt;
      &lt;td&gt;2.725&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;After 10 epochs&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;embed size&lt;/th&gt;
      &lt;th&gt;hidden width&lt;/th&gt;
      &lt;th&gt;train CE&lt;/th&gt;
      &lt;th&gt;val CE&lt;/th&gt;
      &lt;th&gt;test CE&lt;/th&gt;
      &lt;th&gt;time(s)&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;2.811&lt;/td&gt;
      &lt;td&gt;2.829&lt;/td&gt;
      &lt;td&gt;2.839&lt;/td&gt;
      &lt;td&gt;775&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3.014&lt;/td&gt;
      &lt;td&gt;3.027&lt;/td&gt;
      &lt;td&gt;3.024&lt;/td&gt;
      &lt;td&gt;644&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;50&lt;/td&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;2.535&lt;/td&gt;
      &lt;td&gt;2.604&lt;/td&gt;
      &lt;td&gt;2.611&lt;/td&gt;
      &lt;td&gt;27,953&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;3.231&lt;/td&gt;
      &lt;td&gt;3.236&lt;/td&gt;
      &lt;td&gt;3.232&lt;/td&gt;
      &lt;td&gt;708&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Momentum=0.5, after 1 epoch; 50, 200 hidden layer widths&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;learning rate&lt;/th&gt;
      &lt;th&gt;train CE&lt;/th&gt;
      &lt;th&gt;valCE&lt;/th&gt;
      &lt;th&gt;test CE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;5.296&lt;/td&gt;
      &lt;td&gt;5.089&lt;/td&gt;
      &lt;td&gt;5.092&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;4.436&lt;/td&gt;
      &lt;td&gt;4.385&lt;/td&gt;
      &lt;td&gt;4.392&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3.544&lt;/td&gt;
      &lt;td&gt;3.416&lt;/td&gt;
      &lt;td&gt;3.413&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Momenum=0.5, after 10 epochs; 50, 200 hidden layer widths&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;learning rate&lt;/th&gt;
      &lt;th&gt;train CE&lt;/th&gt;
      &lt;th&gt;valCE&lt;/th&gt;
      &lt;th&gt;test CE&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;4.380&lt;/td&gt;
      &lt;td&gt;4.382&lt;/td&gt;
      &lt;td&gt;4.388&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;2.934&lt;/td&gt;
      &lt;td&gt;2.924&lt;/td&gt;
      &lt;td&gt;2.923&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;3.358&lt;/td&gt;
      &lt;td&gt;3.311&lt;/td&gt;
      &lt;td&gt;3.318&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;week-6-mini-batch-gradient-descent-and-learning-rate&quot;&gt;Week 6 (Mini-batch gradient descent and learning rate)&lt;/h1&gt;

&lt;p&gt;The desirable descent or ascent in the case of gradient descent is that we want to move quickly in the directons with low gradient and slow in the directions with high gradient (going faster in this direction can change the sign of gradient)
I can’t help but mention it is just like mountain climbing.&lt;/p&gt;

&lt;p&gt;Stchastic gradient descent (online learning) and mini-batch learning are of utility when there is a lot of data redundancy.
More efficient if mini-batches are balanced for classes. 
If one batch contains one class only and the next the other then that would cause updates to slosh unnecessarily.&lt;/p&gt;

&lt;p&gt;An indicator that the learning rate can be turned down is when the validation error stops decreasing consistently.&lt;/p&gt;

&lt;p&gt;Tricks and recommendations for mini-batch gradient descent:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Initialisation: If a hidden unit has big fan-in, then small changes on many of its incomming weights can cause the learning to overshoot. 
We generally want small incomming weights when fan-in is big, so initialize &lt;script type=&quot;math/tex&quot;&gt;\propto&lt;/script&gt; sqrt(fan-in)&lt;/li&gt;
  &lt;li&gt;Shifting the input: transforming the input such that it is zero mean helps a big way.
It simplifies the error surface.&lt;br /&gt;
&lt;img src=&quot;/assets/images/nn-notes/error-shen-shift.png&quot; alt=&quot;Error surface when we shift&quot; /&gt;&lt;br /&gt;
As shown in the figure, for the two inputs to be satisfied when not shifted have the parabolic troughs lying on &lt;script type=&quot;math/tex&quot;&gt;(w_1*101+w_2*102)&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;(w_1*101+w2*99)&lt;/script&gt; whose intersection we are interested.
The two lines which are almost parallel look a lot different when a constant is subtracted from the input such that the inputs sum to one.&lt;br /&gt;
In this respect, hyperbolic tangent is better than logistic because it produces outputs that roughly sum to 0.
But, logistic is better in other aspects, it gives you “rug to sweep things under” by being robust to fluctuations in the input because of saturation whereas for hyperbolic you have to go to the end of its plateu before it can ignore anything.&lt;/li&gt;
  &lt;li&gt;Scaling the inputs: this has the same effect of making the error surface circular. 
One way of doing this is to make the variance of each of the input components one. 
More sensible thing to do is, decorrelate the inputs (correlated inputs can also increase the training time) by doing PCA, remove components that correspond to low eigen values and divide the rest by the square root of eigen value.&lt;br /&gt;
&lt;strong&gt;In the case of circular error surface, the gradient points straight towards the minimum&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Confusing plateu with local minimum:&lt;/strong&gt; Both the cases below can be confused with local minimum.
    &lt;ul&gt;
      &lt;li&gt;When we start with very big learning rate then the weights will either become big and negative or +ve leading to saturation. 
When saturated, the error derivatives go to zero&lt;/li&gt;
      &lt;li&gt;In a multi-layer networks, the network quickly learns to set the outputs to equal the proportion of times it should be one and may take much longer to improve on it. 
Which may look as if the objective function has plateaued&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Do not turn down the learning rate too soon because that may smooth random fluctations between the batches too much that it can plateau after some time.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tricks-to-speed-up-training&quot;&gt;Tricks to speed up training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Momentum trick: Most common trick for training large neural nets. 
In this metod, we change the velocity instead of position.
The intuition is that often the gradient has two components: one that goes down the ravine and other that goes along. 
It is not desired to go across the ravine and momentum method helps build velocity along the ravine while cancelling the changes across. &lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;v(t)= \alpha * v(t-1) - \epsilon*\frac{\partial E}{\partial w}(t)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\delta w(t) = v(t)&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\alpha&lt;/script&gt; is really viscocity but we call it momentum.
When the surface is a tilted plane then it reaches a terminal velocity where slow down due to viscosity will balance the pull downward.
A momentum that is close to one can multiply the speed by a large amount, the expression for momentum increase is: &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{1-\alpha}&lt;/script&gt;.&lt;br /&gt;
Using just a large learning rate alone may just cause divergent oscillations. 
Momentum on the other hand have larger velocity in directions of consistent gradient.&lt;br /&gt;
Nestorev (‘83) method to improves the momentum method.
In the momentum method, we measure the gradient at this location and add it to the accumulated gradient. 
Nesterov method: make a big jump in the direction of accumulated gradient, measure the gradient at the new location and then move in that direction.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Jacobs in 1980, idea is each connection in net has its own adaptive learning rate which is set empirically.
Gradients in the initial layers are much smaller than the later ones, especially if the initial weights are small.
If a layer has large fan-in, the chances of over-shoot are high by changing the values of incoming weights.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\delta w_{ij} = - \epsilon g_{ij}\frac{\partial E}{\partial w_{ij}}&lt;/script&gt;&lt;br /&gt;
if the previous and the current gradient are of same sign then&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;g_{ij}(t) = g_{ij}(t-1)+.05&lt;/script&gt; else&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;g_{ij}(t) = g_{ij}(t-1)*0.95&lt;/script&gt;&lt;/p&gt;

    &lt;p&gt;The gains should all be started with one, which ensures that when gradients alternate in signs randomly then the value hovers at one instead of converging or diverging.
It is a good idea to cap gains between lower and upper bounds.&lt;br /&gt;
Better use it with full-batch so that fluctuations of mini-batch does not mis-lead.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;rprop and rmsprop: extension of Jacobs for mini-batch learning. 
rmsprop is Hintons favorite.&lt;br /&gt;
rprop: The magnitude of gradient can be very different for different weights and can change during learning, what if w eonly look at signs and make a step in the correct direction based on that.
Step size for each weight needs to be updated for each weight.
Increase/decrease the step size multiplicatively by 1.2 or 0.5 depending on if the last two gradients agree in sign or not.
Advice: limit the step sizes: [50, 1E6]
This has the same ill effects of fluctuating gradients, stochastic gradient descent works by averaging over many updates but this method does not.&lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Enters rmsprop which is the mini-batch version of rprop.
This is a lot different from rprop actually, does not have adaptive learning rate for each weight or not worrying about the gradient value.
It works by keeping a moving average of gradient at every iteration.
The problem with mini-batch is that we divide by a different number for each mini-batch, why not make the number we divide with be close for successive batches.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;MeanSquare(w,t)=0.9*MeanSquare(w,t-1)+0.1*(\frac{\partial E}{\partial w}(t)^2)&lt;/script&gt;&lt;br /&gt;
Then dividing the gradient by root mean square makes learning much better.
It works as well as gradient descent with momentum&lt;br /&gt;
Susketeer (2012) unpublished work combined rmsprop with Nestorev method to good results.&lt;br /&gt;
Yan Lecun’s: “No more pesky learning rates”&lt;/p&gt;

    &lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/slide-lat-lecture6.png&quot; alt=&quot;Useful summary&quot; /&gt;&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;week-7-training-recurrent-neural-networks&quot;&gt;Week 7 (Training Recurrent Neural Networks)&lt;/h1&gt;
&lt;p&gt;Neural Networks for sequences. 
Teaching signal is through trying to predict the next input and blurs the difference between supervised and unsupervised learning. 
The target is the input advanced by one step.
Below  we will summarize existing models for sequences&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Auto regressive models (Memory-less): predict the next input from k previous inputs. 
In Linear auto-regressor, it is just weighted average of previous inputs.&lt;/li&gt;
  &lt;li&gt;Feed forward nets: generalize above by introducing hidden units and input to which is from the k previous inputs.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We move beyond memory-less models by having the hidden state encode some information about what is seen already.
There are two widely used models of this sort&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Linear Dynamical systems (popular with Engineers): Remember the vehicle localzation technique based on measurements from various noisy sources with Kalman Filters.
It is assumed that the vehicle is moving with constant velocity (without acceleration) in a certain direction between measurements. 
The hidden state in this case is the actual position of the vehicle which has (assumed) linear dynamics. 
A linearly transformed Gaussian is a Gaussian again, so the distribution over hidden state given the data so far is Gaussian.&lt;/li&gt;
  &lt;li&gt;Hidden Markove Model on the other hand have discrete one of n states. It is popular with computer scientists probably due to the discreteness of states. 
The transitions between the states is probabilistic and decided by transition matrix.
Given an output, we cannot say which hidden state produced it and can only comment on distribution of possible states hence the name hidden.
To predict the next output, we need to infer the prob. distribution over the hidden states.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Linear Dynamical systems and HMM are stochastic. 
Recall that the transition from HMM state A to B is a draw from transition matrix. 
The posterior probability on the states that generated an output is deterministic, though.
RNN models are deterministic, one can think the hidden states of RNN to be equivalent of deterministic probability distribution over hidden states ina linear dynamical system or hidden markov model.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Limitation of HMM&lt;/strong&gt; one of the hidden state should be selected to generate the output. 
	If there are N hidden states then information equivalent of log(N) bits is coded about what is generated so far. 
    For example, there are several bits of information that are to be stored about a speaker when we are trying to transcribe a sentence.
	We shouild make sure that the various attributes such as intonation, semantics, syntactics etc. match between the first and second half. 
	If there are 300 possible syntactic forms, 100,000 semantic types and 1,000 different voice type+intonation combinations possible, then we need 300*100000*1000 hidden states.&lt;/p&gt;

&lt;p&gt;RNNs to the rescue&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Unlike HMM, they have can several active hidden states, which mean several states can remember several bits of information at the same time.&lt;/li&gt;
  &lt;li&gt;Unlike Linear Dynamic systems, they allow hidden states to update in complicated ways.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;It is required in the case of RNNs the initial states of all the hidden and output states need to be specified. 
There is an alternative, though, if some or none of the states are specified then it is possible to backprop and learn the initial states as well.&lt;/p&gt;

&lt;p&gt;Feeding input to RNN (this is tricky, make sure you understand)&lt;br /&gt;
Imagine a grid of states with inter-connections between layers. 
These are several clones of the network, one can provide input in one of these ways:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;initial states of all the nodes are specified.&lt;/li&gt;
  &lt;li&gt;initial state of some subset of the nodes are specified&lt;/li&gt;
  &lt;li&gt;state of a subset of nodes can be specified at every time step. This is more natural way in the case of sequence data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Similarly, there are ways of providing target.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Specify desired final activities of all the units&lt;/li&gt;
  &lt;li&gt;Specify desired activities of all units for the last few steps – good for learning attractors&lt;/li&gt;
  &lt;li&gt;Specify the desired activity of a subset of the units.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;If we are trying to predict the next in the sequence, then initial states of subset of nodes can be specified and desired activation of the rest can be specified to the target.
Specifying initial states is like feeding a feed-forward network with the input.&lt;/p&gt;

&lt;h2 id=&quot;training-rnn----backprop&quot;&gt;Training RNN – backprop&lt;/h2&gt;
&lt;p&gt;RNN and feed forward nets are equivalent.
RNN is just a layered net that keeps using the same weights. 
We can just do normal backprop as in feed-forward and sum all the gradients to keep the weights same.&lt;/p&gt;

&lt;p&gt;This is followed by a discussion how input and target be specified to RNN which I could not follow.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Toy Example&lt;/strong&gt; is choosen such that it is hard to deal with feed-forward and is good only with RNN.
We can try FFN which takes as input two binary numbers and produce the output, but&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;maximum length of the binary vector should be pre-defined and&lt;/li&gt;
  &lt;li&gt;learning does not generalize from one position in the vector to the other.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;The RNN employed to solve this problem has a hidden layer with three nodes and an output layer. 
The input at any instant are the two bits corresponding to the operands.
The output of RNN is the result of addition of the input two time steps ago, the lag of two steps: one time step for input to hidden and another from hidden to output.&lt;br /&gt;
Finite state automata for this problem will have 4 states: cross of 2 possible output (1,0) and carry and no-carry. 
FSAs are constrained to be in one state at every time step.&lt;br /&gt;
Our RNN will have 4 different activation pattern corresponding to 4 states of FSA.
With N hidden neurons, there are &lt;script type=&quot;math/tex&quot;&gt;2^N&lt;/script&gt; possible binary activity vectors, but there are only N^2 weights (which limit the representational capactiy)&lt;/p&gt;

&lt;h2 id=&quot;why-is-training-rnn-difficult&quot;&gt;Why is training RNN difficult&lt;/h2&gt;
&lt;p&gt;Backpropagation in general irrespective of the activation function behaves like a linear system.
Once we fix the output values and consider their gradient the output is no longer bounded.
This leads to explosion or lead to dying of the weights especially is we are back-prop-ing across many layers.
Another difficulty with RNN is to capture teh long range dependencies, how do you output now depends on input k time steps ago.
4 ways of dealing with this.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM – Make RNNs ouyt of memory modules that are designed to remember for long times.&lt;/li&gt;
  &lt;li&gt;Hessian free optimization&lt;/li&gt;
  &lt;li&gt;Echo state networks and&lt;/li&gt;
  &lt;li&gt;Good initialization with momentum.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;long-term-short-term-memory&quot;&gt;Long term short term memory&lt;/h2&gt;
&lt;p&gt;Hochreiter &amp;amp; Schmidhuber (1997) demonstrated the remembrance for 100 steps.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/lstm.png&quot; alt=&quot;Useful summary&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Whenever the data is read from the memory cell and error that is caused by that is back-propagated all the way to time step where the cell is written to.&lt;/p&gt;

&lt;h2 id=&quot;interesting-questions-from-quiz&quot;&gt;Interesting questions from Quiz&lt;/h2&gt;

&lt;p&gt;How many bits of information can be modeled by the vector of hidden activities (at a specific time) of a Recurrent Neural Network (RNN) with 16 logistic hidden units?&lt;br /&gt;
Initially, I just gave my answer as 16, but there’s more to it. 
If there are only two possible asctivation values: 1 and 0 say, then the answer is correct. 
Since it is a logistic hidden unit, let us say there are three values: close to 1, close to 0 and close to 0.5 – in which case the answer would be log(16*log3), where the base is 2. 
In reality, the values are continuous and the amount of information is much more.&lt;/p&gt;

&lt;p&gt;Consider the network below:&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/quiz7-q5.png&quot; alt=&quot;Trouble-network&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This is the case where the gadient problem occurs in RNN.
Let us say is inputs at t=1,2,3,4 are 1,0,0,0 and output is seen only at t=4. 
The gradient with respect to &lt;script type=&quot;math/tex&quot;&gt;W_{xh}&lt;/script&gt; includes the product like &lt;script type=&quot;math/tex&quot;&gt;h_1*(1-h1)&lt;/script&gt; 4 times for each of the hidden units, that is eight terms less than 1 which leads to a diminishing gradient.&lt;/p&gt;

&lt;h1 id=&quot;week-8-more-rnns&quot;&gt;Week 8 (More RNNs)&lt;/h1&gt;

&lt;p&gt;In this lecture, we see how RNN can be used to modeling text.&lt;br /&gt;
Modeling on character level is better than on word level for several reasons. 
A learning method that is powerful enough should understand what strings make up words.
Dealing with words can have the problems with: (1) Morphemes, variations of one word (2) Words that should go together like New York (3) Some languages like Finnish can contain several morphemes in one single word which makes canonicalization hard.
Also, it is a lot easier to predict one of 86 characters than 100,000 words.&lt;/p&gt;

&lt;p&gt;Just like any other RNN, each of the hidden units takes as input the activations in the previous time steps and the current input (character).
To understand the power of RNN in language modeling, imagine you are trying to predict the next character based on a tree traversal as shown in the image below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/lec8-tree.png&quot; alt=&quot;tree-like-model&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The traversal is dependant on characters seen so far or in a time window and the current character.
The tree grows exponentially with branching factor of 86 (# chars).
In an RNN or any NN for that matter, each of the node is equivalent to hidden state vector.
This is a game changer because it reduces the complexity of representation and inference and because of distributed representation, different nodes can share structure.
For example, in the image above if it the tree learns that &lt;em&gt;fixin&lt;/em&gt; is &lt;em&gt;fixi&lt;/em&gt; is the current state because &lt;em&gt;fix&lt;/em&gt; is a verb and it is likely to have an &lt;em&gt;ing&lt;/em&gt; form.
This knowledge is not shared across all the verbs; which is not the case when we adopt a distributed representation.&lt;br /&gt;
Since we require the hidden-to-hidden weights to depend on the character input, one way of doing it is to use 86 different transition matrices which leads to 86x1500x1500 parameters (1500 is the number of hidden nodes). 
To avoid the explosion in number of parameters, we need a different transition matrix that also takes the current input into consideration.
Also, we want to share parameters amoung character inputs that are similar that is transition matrices for characters 9 and 8 would be very similar.&lt;br /&gt;
For this reason, we use factor model.
The idea is that there are several factors and each of them take two inputs: the previous hidden state vector and the current input. 
The weighted sum of both the inputs is computed and multiplied with the outbound weights that contribute to the hidden state vector in the next time step.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;c = \sum_{f} {(b^T w_f)(u_f * v_f^T)a}&lt;/script&gt;

&lt;p&gt;where c is the hidden state vector in the next time step and a the state vector in this time step, b is the current input.
&lt;script type=&quot;math/tex&quot;&gt;u_f, v_f, w_f&lt;/script&gt; are the weights connecting current state, next state and the current input respectively.&lt;br /&gt;
The number of parameters in such a model is F*(two times the number of hidden nodes + size of the input vocabulary), so things are more manageable.&lt;/p&gt;

&lt;p&gt;The reading assignment of this lecture is about a work that made an RNN that predicts the next character.
The RNN model trained on 5 million strings with 100 chars each from Wikipedia on multiple GPUs for 5 days and optimized with HF optimizer.
For each string, it starts predicting from the 11th character.&lt;br /&gt;
The model does a great job at:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;producing the correct words almost all the time and when it produces a wrong word, it is more or less sensible.&lt;/li&gt;
  &lt;li&gt;it can match quotes, brackets etc. infact it can count and keep track of them.&lt;/li&gt;
  &lt;li&gt;it has a very good syntactic knowledge, but hard to pin how the knowledge is represented.&lt;/li&gt;
  &lt;li&gt;knows awful lot about semantic associations, for example: cabbage and vegetable, milk and cow. 
One problem, is that it produces sentences with such associations, but they does not always make sense. 
That it milk after cow does not always make sense.&lt;/li&gt;
  &lt;li&gt;knows a lot about proper names, dates and numbers.
To the extent that it generated a lot of proper names not seen in the training data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;When promped for next character in &lt;em&gt;Sheila thrunge&lt;/em&gt;, it gives &lt;em&gt;s&lt;/em&gt; and for &lt;em&gt;People thrunge&lt;/em&gt;, it gives *&lt;space&gt;* meaning that it can differentiate bewtween singular and plural, although thrunge is not even an english word.  
It knows about capitalization: *Shiela, Thrungelini del Rey*&lt;/space&gt;&lt;/p&gt;

&lt;p&gt;Tomas Mikolov and co are working on predicting the next word with embeddings and RNN (which do better than feed-foward networks)&lt;br /&gt;
“RNNs require much less training data to reach the same level of performance as other models.”
Other models as in n-gram models.
According to him, this will make them hard to beat.&lt;/p&gt;

&lt;h2 id=&quot;echo-state-networks&quot;&gt;Echo state networks&lt;/h2&gt;
&lt;p&gt;I do not completely understand these, on one level they make no sense to me. 
The echo state networks (ESN) have a carefully set input-&amp;gt;hidden and hidden-&amp;gt;hidden weights. 
The hidden-&amp;gt;hidden weights are set such that the length of the activity vectors stays about the same after each iteration. 
This allows the input to echo around the network for a long time.&lt;br /&gt;
Use sparse connectivity: creates a lot of loosely coupled oscillators.&lt;br /&gt;
The scale of input-&amp;gt;hidden weights is set carefully such that “They need to drive the loosely coupled oscillators without wiping out the information from the past that they already contain. “&lt;/p&gt;

&lt;p&gt;ESNs are fast because they just need to set the hidden-&amp;gt;output weights which is linear and can do impressive modelling of one dimensional time series. 
For example, given a signal of the frequency wave, it can generate the sine wave with the input frequency.&lt;/p&gt;

&lt;p&gt;Ilya Sutskever (2012) using rmsprop with momentum showed that if and when ESNs can be used to also back-prop to the hidden-&amp;gt;hidden weights they can be trained very effectively.&lt;/p&gt;

&lt;h2 id=&quot;reading-assignment-generating-text-with-recurrent-neural-networks&quot;&gt;Reading Assignment (Generating Text with Recurrent Neural Networks)&lt;/h2&gt;

&lt;p&gt;RNNs have unstable relationship between dynamics of hidden states and weights which lead exploding or diminishing gradient.
This led to surprisingly low interest in RNNs.
Hessian-free optimization in the case of deep neural networks provided a solution to this problem which is extended to the case of RNNs in &lt;a href=&quot;http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf&quot; title=&quot;Learning Recurrent Neural Networks with Hessian-Free Optimization&quot;&gt;Martens &amp;amp; Sutskever 11&lt;/a&gt; with a novel damping mechanism.&lt;/p&gt;

&lt;p&gt;The goal of this work is to train a character level language model so as to predict the next character and in better compression of text.
Better compression beyond a ceretain point is possible only through deeper understanding of text’s meaning.
The final model learned demonstrated deep knowledge about the meaning of text as discussed in the lecture.&lt;/p&gt;

&lt;p&gt;Few ways to combat the back-prop fiasco:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;LSTM&lt;/li&gt;
  &lt;li&gt;HF optimization&lt;/li&gt;
  &lt;li&gt;Echo state networks.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In a typical RNN, the sum of weighted sum of previous hidden state and weighted sum of input generates the input to the next state. 
This model did not perform well because the hidden transition matrix is independent of input.
They proposed: Multiplicative RNN with new temporal arch.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;The diffculty in learning weights&lt;/strong&gt;
The effective weight &lt;script type=&quot;math/tex&quot;&gt;W_{ij}^{(c)}&lt;/script&gt; the weight from hidden layer i to j given the input c is given by &lt;script type=&quot;math/tex&quot;&gt;\sum_f{W_{if}W_{fc}W_{fj}}&lt;/script&gt;.
If for example, &lt;script type=&quot;math/tex&quot;&gt;W_{if}&lt;/script&gt; is very small and &lt;script type=&quot;math/tex&quot;&gt;W_{fj}&lt;/script&gt; very small then we may have large gradient for small value and small gradient for large value.
This can be handled by HF optimizers with second order derivatives.
This work uses them for this reason.&lt;/p&gt;

&lt;p&gt;The character level language model seems unnecessary since we know that morphemes are appropriate unites to make semantic and syntactic predictions.
Converting a large text with words into smaller chunks of morphemes is non-trivial.&lt;/p&gt;

&lt;h2 id=&quot;hessian-free-optimization-optional-lecture-material&quot;&gt;Hessian-free optimization (optional lecture material)&lt;/h2&gt;

&lt;p&gt;How far to move in the direction ogf gradient before the vaklue starts rising again. 
It depends on the curvature, we generally assume uniform curvature (quadratic surface).&lt;br /&gt;
A good direction to move in is one with a high ratio of gradient to curvature, even if the gradient itself is small.&lt;/p&gt;

&lt;p&gt;The direction of gradient is fine on  cicular cross section so lets remove the curvature by Newton’s trick.
Newton’s method of finding such directions is multiplication with inverse of curvature matrix.
On a real quadratic surface it jumps to the minimum in one step. 
Unfortunately, with only a million weights, the curvature matrix has a trillion terms and it is totally infeasible to invert it.&lt;/p&gt;

&lt;p&gt;The off-diagonal terms in curvature matrix correspond to twists in error surface.
There are tricks to avoid inverting large hessian matrix:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;What if we just use the terms on the main diagonal i.e. self interactions – Le Cun&lt;/li&gt;
  &lt;li&gt;Approximation methods: Hessian-free methods and LBFGS&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hessian free methods use a minimization technique called: conjugate descent.
We do not make one-shot minimization, but minimize iteratively.&lt;br /&gt;
“Use a sequence of steps each of which finds the minimum alongone direction. “&lt;br /&gt;
“Make sure that each new direction is “conjugate” to the previous directions so you do not mess up the minimization you already did.”&lt;/p&gt;

&lt;h2 id=&quot;regularization-in-rnns&quot;&gt;Regularization in RNNs&lt;/h2&gt;
&lt;p&gt;There has been considerable amount of effort put in to making RNNs learn long range dependencies.&lt;br /&gt;
To handle the vanishing and exploding gradients in RNN, tensorflow uses the technique of &lt;code class=&quot;highlighter-rouge&quot;&gt;clip by global norm&lt;/code&gt; proposed by T. Mikolov and co. in http://arxiv.org/pdf/1211.5063.pdf.
The technique itself is quite simple: &lt;code class=&quot;highlighter-rouge&quot;&gt;t_list[i] * clip_norm / max(global_norm, clip_norm)&lt;/code&gt; where t_list is the tensor supplied.
&lt;code class=&quot;highlighter-rouge&quot;&gt;clip_norm&lt;/code&gt; is a supplied parameter and global norm is the computed norm of t_list.&lt;/p&gt;

&lt;h1 id=&quot;week-9-ways-to-make-neural-networks-generalize-better&quot;&gt;Week 9 (Ways to make neural networks generalize better)&lt;/h1&gt;

&lt;p&gt;Overfitting happens because there can be sample error in the training
sample due to the way the data is sampled. This will reduce generalization.&lt;/p&gt;
&lt;h2 id=&quot;overview-of-ways-to-improve-generalizaton&quot;&gt;Overview of ways to improve generalizaton&lt;/h2&gt;
&lt;h3 id=&quot;get-more-data&quot;&gt;Get more data&lt;/h3&gt;
&lt;p&gt;A best bet if more data is available.&lt;/p&gt;
&lt;h3 id=&quot;use-a-model-that-has-the-right-capacity&quot;&gt;Use a model that has the right capacity&lt;/h3&gt;
&lt;h3 id=&quot;average-many-different-models&quot;&gt;Average many different models&lt;/h3&gt;
&lt;p&gt;Models with different forms will make different mistakes making the
average better.&lt;br /&gt;
Train the model on different subsets of training data called
“bagging”.&lt;/p&gt;
&lt;h3 id=&quot;bayesisan-approach&quot;&gt;Bayesisan approach&lt;/h3&gt;
&lt;p&gt;Use a single neural network architecture, but average the predictions
made by many different weight vectors.&lt;br /&gt;
Bayesian approach can come in handy when there is very less training
data and basically does it by introducing priors.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Limiting model capacity&lt;/strong&gt;&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Architecture: limit number of hidden layers or their width&lt;/li&gt;
  &lt;li&gt;Early stopping: Start with small weights and stop learning before
overfits&lt;/li&gt;
  &lt;li&gt;Weight decay: Penalize large weights with L2 or L1 penality. It is
referred to as weight penality because the penality acts like a
force that is pulling the weights towards zero.&lt;/li&gt;
  &lt;li&gt;Noise: Add noise to the weights or activity.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Imagine a neural network starting with small weights and logistic
hidden units, when the weights are near 0, the inputs function like a
linear unit. This reduces the effective capacity of the model. 
As training happens, more hidden units evolve towards non-linear. 
The model capacity smoothly transitions from low to high as it is
learned.
The network with smaller weights is simpler than the one with larger
one is the basis for Early stopping and weight decay.&lt;/p&gt;

&lt;h3 id=&quot;weight-decay&quot;&gt;Weight decay&lt;/h3&gt;
&lt;p&gt;L2 weights improve generalization a lot because it prevents the
network from using the weights that are not needed. 
It creates a smoother model in which the output changes more slowly
with the input. 
&lt;strong&gt;If the network has two similar inputs, then it tends to put equal
weight on them rather than unsymmetrical w,0 like.&lt;/strong&gt;&lt;br /&gt;
L1 penality sets can make many weights equal to zero which helps in
interpretation.&lt;br /&gt;
Sometimes it is better if we only try to pull small weights but have negligible effect on large weights.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Weight penality vs Weight constraint&lt;/strong&gt;&lt;br /&gt;
Weight constrainsts are a way of saying that the weights should not
explode beyond certain threshold. 
It does hold on to weights from growing and avoids explosion.
It is easier to set a sensible value for it. 
When the weights exceed the limit, then all the weights are scaled
determined by the big gradient.&lt;br /&gt;
This is more effective than a fixed penalty at pushing the irrelevant
weights to zero.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Setting parameters on training set alone is insufficient&lt;/strong&gt;&lt;br /&gt;
For this reason, we divide the data in to training, validation and test set. 
Meta-params are set on validation data and final performance is
reported on test set.
It is possible that we are over-fitting on the validation data.&lt;/p&gt;

&lt;p&gt;We cannot afford to do it when the training data is too small. 
Cross-validation lets training on entire data by rotating the
validataion data subset.&lt;/p&gt;

&lt;h3 id=&quot;using-noise-as-regularizer&quot;&gt;Using noise as regularizer&lt;/h3&gt;
&lt;p&gt;We can add noise to either input or activations and achieve
regularization.&lt;/p&gt;

&lt;p&gt;Consider a simple network with linear units, if we add guassian noise
to the input with variance sigma^2 then the output of the hidden node
will have an additive noise of w_i^2*sigma^2.
Minimizing the sum y_i plus this noise is equivalent to L2 penality
over the weights with the penality strength sigma^2.&lt;/p&gt;

&lt;p&gt;Adding noise to the weights of multilayer non-linear neural net is not
exactly equivalent to using L2 penality.
It may work better especially in RNN; A. Graves RNN that recognizes
hand-writing works significatly better when noise is added to the
weights.&lt;/p&gt;

&lt;p&gt;We can instead add noise to the hidden unit activations. 
For example, in a logistic unit, we compute the activation and treat
like the probability of the output unit to be one. 
The unit behaves like a binary stochatic unit, where:&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;P(s=1) = \frac{1}{1+e^{-z}}&lt;/script&gt;&lt;br /&gt;
But, during the backpropagation we compute as if we had done the
forwrd pass properly.&lt;/p&gt;

&lt;h3 id=&quot;bayesian-approach-to-regularization&quot;&gt;Bayesian approach to regularization&lt;/h3&gt;
&lt;p&gt;Wenever you see a squared error being minimized, you can find a
probabilistic equivalent that is to maximize the log likelihood. 
This comes from the assumption that the output is a mean of the
gaussian and likelihood of the target value is given by the guassian
with mean given by the output.&lt;/p&gt;

&lt;p&gt;In the bayesian fremework we try to maximize P(W|D) which is
P(D|W)*P(W) (ignoring normalization term that does not depend on W).
log P(D|W) will give a term like:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\sum{(y_c-t_c)^2}}{(\sigma_D)^2}&lt;/script&gt;

&lt;p&gt;and log P(W) will give&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{w^2}{\sigma_w^2}&lt;/script&gt;

&lt;p&gt;Assuming that w is a guassian prior with zero mean. 
The regularization prior term is given by the ratio of &lt;script type=&quot;math/tex&quot;&gt;\sigma_D&lt;/script&gt;
and &lt;script type=&quot;math/tex&quot;&gt;\sigma_w&lt;/script&gt; and is not a random term in this framework.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;MacKay’s quick and dirty way of fixing weight costs&lt;/strong&gt;&lt;br /&gt;
This allows for different weight penalities for different subset of
weights in the network because it does not use validation sets. 
MacKay used it win several competitions.&lt;/p&gt;

&lt;p&gt;The summary/method is:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Start with some initial guess for ratio of noise variance and weight
variance&lt;/li&gt;
  &lt;li&gt;DONE In a loop:
    &lt;ul&gt;
      &lt;li&gt;Do some learning using the ratio as weight penality coeff.&lt;/li&gt;
      &lt;li&gt;Set the noise variance to the variance of the residual errors&lt;/li&gt;
      &lt;li&gt;Set he weight prior variance to variance of distribution of
actual learned weights: &lt;script type=&quot;math/tex&quot;&gt;\sum_j{(w_ij-0)^2}&lt;/script&gt; (I am not very
clear about this.)&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;good-questions-from-quiz-1&quot;&gt;Good questions from quiz&lt;/h2&gt;
&lt;p&gt;L2 regularization will penalize larger weights than the smaller ones
because the function is of second order. 
If you want to penalize smaller weights more than the larger weights,
you should use f(x)=x^0.5 which looks like an inverted banana peel.&lt;/p&gt;

&lt;p&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;Different regularization methods have different effects on the
learning process. For example L2 regularization penalizes high weight
values. L1 regularization penalizes weight values that do not equal
zero. Adding noise to the weights during learning ensures that the
learned hidden representations take extreme values. Sampling the
hidden representations regularizes the network by pushing the hidden
representation to be binary during the forward pass which limits the
modeling capacity of the network.&lt;/code&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/activation-hist.png&quot; alt=&quot;activation histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;An activation histogram that looks like this could mean that
activation sampling and adding noise to the weights is used as
regularizers. 
Sampling leads to non-continuous outputs leading to either firmly on
(+1) or firmly off (-1) states of the previous layer.&lt;br /&gt;
It cannot be L1 or L2 because in that case the middle region where it
is closer to zero cannot be as empty as here.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/weight-hist.png&quot; alt=&quot;weight histogram&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The histogram above corresponds to L2 regularization because the
frequency of the large weights is seriously limited.&lt;/p&gt;

&lt;h2 id=&quot;programming-assignment-1&quot;&gt;Programming Assignment&lt;/h2&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Learning rate&lt;/th&gt;
      &lt;th&gt;Momentum&lt;/th&gt;
      &lt;th&gt;Training Loss&lt;/th&gt;
      &lt;th&gt;Validation Loss&lt;/th&gt;
      &lt;th&gt;Test Loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.002&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.304283&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.01&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.302117&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.05&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.292967&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.228969&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1.598844&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.301322&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.002&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.300135&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.01&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.284022&lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
      &lt;td&gt; &lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.05&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.008606&lt;/td&gt;
      &lt;td&gt;2.018598&lt;/td&gt;
      &lt;td&gt;2.008179&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;1.083429&lt;/td&gt;
      &lt;td&gt;1.122502&lt;/td&gt;
      &lt;td&gt;1.097623&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1.0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.018723&lt;/td&gt;
      &lt;td&gt;2.041323&lt;/td&gt;
      &lt;td&gt;2.038473&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5.0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;20.0&lt;/td&gt;
      &lt;td&gt;0.9&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;With 200 hidden units, 0 weight decay and learning rate of 0.3, momentum 0.95, mini-batch size of 100 and 1000 iterations lead to
&lt;code class=&quot;highlighter-rouge&quot;&gt;a3(0, 200, 1000, 0.35, 0.9, false, 100)&lt;/code&gt;
The loss on the training data is 0.002614
The classification error rate on the training data is 0.000000&lt;/p&gt;

&lt;p&gt;The loss on the validation data is 0.430185
The classification error rate on the validation data is 0.087000&lt;/p&gt;

&lt;p&gt;The loss on the test data is 0.464988
The classification error rate on the test data is 0.093778
With early stopping: 0.334505&lt;/p&gt;

&lt;p&gt;Effect of weight decay on loss&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Wd_param&lt;/th&gt;
      &lt;th&gt;Training loss&lt;/th&gt;
      &lt;th&gt;Validation loss&lt;/th&gt;
      &lt;th&gt;test loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.002614&lt;/td&gt;
      &lt;td&gt;0.430185&lt;/td&gt;
      &lt;td&gt;0.464988&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
      &lt;td&gt;0.007561&lt;/td&gt;
      &lt;td&gt;0.348294&lt;/td&gt;
      &lt;td&gt;0.369097&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.001&lt;/td&gt;
      &lt;td&gt;0.070793&lt;/td&gt;
      &lt;td&gt;0.287910&lt;/td&gt;
      &lt;td&gt;0.289973&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.01&lt;/td&gt;
      &lt;td&gt;0.442156&lt;/td&gt;
      &lt;td&gt;0.509763&lt;/td&gt;
      &lt;td&gt;0.511233&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;5&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
      &lt;td&gt;2.302585&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Effect of model capacity&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Width of hidden layer&lt;/th&gt;
      &lt;th&gt;Training loss&lt;/th&gt;
      &lt;th&gt;Validation Loss&lt;/th&gt;
      &lt;th&gt;Test Loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;10&lt;/td&gt;
      &lt;td&gt;0.011050&lt;/td&gt;
      &lt;td&gt;0.421705&lt;/td&gt;
      &lt;td&gt;0.389471&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;30&lt;/td&gt;
      &lt;td&gt;0.004042&lt;/td&gt;
      &lt;td&gt;0.317077&lt;/td&gt;
      &lt;td&gt;0.364651&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;100&lt;/td&gt;
      &lt;td&gt;0.002849&lt;/td&gt;
      &lt;td&gt;0.368593&lt;/td&gt;
      &lt;td&gt;0.408845&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;130&lt;/td&gt;
      &lt;td&gt;0.002715&lt;/td&gt;
      &lt;td&gt;0.397597&lt;/td&gt;
      &lt;td&gt;0.418396&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;200&lt;/td&gt;
      &lt;td&gt;0.002614&lt;/td&gt;
      &lt;td&gt;0.430185&lt;/td&gt;
      &lt;td&gt;0.464988&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;Effect of model capacity with early stopping&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;Width of hidden layer&lt;/th&gt;
      &lt;th&gt;Training loss&lt;/th&gt;
      &lt;th&gt;Val. Loss&lt;/th&gt;
      &lt;th&gt;Test loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;18&lt;/td&gt;
      &lt;td&gt;0.037047&lt;/td&gt;
      &lt;td&gt;0.306083&lt;/td&gt;
      &lt;td&gt;0.284525&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;37&lt;/td&gt;
      &lt;td&gt;0.284525&lt;/td&gt;
      &lt;td&gt;0.265165&lt;/td&gt;
      &lt;td&gt;0.282510&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;83&lt;/td&gt;
      &lt;td&gt;0.059285&lt;/td&gt;
      &lt;td&gt;0.311244&lt;/td&gt;
      &lt;td&gt;0.337624&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;113&lt;/td&gt;
      &lt;td&gt;0.064678&lt;/td&gt;
      &lt;td&gt;0.313749&lt;/td&gt;
      &lt;td&gt;0.347098&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;236&lt;/td&gt;
      &lt;td&gt;0.076253&lt;/td&gt;
      &lt;td&gt;0.343841&lt;/td&gt;
      &lt;td&gt;0.339124&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h1 id=&quot;week-10-combining-multiple-neural-networks-to-improve-generalization&quot;&gt;Week 10 (Combining multiple neural networks to improve generalization)&lt;/h1&gt;
&lt;p&gt;When you average across many models, then you can expect to do better than any single model.
We can reduce the over-fitting in the case where we have small amount of training data.
It helps most when the models make very different predictions.&lt;/p&gt;

&lt;p&gt;The squared error is sum of bias and variance, for a complex model the bias is reduced and also the evariance can be reduced by averaging over many models.
The variance in the case of when we consider the output as the average over all the models is less than that of when any of the model outputs is randomly choosen by the variance of model outputs.&lt;/p&gt;

&lt;p&gt;when &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
\bar{y}=&lt;y_i&gt;_i=\frac{1}{N}\sum_{i=1}^{N}{y_i} %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
&lt;(t-y_i)^2&gt;_i = (t-\bar{y})^2 + &lt;(y-\bar{y})^2&gt;_i %]]&gt;&lt;/script&gt;

&lt;p&gt;This trick of averaging will work only for convex or concave functions depending on if you are minimizing or maximizing.&lt;br /&gt;
We can train different models that make different predictions by&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;re-running and hoping that the learning algo. will each time gets stuck in different local optima.&lt;/li&gt;
  &lt;li&gt;Use lots of differnt models, does not matter if they are neural nets.&lt;/li&gt;
  &lt;li&gt;Diff Neural nets by varying:
    &lt;ul&gt;
      &lt;li&gt;NUmber of hidden layers, Number of hidden units, activation functions&lt;/li&gt;
      &lt;li&gt;weight penality (L1, L2)&lt;/li&gt;
      &lt;li&gt;learning algorithm: mini-batch, full-batch
Another trick is to train on different subsets of data: bagging (pick data subsets with replacement). 
Boosting: train several low capacity models and weight them such that much computational resources are not spent on instances that some other classifier already did a good job on.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;mixture-of-experts-developed-in-1990s&quot;&gt;Mixture of experts (developed in 1990s)&lt;/h2&gt;
&lt;p&gt;This can make very good use of extremely large datasets
We train models that learn on subsets of data making them an experts in the data they have seen.&lt;/p&gt;

&lt;p&gt;One one hand we have models like K-nearest neighbours that are very local and make a decision depending on local context, on the other hand we have global models which try to fit one model to all the input, output pairs. 
Mixture of experts falls somewhere in between.
We will have number of experts less than that of KNN and also we are not interested in clustering over input alone in the case of Mixture of Experts, but rather interested in grouping of inputs such that their outputs can be better modeled.&lt;/p&gt;

&lt;p&gt;E = &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;(t-\bar{y})^2&gt;_i %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;will train the mixture of cooperative models.
The models each will try to reduce the error caused by other models whether or not that will bring them closer or away from target.&lt;/p&gt;

&lt;p&gt;E = &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;p_i(t-y_i)^2&gt;_i %]]&gt;&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The weights &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; are determined by the manager or the gated network.
The update equation for this network are such that the correction to the weights of an expert is the product of &lt;script type=&quot;math/tex&quot;&gt;p_i&lt;/script&gt; and the error that it made. 
The update equation for variable that results in gate output for a certain gate is based on if the error of this expert is more or less than the average error.&lt;/p&gt;

&lt;h2 id=&quot;full-bayesian-learning-revisited&quot;&gt;Full Bayesian learning revisited&lt;/h2&gt;

&lt;p&gt;Unlike MLE, we do not estimate one set of parameter that do well over the data, but full posterior distribution over all possible parameter settings.
In this setting, there is no case of over-fitting. 
When you keep track all the parameter setting, each of the parameters will have blunt probability distribution, but together can perform well; As we get more data the probability distr. of parameters get sharper.&lt;/p&gt;

&lt;p&gt;In the case of neural networks, it is not practical to do full bayesian learning. 
Imagine a case where we have 6 parameters and allow each of them to take 9 different values then the grid is 9^6 large.(not feasible for large nets)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(t_{test}/input_{test}) = \sum_{i}{P(W_i/D)*P(t_{test}/input_{test},W_i)}&lt;/script&gt;

&lt;h2 id=&quot;making-it-practical-with-mcmc&quot;&gt;Making it practical with MCMC&lt;/h2&gt;
&lt;p&gt;We start with random vectors in the weight space and update the weights with some noise so the weights will never converge, but keep wandering.
We can use mini-batch gradients such that the noise we require can be supplied by sampling noise of mini-batch.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt; If we use just the right amount of noise, and if we let the
weight vector wander around for long enough before we take a sample, we
will get an unbiased sample from the true posterior over weight vectors. 
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;h2 id=&quot;dropout----averaging-without-using-several-models&quot;&gt;Dropout – Averaging without using several models&lt;/h2&gt;
&lt;p&gt;In the case of single hidden layer and dropout, we omit the output of one of the hidden units with some probability. 
Which means that there are 2^H different architectures possible, some of them may not even get trained explicitly. 
All the different networks share weights which works like a strong regularizer.
The weight sharing is a much better regularizer than L2 or L1 penalities which just pull the weights towards zero while in dropout the weights are pulled towards the correct value.&lt;/p&gt;

&lt;p&gt;In the case of single hidden layer, dropout exactly computes the geometric mean of predictions of 2^H models.
When more than one hidden layer: dropout is a good approximation to averaging separate dropped out models.&lt;/p&gt;

&lt;p&gt;We can also use dropout in input layer, typically with higher retention probability and is in use: “denoising autoencoders” P. Vincent.&lt;/p&gt;

&lt;p&gt;Dropout prevent complex co-adaptations. 
These co-adaptations can go wrong over the new data.
When a hidden unit is forced to work with othe units which are unpredictable since there are many, it is more likely to do something that is individually useful and not useful when used by unit that is fed into.&lt;/p&gt;

&lt;h2 id=&quot;reading-assignment-1-adaptive-mixtures-of-local-experts-linkexpert-mixture&quot;&gt;Reading Assignment 1 (Adaptive Mixtures of Local Experts) &lt;a href=&quot;http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf&quot; title=&quot;Adaptive Mixtures of Local Experts&quot;&gt;link&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Some points worth noticing and not discussed in the lecture are:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E^c = \|d^c-\sum_i{p_i^co_i^c}\|^2&lt;/script&gt;

&lt;p&gt;A loss function computed like above used to be the standard before this one and it is bad for the following reasons.
The prediction of the system is contributed by all the experts, so in order to reduce the loss the experts cooperate and co-adapt so that one reduces the residue left by the other.
This is bad because we are using several experts for each case and it will be slow to train and as interference effects shown by a normal model.
Instead&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E^c=\sum_i{p_i^c\|d^c-o_i\|^2}&lt;/script&gt;

&lt;p&gt;It is also observed that a slightly different variant is quick to converge&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E^c=-log{\sum_i{p_i^ce^{-\frac{1}{2}\|d^c-o_i^c\|^2}}}&lt;/script&gt;

&lt;p&gt;This is better because, the expression &lt;script type=&quot;math/tex&quot;&gt;\frac{\partial{E^c}}{\partial{o_i^c}}&lt;/script&gt; contains sum over all experts in the denominator and hence helps to quickly pass the information that some expert is already doing well over a case, c.&lt;/p&gt;

&lt;p&gt;Finally, the gating network can be a feed-forward network that receives the same input as the experts and outputs a softmax score over which expert to choose.
Then, a stochastic one-out-of-n selector will select an expert based on the expert responsibility probabilities above.&lt;/p&gt;

&lt;h2 id=&quot;reading-assignment-2-improving-neural-networks-by-preventing-co-adaptation-of-feature-detectors-linkdropout&quot;&gt;Reading Assignment 2 (Improving neural networks by preventing co-adaptation of feature detectors) &lt;a href=&quot;https://arxiv.org/pdf/1207.0580.pdf&quot; title=&quot;Improving neural networks by preventing co-adaptation of feature detectors&quot;&gt;link&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;The paper does not much detail that is not already covered in the lecture.&lt;/p&gt;

&lt;p&gt;A few interesting points are:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;dropout can pretty much be applied to any layer, be it input or any of the hidden layers. In the article, at least, the dropout on the input is reported to have lesser effect on performance 20% compared to 50% improvement.&lt;/li&gt;
  &lt;li&gt;For fully onnected layers, droput in all hidden layers works better than droput in only one hidden layer and more extreme probabilities tend to be worse, which is dropout of 0.5 is generally used.&lt;/li&gt;
  &lt;li&gt;“For datasets in which the required input-output mapping has a number of fairly different regimes, performance can probably be further improved by making the dropout probabilities be a learned function of the input, thus creating a statistically efficient “mixture of experts” (13) in which there are combinatorially many experts, but each parameter gets adapted on a large fraction of the training data.”&lt;/li&gt;
  &lt;li&gt;Dropout can be seen as a Bayesian model, but instead of weighting the output of each model by their coditional on the data, Droput assumes that all the models are equally likely.&lt;/li&gt;
  &lt;li&gt;“Dropout can be seen as an extreme form of bagging in which each model is trained on a single case and each parameter of the model is very strongly regularized by sharing it with the corresponding parameter in all the other models. This is a much better regularizer than the standard method of shrinking parameters towards zero.”&lt;/li&gt;
&lt;/ul&gt;

&lt;h1 id=&quot;week-11-hopfield-nets-and-boltzmann-machines&quot;&gt;Week 11 (Hopfield Nets and Boltzmann machines)&lt;/h1&gt;
&lt;p&gt;Hopfield Nets is composed of binary threshold units with recurrent connections between them.&lt;br /&gt;
These are generally hard to train, but if the connections are symmetric, there is a good energy function.&lt;br /&gt;
The energy function for the ho. Net with symmetric connections is:&lt;/p&gt;

&lt;p&gt;&lt;script type=&quot;math/tex&quot;&gt;E = -\sum_{i}{s_i*b_i} - \sum_{i,j}{s_i s_j w_i w_j}&lt;/script&gt;&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;\delta{E_i} = E(s_i=0)-E(s_i=1)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The states of the system is set to a value based on the energy gap defined above, but it may not be the deepest possible.&lt;br /&gt;
We only make sequential update, else the energy may not go down or can get stuck in oscillations.&lt;/p&gt;

&lt;p&gt;A paper published in 1982 put forward the use case in applications of memory retrieval and content-addressable memory.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The network can restore the neural network weights from “corrupted data” by running the conf. to minimum energy.&lt;/li&gt;
  &lt;li&gt;It is possible to access content with partially specified bits.&lt;/li&gt;
  &lt;li&gt;A system can be made robust if the other components converge to a better state when one of the components have failed.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To store memory in such network, we update the weight of the connection between two nodes as &lt;script type=&quot;math/tex&quot;&gt;\delta{w_{ij}}=s_i*s_j&lt;/script&gt;.
Which correspond to the least energy state and hence can correct if it deviates from it.&lt;/p&gt;

&lt;h2 id=&quot;the-memory-capacity-of-hopfield-nets-and-methods-to-improve-it&quot;&gt;The memory capacity of hopfield nets and methods to improve it&lt;/h2&gt;

&lt;p&gt;In a Hopfield net with N memory units, the state of the bits of each of the states are not completely uncorrelated, because it should be possible to retrieve memory without ambiguity.
This limits the capacity, and according to hopfield, it is effectively 0.15N^2 for the N^2 total connections in the network.
The capacity is almost useless because of the storage required for N^2 weights and biases which is N^2log(2M+1) where M is the  number of memory units; log(M) because we either add or subtract one each time we make an update to w, which means &lt;script type=&quot;math/tex&quot;&gt;w \in [-M, M]&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;To explain a little more on limited capacity of the HN, when the energy of two state space are very close then they may fall in the vicinity of another state configuration that has even smaller energy, i.e. they are no longer the local minima.
This means that we cannot use those state values.&lt;/p&gt;

&lt;p&gt;There is an interesting description of this spurious minima can be avoided with “unlearning” and a theory put forth by Crick and Mitchinson that we are possibly doing unlearning to correct the spurious minima while dreaming (REMM state).
That is our brains do some unlearning and fit a MLE model on how well your NN is able to explain the things it has seen in the day.&lt;/p&gt;

&lt;p&gt;The solution to improving capacity is to use perceptron convergence procedure and instead iterate over the data several times.&lt;/p&gt;

&lt;h2 id=&quot;using-hopfield-energy-states-for-a-good-interpretation-of-inputs-with-the-underlying-hidden-states&quot;&gt;Using Hopfield energy states for a good interpretation of inputs with the underlying hidden states&lt;/h2&gt;

&lt;p&gt;Can we use them such that low energy correspond to a good interpretation?&lt;/p&gt;

&lt;p&gt;For example, although we see the world in 2D, projected on retina, we constantly interpret the world in 3D.
The example provided in lecture is our interpretation of structure based on observing a few bunch of 2D lines.
If we have one neuron for every possible 2D line that we see in the world, then the activated neurons each will activate hidden states that activate the neurons that corresponds to 3D edges and common-sensical knowledge that, for example edges generally connect orthongonally in real world and that two intersecting edges should have same depth at intersection, all lead to added constraints. 
If you consider necker cube (image below), then it has two possible interpretations which have same low energy states.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Necker_cube.svg/220px-Necker_cube.svg.png&quot; alt=&quot;necker cube&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;simulated-annealing-and-thermal-equilibrium&quot;&gt;Simulated annealing and thermal equilibrium&lt;/h2&gt;
&lt;p&gt;One way to avoid local minima is to add noise to the threshold states initially and calm the noise down from there on (Simulated annealing)&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(s_i=1) = \frac{1}{1+e^{\delta{E_i}/T}}&lt;/script&gt;

&lt;p&gt;where T is temperature and &lt;script type=&quot;math/tex&quot;&gt;\delta{E_i}&lt;/script&gt; is the energy gap.&lt;/p&gt;

&lt;p&gt;After running the system for long enough time, the probability distribution of the configurations stabilise and is refered to as “thermal equilibrium”
Each of the configurations may not have stabilised, but the switching of one confguration to other is such that their dist. remains the same.&lt;/p&gt;

&lt;p&gt;These two concepts play a role in Boltzmann machines.
Boltzmann machines make a stochastic update with an assumed constant temperature of one. 
Hopfield nets update weights deterministicaly that is as if T=0.&lt;/p&gt;

&lt;p&gt;There is a stark similarity between activation of a binary threshold unit in BM and in a neural network.
In a BM with temp=1:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(s_i=1) = \frac{1}{1+e^{-\delta{E}/T}}&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\delta{E} = E(s_i=0)-E(s_i=1)&lt;/script&gt;

&lt;p&gt;The energy gap is exactly equal to the weighted sum of nodes connected to the node being updated plus bias.&lt;/p&gt;

&lt;h2 id=&quot;boltzmann-machines-aka-stochastic-hopfield-networks&quot;&gt;Boltzmann Machines aka stochastic hopfield networks&lt;/h2&gt;
&lt;p&gt;A causal generative model generates by sampling the hidden state from their probability distribution and generating the visible states conditional on the hidden states.
BM on the other hand, everything is defined as energy.
&lt;script type=&quot;math/tex&quot;&gt;P(v,h) \propto e^{-E(v,h)}&lt;/script&gt;
The expression for E contains 5 terms that is, the bias terms for visible and hidden states, weights connecting visible, hidden and between.
Since for the computation of P(v,h) requires the partition function, hence MCMC.
MCMC is used to sample (v,h) and also to sample posterior, distribution over h.
For sampling over posterior, the visible states are clamped to obeserved and only the hidden states are updated when we stochastically pick a node and update it based on energy gap.&lt;/p&gt;
&lt;h1 id=&quot;week-12-restricted-boltzmann-machines----rbms&quot;&gt;Week 12 (Restricted Boltzmann Machines – RBMs)&lt;/h1&gt;
&lt;h2 id=&quot;boltzmann-machine-learning-algorithm&quot;&gt;Boltzmann Machine Learning algorithm&lt;/h2&gt;
&lt;p&gt;We have seen how BMs can be used to model probability distributions of a given data vectors, we shall see how they are learned.&lt;/p&gt;

&lt;p&gt;Unlike in the supervised setting, where we have noth data and labels and we back-propagate to learn the weights; In BMs there are no labels.
The goal of learning is to maximize the product of probabilites that BM assigns to the binary vectors in the training sets.&lt;/p&gt;

&lt;p&gt;Learning BMs can be hard because the updates to weight between any two states requires the information of other weights.
For example, in figure below: for the training data, (1,0) and (0,1), we want the visible units to be negatively correlated. 
If all the weights are +ve or -ve then the inputs are +ve correlated, hence to update weight w1, we need info about w3.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/BM-learning-difficult.png&quot; alt=&quot;Why learning BM difficult&quot; /&gt;&lt;/p&gt;

&lt;p&gt;Given the complex interaction between weights, it is surprising that local information can cater to the weight update.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;% &lt;![CDATA[
\frac{\partial{P(v)}}{\partial{w_{ij}}} = &lt;s_i*s_j&gt;_{data}-&lt;s_i*s_j&gt;_{model} %]]&gt;&lt;/script&gt;

&lt;p&gt;The change to a weight is proportional to difference between correlation between &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt; when the inputs are clamped to the visible vectors and the network is allowed to settle to a thermal equilibrium and correlation when there is no clamping.
The negative term to the right gets rid of the spurious minimum. It corresponds to the normalizing term in the denominator.&lt;/p&gt;

&lt;p&gt;The process of settling to the thermal equilibrium itself propagates information about the weights – don’t need backpropagation.&lt;/p&gt;

&lt;p&gt;To collect statistics over positive and negative phase, we start with some initial configuration and let the system reach thermal equilibrium with or without the visible unit clamping.
When we start with global configs (-ve phase), we expect that a small fraction of states corresponding to the data have low energy and the rest high energy, things get complex when the underlying model is multi-modal and hence can have different states with low energy.
These problems are fixed with thechniques discussed in next section.&lt;/p&gt;

&lt;h2 id=&quot;more-efficient-ways-to-train-bms&quot;&gt;More efficient ways to train BMs&lt;/h2&gt;

&lt;h2 id=&quot;restricted-boltzmann-machines-rbms&quot;&gt;Restricted Boltzmann Machines (RBMs)&lt;/h2&gt;
&lt;p&gt;Makes learning simpler by removing conections between the hidden states, also no connections between visible units.&lt;/p&gt;

&lt;p&gt;In an RBM, it only takes one step to reach thermal equilibrium when the visible units are clamped.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(h_j=1)=\frac{1}{1+e^{-(b_j+\sum_{i\in vis}{v_iw_{ij}})}}&lt;/script&gt;

&lt;p&gt;As can be seen, the hidden state is active depending on the input from all the visible states (alone).&lt;/p&gt;

&lt;p&gt;An efficient mini-batch learning procedure – Persistent CD (PCD) by Tieleman, 2008. 
The basic idea od to use single markov state, which has a persistent state.
See the deeplearning.net resource below.&lt;/p&gt;

&lt;p&gt;Contrastive divergence procedure of learning weights.
This is what we do when we wait for thermal equilibrium to update weights&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/rbm-slide23.png&quot; alt=&quot;Slide23&quot; /&gt;&lt;/p&gt;

&lt;p&gt;When we clamp the visible states to the training data and update the hidden states at time 0 (with the logistic activation above), we then reconstruct the visible state from hidden state at time step 1 and so on. 
The visible state at the end when the updates have stabilised is the &lt;em&gt;fantasy&lt;/em&gt;.
Obviously, that will take several updates.&lt;/p&gt;

&lt;p&gt;Instead, we can just …&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/rbm-slide24.png&quot; alt=&quot;Slide24&quot; /&gt;&lt;/p&gt;

&lt;p&gt;correct for the confabulations rather than wandering a long way into the update.
This can have the effect of increasing the energy of alternate reconstructions surrounding the data point, creating a trough at the point itself.
This procedure may fail to recognize regions of data-space that the model likes, but are far from any data.
These low energy holes cause the normalization terms to bloat, but this procedure would fail to sense it.&lt;/p&gt;

&lt;p&gt;A good summary of all the procedures of learning can be found [here][deeplearnin.net]&lt;/p&gt;

&lt;h2 id=&quot;rbms-for-collaborative-filtering&quot;&gt;RBMs for collaborative filtering&lt;/h2&gt;

&lt;p&gt;Given that RBMs are good at filling the missing data, collaborative filtering which is filling of value in user-item matric is a natural problem.&lt;/p&gt;

&lt;p&gt;The Netflix challenge has 0.5M users that rated 18,000 movies in total with a rating from 1-5. 
The task is to predict the rating for a user and movie pair in the held-out dataset. 
One way to see the problem is to model the user, movie, rating triplet.
If we use the language model, we would come up with a feature vector for user and movie and in the end feed the two vectors in say a network with hidden layer to predict the rating (Hinto explained though that, computation just the scalar product itself did equllay good). 
This approach is similar to the alternative, Matrix Factorization.&lt;/p&gt;

&lt;p&gt;Using RBMs for this task requires re-formulation of the problem.
Every user is treated as an 18,000 vector of movies while the movie dimension can take 5 values. 
Since the data for one user could be sparse, the user vector is represented by only the movies rated by that user.
We use a separate RBM for each user and share the hidden to movie weights between them.&lt;/p&gt;

&lt;p&gt;For more, look at &lt;a href=&quot;http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf&quot; title=&quot;Restricted Boltzmann Machines for Collaborative Filtering&quot;&gt;Salakhutdinov et al. 2007&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;good-questions-from-quiz-2&quot;&gt;Good questions from quiz&lt;/h2&gt;

&lt;p&gt;The weight update term: &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; that can be obtained from averge of product of states: &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt; is only an approximation because in the case of BM models because computing &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;s_is_j&gt; %]]&gt;&lt;/script&gt; over model or data is hard in general because it involves sampling from the distribution and several steps of update.
On the other hand, the computation of &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;s_is_j&gt;_{data} %]]&gt;&lt;/script&gt; can be computed exactly because the system reaches thermal equilibrium in just one step.&lt;/p&gt;

&lt;p&gt;If we ignore the former or the latter term in the weight update equation given by the difference in average over data and model, it does not lead to a good solution.
If we omit the former term, then we are not making use of data and if we omit the latter term then we are reducing the energy of the observed data point which could also be reducing the energy of other points that are not seen.&lt;/p&gt;

&lt;p&gt;PCD can also be applied to BM.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/bm-activation.png&quot; alt=&quot;BM-activation&quot; /&gt;&lt;/p&gt;

&lt;h1 id=&quot;week-13-belief-nets&quot;&gt;Week 13 (Belief nets)&lt;/h1&gt;

&lt;h2 id=&quot;a-brief-history-of-backpropagation&quot;&gt;A brief history of backpropagation&lt;/h2&gt;

&lt;p&gt;Backpropagation is proposed during 1970-80.
It was abandoned by many machine learning for reasons (actual according to Hinton): less computational power, less data, deep network weights not initialized properly, as a result, the gradients tended to die.&lt;/p&gt;

&lt;p&gt;Backprop was abondoned for SVM which have a much fancier theory and easy to condition on a small data. 
He discusses two different views of SVMs: both views look at SVMs as clever reincarnation of perceptron.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;View 1: SVMs traforms the input into very large layer of non-linear non-adaptive features which is followed by one layer of adaptive weights. They have clever way of learning weights so as to avoid over-fitting through tarining of max-margin classifier.&lt;/li&gt;
  &lt;li&gt;View 2: Use each input vector in training set to define a non-adaptive feature, and given a test input, a match is computed between the test and the train instance. The learning algo. learns to select the right feature(input) and weight them.&lt;br /&gt;
Major drawback of SVM is that they cannot be trained to learn multiple layers of representations.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;belief-nets&quot;&gt;Belief nets&lt;/h2&gt;
&lt;p&gt;The need for alternative or why back-propagation does not always work:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Requires lot of labeled data (Supervised learning)&lt;/li&gt;
  &lt;li&gt;learning time is very large probably because the weights are not properly initialized.&lt;/li&gt;
  &lt;li&gt;local optima problem.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;He mentioned how probability was abhored by the AI community. 
According to them, computers are all about processing discrete symbols and introducing probability will infest everything.&lt;/p&gt;

&lt;p&gt;Belief nets are directed acyclic graphs, typically sparsely connected.
The edges are the dependencies between variables.
Early graphical models used experts to define the graph structure and the conditional probabilities (such as the probability tables).
They only focused on inference (figuring out distribution of hidden states given the visible state) and not learning (because the graph is given by the experts).&lt;/p&gt;

&lt;p&gt;Boltzmann machine is a binary siochastic neurons connected with symmetric weights (undirected) which are hard to learn as seen before, but the learning can be slightly made better by restricting (but RBMs are only one layer though)&lt;/p&gt;

&lt;p&gt;Sigmoid belief nets are easier to learn with an appropriate algorithm.
This is because it is hard to infer the posterior distribution given a datavector due to explaining away property.
Consider the case where the two causes: &lt;em&gt;truck hits house&lt;/em&gt; and &lt;em&gt;earthquake&lt;/em&gt; can cause the effect: &lt;em&gt;house jumps&lt;/em&gt;.
Although, the two causes are independent, they get dependent when the effect is observed.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;truck hits house&lt;/th&gt;
      &lt;th&gt;earthquake&lt;/th&gt;
      &lt;th&gt;probability&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.4999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.4999&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0&lt;/td&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;h2 id=&quot;wake-sleep-algorithm&quot;&gt;Wake-sleep algorithm&lt;/h2&gt;

&lt;p&gt;To evade the &lt;em&gt;explaining away&lt;/em&gt; problem, we draw posterior from a wrong distribution and yet make the learning work.&lt;/p&gt;

&lt;p&gt;The poserior distribution is wrongly assumed to be a factorial diatribution, that is they are assumed to be independent even in the posterior just as in the prior.
A general probability distribition over N variables has 2^N-1 degrees of freedom (one less because they shopuld sum upto one).
A factorial distribution on the other hand has N degrees of freedom.&lt;/p&gt;

&lt;p&gt;This is the algorithm that led to variational learning methods.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/wake-sleep.png&quot; alt=&quot;Wake-sleep algorithm&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The slide above summarizes the wake-sleep algorithm.&lt;/p&gt;

&lt;p&gt;The wake sleep algorithm leads to incorrect mode averaging due to the factorial distribution assumption.
For example, in the example above of house jumping, since both (1,0) and (0,1) over the two causes are equally likely.
That is, when we generate from the model half the instances of a 1 at data layer will be caused by a (1,0) and other by (0,1).
Hence, the recognition weights (the weights that go from visibe state to hidden) will be 0.5 for both states which puts equal mass even on unlikely states: (1,1) and (0,0).&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Expression for gradient on weight in SBN&lt;/strong&gt;
Let &lt;script type=&quot;math/tex&quot;&gt;w_{ij}&lt;/script&gt; denote the weight from parent &lt;script type=&quot;math/tex&quot;&gt;s_j&lt;/script&gt; to the child &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;.
The probability of certain state of the network be denoted by &lt;em&gt;S&lt;/em&gt;.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;log(P(S)) = log(R)+log(Pn)+log(P(s_i/Pn))&lt;/script&gt;

&lt;p&gt;where Pn is the parent nodes of &lt;script type=&quot;math/tex&quot;&gt;s_i&lt;/script&gt;, R is the rest of the network. 
&lt;script type=&quot;math/tex&quot;&gt;\frac{\partial log(P(S))}{\partial w_ij}&lt;/script&gt; depends only on the last term in the above expression.
Making use of the fact that the network uses sigmoid activations, we have the gradient equal to:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\frac{\partial log(P(S))}{\partial w_{ij}} = s_j * (s_i-p(s_i))&lt;/script&gt;

&lt;p&gt;which should be straightforward to derive.&lt;/p&gt;

&lt;h2 id=&quot;reading-assignment-1-the-wake-sleep-algorithm-for-unsupervised-neural-networkswake-sleep&quot;&gt;Reading Assignment 1 (&lt;a href=&quot;http://www.gatsby.ucl.ac.uk/~Dayan/papers/hdfn95.pdf&quot; title=&quot;The wake-sleep algorithm for unsupervised neural networks&quot;&gt;The wake-sleep algorithm for unsupervised neural networks&lt;/a&gt;)&lt;/h2&gt;
&lt;p&gt;The aim of learning is to minimize the “description length” which is the total number of bits that would be required to communicate the input vectors in this way.
By forcing the network to learn an economical representation of the data can help in realizing any regularities in the data.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;The neural network has two quite different sets of connections. The bottom-up “recognition” connections are used to convert the input vector into a representation in one or more layers of hidden units. The top-down “generative” connections are then used to reconstruct an approximation to the input vector from its underlying representation.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;The activation of the stochastic neuron is given by:&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(s_v=1) = \frac{1}{1+exp(-b_v-\sum_{u}{s_uw_{uv}})}&lt;/script&gt;

&lt;h2 id=&quot;programming-assignment-2&quot;&gt;Programming assignment&lt;/h2&gt;
&lt;p&gt;The valiudation cross entropy loss for different classification learning rate
with rbm learning rate of 0.02, 300 hidden units and 1000 iterations over an RBM
is as shown in the table.
Also, the RBM input to hidden weights are not learned when training for classification.&lt;/p&gt;

&lt;table&gt;
  &lt;thead&gt;
    &lt;tr&gt;
      &lt;th&gt;classification learning rate&lt;/th&gt;
      &lt;th&gt;validation xentropy loss&lt;/th&gt;
    &lt;/tr&gt;
  &lt;/thead&gt;
  &lt;tbody&gt;
    &lt;tr&gt;
      &lt;td&gt;0.0001&lt;/td&gt;
      &lt;td&gt;1.820011&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.0005&lt;/td&gt;
      &lt;td&gt;0.968603&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.001&lt;/td&gt;
      &lt;td&gt;0.673805&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.005&lt;/td&gt;
      &lt;td&gt;0.322890&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.01&lt;/td&gt;
      &lt;td&gt;0.259159&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;.05&lt;/td&gt;
      &lt;td&gt;0.202482&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.08&lt;/td&gt;
      &lt;td&gt;0.198855&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.1&lt;/td&gt;
      &lt;td&gt;0.198644&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.2&lt;/td&gt;
      &lt;td&gt;0.203790&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;0.5&lt;/td&gt;
      &lt;td&gt;0.224757&lt;/td&gt;
    &lt;/tr&gt;
    &lt;tr&gt;
      &lt;td&gt;1&lt;/td&gt;
      &lt;td&gt;0.272533&lt;/td&gt;
    &lt;/tr&gt;
  &lt;/tbody&gt;
&lt;/table&gt;

&lt;p&gt;One of the questions in the programming assignments is about estimating the partition function when there are 10 hidden states and 256 visible states, which means that a brute force summation over all the terms would need sum over &lt;script type=&quot;math/tex&quot;&gt;O(2^{266})&lt;/script&gt; terms.
Instead it is possible to calculate the summation in &lt;script type=&quot;math/tex&quot;&gt;O(2^{10}256)&lt;/script&gt; calculations. That is, we iterate over all possible hidden states and for a given hidden state and the weight matrix, the summation over all the visible states is just &lt;script type=&quot;math/tex&quot;&gt;\prod_{i}{1+e^{prob_v(i)}}&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;prob_v&lt;/script&gt; is &lt;script type=&quot;math/tex&quot;&gt;h*rbm_w&lt;/script&gt;&lt;/p&gt;

&lt;h2 id=&quot;cheat-sheet&quot;&gt;Cheat sheet&lt;/h2&gt;
&lt;h3 id=&quot;bm&quot;&gt;BM&lt;/h3&gt;
&lt;p&gt;Most of the formalae in this section are also applicatble to RBMs since they are just BMs with only one hidden layer and visible layer and with some additional constraints.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(v,h) = -\sum_{i}{a_iv_i}-\sum_{j}{b_jh_j}-\sum_{i,j}{v_ih_jW_{ij}}&lt;/script&gt;

&lt;p&gt;This leads to&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(h_j=1|v)=\frac{1}{1+exp(-\sum_{i}{W_{ij}v_i}-b_j)}&lt;/script&gt;

&lt;p&gt;&lt;strong&gt;Partition function&lt;/strong&gt; Z is given by &lt;script type=&quot;math/tex&quot;&gt;\sum_{v}\sum_{h}e^{-E(v,h)}&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Goodness&lt;/strong&gt; is just the negative if the energy.&lt;/p&gt;

&lt;h3 id=&quot;rbm&quot;&gt;RBM&lt;/h3&gt;

&lt;p&gt;&lt;em&gt;CD1 update expression&lt;/em&gt;
In the CD1 update, which can readily be extended to CDn in general, we start with a visible state and with the given hidden to weight vector.&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(h_1/v) = logistic(rbm_w*v)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_1 \sim P(h_1/v)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(v_2/h_1) = logistic(h*rbm_w)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;v_2 \sim P(v_2/h_1)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;P(h_2/v_2) = logistic(rbm*v_2)&lt;/script&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;h_2 \sim P(h_2/v_2)&lt;/script&gt;

&lt;p&gt;Loss = &lt;script type=&quot;math/tex&quot;&gt;-E(v, h_1) + E(v_2, h_2)&lt;/script&gt;&lt;/p&gt;

&lt;p&gt;The &lt;script type=&quot;math/tex&quot;&gt;v_2&lt;/script&gt; term above is often referred to as “reconstruction” visible state.
The first term in the loss is to make sure that the visible state is more likely according to the model and the second term is so that the reconstructed weights have high energy.&lt;/p&gt;

&lt;h1 id=&quot;week-14-deep-neural-nets-with-generative-pre-training&quot;&gt;Week 14 (Deep neural nets with generative pre-training)&lt;/h1&gt;
&lt;h2 id=&quot;layers-of-features-by-stacking-rbms&quot;&gt;Layers of features by stacking RBMs&lt;/h2&gt;
&lt;p&gt;Given that RBMs are easy to learn, can we stack RBM spo as to learn layers of features effectively?&lt;/p&gt;

&lt;p&gt;When you stack several RBMs and learn multi-layer features, you expect that the resulting model would be equivalent to a BM, but instead it behaves like a SBN. 
Hence, it provides an interesting way of training the SBN.&lt;/p&gt;

&lt;blockquote&gt;
  &lt;p&gt;It can be proved that each time we add another layer of features we improve a variational lower bound on the log probability of generating the training data. (based on equivalence between RBM and infinitely deep belief nets.)&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;Deep belief nets are a hybrid of boltzmann machines (undirected connections) and SBN (directed connection)&lt;/p&gt;

&lt;p&gt;Composing two RBMs make a DBN
&lt;img src=&quot;/assets/images/nn-notes/rbns-make-dbn.png&quot; alt=&quot;RBMs make a DBN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The DBN shown in the diagram above has top two layers that make an RBM and the bottom two layers make a sigmoid belief net.
The reason for how and why is more complicated.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Training a DBN with a variant of wake-sleep algorithm:&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Do stochastic bottom up pass and fine-tune the top down weights to reconstruct the features in the bottom layers&lt;/li&gt;
  &lt;li&gt;Do a few iterations of sampling in the top level RBM using CD (contrastive divergence)&lt;/li&gt;
  &lt;li&gt;Do a stochastic top-down pass and adjust bottom-up weights&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;We have seen a variant of wake-sleep for learning stack of RBMs to make them better at generation of inputs. 
We can use backprop to make this network better at discrimination.&lt;/p&gt;

&lt;p&gt;Backprop works better when we pre-train weights because of &lt;strong&gt;optimization and generalization reasons&lt;/strong&gt;. 
We do not start backprop until we have sensible feature detectors from pre-training which means that the gradients make more sense and backprop does not have to do a global search, but only a local search around the existing weights.&lt;/p&gt;

&lt;p&gt;Because, most of the training happens on the unlabeled data and we use fine-tuning of the labels in the final stages, we do not require as much trainng data for fine-tuning.
In the fine-tuning stage, we are not learning to produce any new features, but just figuring out how to use them.&lt;/p&gt;

&lt;p&gt;Hinton demonstrated with reults on permutation invariant MNIST task (where the pixels in MNIST images are randomly permutated) that learning a generative model of unlabelled digits followed by gentle backpropagation does better than generative model of joint density of images and labels, SVM, and backprop based optimizations in this order.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/pretraining-affect-learning-space.png&quot; alt=&quot;RBMs make a DBN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The image above shows the t-SNE visualization of output mappiongs of various models in 2D space with and without pre-training.
Each blob in the image above correspond to a model, the cluster on the top is without pre-training and the bottom one is with pre-training.
The color of the blob indicates the epoch, blue-&amp;gt;yellow as the # epochs increase.
The picture above is generated by concatenating of all the outputs on a test suite.
Things to note:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;The cluster at the top is more scattered meaning that the outputs learn a local minimum that is sensitive to the initial state. While in the bottom one, the solutions are not as scattered.&lt;/li&gt;
  &lt;li&gt;There is no overlap between the clusters meaning that the solutions obtained with and without pre-training are qualitatively different.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Why unsupervised pre-training works&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/why-pretrain.png&quot; alt=&quot;Why pretrain?&quot; /&gt;
As shown in the image above, image(high bandwidth) captures lot more information for the world than a label (low bandwidth).
That is when an image is labeled with “cow”, we do not know if the image contains any other animals or if the cow is dead or where in the image the cow is.
Hence, follows the justification for the text provided in the image above.&lt;/p&gt;

&lt;h2 id=&quot;modeling-of-real-valued-data&quot;&gt;Modeling of real-valued data&lt;/h2&gt;
&lt;p&gt;We can only go to some extent with logistic neaurons (actually they are referred to as “mean field logistic neurons”, I don’t quite understand what they are).
Consider the case of hand-writing images where a pixel can have varying intensity levels depending on the pressure applied by the user. 
The intermediate values (i.e. between 0 and 1) can be treated as the probability that a pixel is inked, but it won’t be able to capture a fact that a pixel is almost always a mean of the neighbouring pixels, that is a pixel with intensity 0.69 is less likely to be 0.67 or 0.71.
The confidence interval (variance) is missing.
Hence, we use a linear unit with Gaussian noise.&lt;/p&gt;

&lt;h2 id=&quot;guassian-binary-rbm&quot;&gt;Guassian Binary RBM&lt;/h2&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;E(v,h) = \sum_{i\in vis}{\frac{(v_i-b_i)^2}{2\sigma_i^2}}-\sum_{j\in hid}{b_jh_j} - \sum_{i,j}{\frac{v_i}{\sigma_i}h_jw*{ij}}&lt;/script&gt;

&lt;p&gt;THe visible state &lt;script type=&quot;math/tex&quot;&gt;v_i&lt;/script&gt; is now approximated by the bias term &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; and scaled with &lt;script type=&quot;math/tex&quot;&gt;\sigma_i&lt;/script&gt;.
The first term in the expresssion is a parabola with minimum at &lt;script type=&quot;math/tex&quot;&gt;b_i&lt;/script&gt; while the last term depends on &lt;script type=&quot;math/tex&quot;&gt;v_i&lt;/script&gt; linearly.&lt;/p&gt;

&lt;p&gt;It is hard to train these to obtain tight variance bounds because the top-down (from hidden to visible) effects are multiplied with &lt;script type=&quot;math/tex&quot;&gt;\sigma_i&lt;/script&gt; while the bottom up effects are divide by &lt;script type=&quot;math/tex&quot;&gt;\sigma_i&lt;/script&gt;. 
As a result, the hidden units get saturated and either turn “on” or “off” all the time.
Can avoid this effect by employing lot more hidden units than visible units which multiplies the top-down effect.&lt;/p&gt;

&lt;p&gt;This idea is applied with a clever technique of &lt;em&gt;Stepped Sigmoid Units&lt;/em&gt;.
In a stepped sigmoid unit, we have various copies of a stochastic binary unit.
All copies have same weight and same adaptive bias, &lt;em&gt;b&lt;/em&gt;, but they have different fixed offsets to the bias: &lt;em&gt;b-0.5, b-1.5, b-2.5 …&lt;/em&gt;&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/stepped-sigmoid.png&quot; alt=&quot;Stepped sigmoid unit&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As &lt;em&gt;x&lt;/em&gt; increases, the number of units that gets activated increases.
Which means as the variance of the visible units decreases, it increases the bottom-up values leading to more number of hidden units being activated.
Since there are more number of activated hidden units, top-down reconstruction is taken care of.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Approximation&lt;/strong&gt;&lt;/p&gt;

&lt;script type=&quot;math/tex; mode=display&quot;&gt;\langle y \rangle = \sum_{n=1}^{n=\infty}{\sigma(x+0.5-n)} \approx log(1+e^x) \approx max(0, x+noise)&lt;/script&gt;

&lt;p&gt;There comes the rectified linear units.&lt;/p&gt;

&lt;h2 id=&quot;equivalence-of-rbm-and-sigmoid-belief-net&quot;&gt;Equivalence of RBM and sigmoid belief net&lt;/h2&gt;
&lt;blockquote&gt;
  &lt;p&gt;An RBM is actually just an infinitely deep sigmoid belief net with a lot of weight sharing.&lt;/p&gt;
&lt;/blockquote&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/nn-notes/infinite-sbn.png&quot; alt=&quot;Infinite SBN&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The weights from any hidden to visible layer is W.
The effect of “explaining away” is taken care of by inifinite sigmoid net because when we compute bottom up value of hidden state then we are multiplying with the likelihood term (the weight matrix) and the prior term (complementary prior) itself.&lt;/p&gt;

&lt;p&gt;The inifinite SBN is learned by first sharing all the weights which is equivalent to the case of learning in RBM.
Once the weight between the last two layers is learned, they are fixed and the aggregate posterior distribution over h0 is used as data for the new layer of RBM.
Although, because of weights learned can differ from the frozen weights, the gain from better approximation of aggregated posterior distribution outweights the loss due to less accurate inference.&lt;/p&gt;

&lt;p&gt;In this section, it is explained why CDn works&lt;/p&gt;

&lt;h1 id=&quot;week-15-auto-encoders-and-semantic-hashing----modeling-hierarchical-structure-with-neural-nets&quot;&gt;Week 15 (Auto-encoders and semantic hashing – Modeling hierarchical structure with neural nets)&lt;/h1&gt;

&lt;h2 id=&quot;from-pca-to-autoencoder&quot;&gt;From PCA to Autoencoder&lt;/h2&gt;
&lt;p&gt;By finding how we can do PCA with neural networks, we can represent non-linear manifolds.&lt;/p&gt;

&lt;p&gt;We can use backprop to implement PCA inefficiently.
If we have a input layer that maps to a code which then maps to an output layer, such that the input and output layer of the same size is a simple auto-encoder.
If we the input to hidden and hidden to output layers are linear, then the network minimizes the squared reconstruction error exactly like PCA.
The M hidden units will span the same space as the first M components found by PCA.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Their M vectors may not be orthogonal&lt;/li&gt;
  &lt;li&gt;They will tend to have equal variance&lt;br /&gt;
That is, when the data points in XYZ are scattered on an XY plane, then the M vectors found be the encoder are in the XY plane only, but not exactly the same.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Deep Auto-encoders&lt;/strong&gt; Now, we can switch the input to hidden and hidden to output to non-linear and hope to do better than PCA.
This is more appealing because the training time in linear in number of training cases and during the test time all that needs to be done is matrix multiplication(s) to find the code.
They happened to be hard to train because the gradients tend to die with small initialization weights.&lt;/p&gt;

&lt;p&gt;They can be either repaired by careful initialization (like Echo State Networks) or pre-training.
They were shown to do well with stacked RBM layers from input to code and code to output such that the weights from code to outputs is the transpose of the input.&lt;/p&gt;

&lt;p&gt;Deep auto-encoders on documents do better than Latent Semantic Analysis (LSA).
LSA implements PCA over word vectors.
It is shown that Deep auto-encoders with 10D code do better than LSA with 50D. 
There are a few tricks that went into the training of the auto-encoder:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Since the word count vectors are not real value unlike images, they are normalized by dividing with N, the total number of words in the document. The vector then behaves like a probability vector, probability of picking the word from the document.&lt;/li&gt;
  &lt;li&gt;The input to hidden weights in the RBM are multiplied by N, since N samples of the distribution (not really sure what that means!). Nothing like that for hidden to output, though.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;semantic-hashing&quot;&gt;Semantic Hashing&lt;/h2&gt;
&lt;p&gt;Arrange stuff like in a super-market. 
Things close by are more similar.
The entities are hashed to a value that puts them closer to other similar.
The difference from auto-encoders is that the code is a binary vector unlike real values in the other case.&lt;/p&gt;

&lt;p&gt;Architecture to generate this is very similar to auto-encoders except that the code units are now logistic units.
Since the logistic units can also generate values between 0 and 1, we add noise to the inputs to the code units to force the logistic units to either go to 1 or 0, since it cannot rely on between values when the input is noisy.
Instead, it is possible to just use stochastic binary units which activates stochastically based on the input from logistic unit and while backprop, we pretend that we have passed the real values of the logistic unit so that we get smooth gradients.&lt;/p&gt;

&lt;p&gt;In the case of image retrival where we may not be able to encode all the information in the short code, we use a smaller code for semantic hashing which gives a list of “good” set of candidate images and can then use sequential search for most closest one with larger code.&lt;/p&gt;

&lt;p&gt;In the case of images, a clever trick is employed to make the network focus more on the objects in image rather than the pixels in itself.
For example, we want the nertwork to consider several images of elephant in different poses to be the same although the pixel intensity distributions can be quite different.
For this reason, a model like AlexNet is used and the hidden activities in the last layer are what is encoded.&lt;/p&gt;

&lt;h2 id=&quot;using-auto-encoders-for-pre-training&quot;&gt;Using Auto-encoders for Pre-training&lt;/h2&gt;
&lt;p&gt;The auto-encoders when trained on unlabeled data can discover a code which is basically a collection of good features over the data.
It is possible to use these features further in discriminative training. 
Especially when the labeled data is limited, it is possible to find the features with auto-encoder which can then be reweighted using the limited training data.&lt;/p&gt;

&lt;p&gt;RBMs can be seen as autoencoders because they minimize the distance between the input and the reconstruction (with CD1 learning).
If we train RBM with ML, if the noise is fed to an autoencoder, it will try to reproduce the noise, whereas RBM ignores the input and learns only the bias param.
However, it is limited because the hidden states can only be binary.&lt;/p&gt;

&lt;p&gt;Denoising and Contractive encoders do better job at pre-training.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Denoising autoencoder&lt;/strong&gt; adds noise to the input by setting many of its c0omponents zero (like dropout on inputs). 
That way, the network is forced to reconstruct the input and hence forced to learn the correlations between the components.
This avoids the risk that a hidden state can just copy the value of one of its component.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Contractive encoder&lt;/strong&gt; presents an alternative of penalizing the squared gradient of each hidden activity wrt the inputs.
The codes tend to have the property that only a small subset of teh hidden units are sensitive to the changes in input.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Pre-training was the first good way to initialize the weights for deep nets, but now there are better ways&lt;/strong&gt;&lt;/p&gt;

&lt;h1 id=&quot;good-questions-from-final-quiz&quot;&gt;Good questions from final quiz&lt;/h1&gt;

&lt;ul&gt;
  &lt;li&gt;The contrastive divergence (CD 10) has advantage over CD1 that the reconstruction generated by CD10 are closer to the model distribution than in the case of CD1. 
In the case of CD1, it is easy to compute the gradients and the computed gradients have low variance.&lt;/li&gt;
  &lt;li&gt;In an RBM, there are no connections between hidden states and between visible states, the hidden states are independent given the visible states.
SBNs on the other hand suffer from the explainining away property because the posterior distributons are no longer independent given the visible states.&lt;/li&gt;
  &lt;li&gt;The sole purpose of momentum is to speed up learning, but not to provide any qualitative improvements.&lt;/li&gt;
&lt;/ul&gt;
</description>
        <pubDate>Wed, 15 Mar 2017 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/deep-learning/notes/coursera/2017/03/15/notes.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep-learning/notes/coursera/2017/03/15/notes.html</guid>
        
        
        <category>deep-learning</category>
        
        <category>notes</category>
        
        <category>coursera</category>
        
      </item>
    
      <item>
        <title>Literature Survey on Adaptive Learning in the context of neural networks</title>
        <description>&lt;h1 id=&quot;introduction&quot;&gt;Introduction&lt;/h1&gt;

&lt;p&gt;Adaptation of neural network models in the context of speech, speaker adaptation, is well researched. 
Acoustic models have evolved starting from GMM or HMM models to hybrid models that pushed in to the neural network regime, making way for deep neural networks to LSTM based RNN models.
The problem of low performance due to train and test domain differences is acute in speech and is well addressed in the past even before the introduction of neural networks.
In this post, I will discuss the domain adaptation problem only in the context of neural nets.&lt;/p&gt;

&lt;h2 id=&quot;speaker-adaptation-of-hybrid-nnhmm-based-on-speaker-codesspeaker-code&quot;&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/document/6639211/&quot; title=&quot;Fast speaker adaptation of hybrid NN/HMM model for speech recognition based on discriminative learning of speaker code&quot;&gt;Speaker Adaptation of hybrid NN/HMM based on speaker codes&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;In a NN/HMM model, the GMM component that is used to estimate the emission probabilities of each state is replaced by neural network which when fed with the feature vector, outputs the posterior over HMM state labels.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speaker-code-fig1.png&quot; alt=&quot;speaker-code-fig1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the image above, the proposed adaptation method relies on learning adaptation NN and speaker codes.
All the layers in adaptation NN are standard fully connected layers.
The transformed feature vector should have the same dimension as the input feature vector.
During the adaptation phase, both the weights of adaptation NN and speaker need to be learned.&lt;/p&gt;

&lt;h3 id=&quot;training&quot;&gt;Training&lt;/h3&gt;

&lt;p&gt;The training of speaker independent model and adaptive NN along with speaker code is carried in two separate steps.
Speaker Independent (SI) model is learned as if adapt NN does not exist, that is standard NN-HMM model is trained without using any speaker specific information.&lt;br /&gt;
&lt;strong&gt;Once the SI model is trained, its weights are freezed and speaker code and adaptNN weights are learned jointly with back-propagation to optimize frame-wise classification performance.&lt;/strong&gt;&lt;br /&gt;
Acknowledges that there are several other plausible ways of training the network, but do not provide any rationale as to why this method was choosen as such.&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;it is possible to tweak the weights of SI model when optimizing for speaker codes and adaptNN weights.&lt;/li&gt;
  &lt;li&gt;Learn all the three parameters jointly. “However, this may result in two inseparable NNs and they eventually become one large deep NN with only a number of lower layers receiving a speaker code.”&lt;/li&gt;
  &lt;li&gt;Another possibility is to learn SI model over the features transformed with an already trained adaptNN, that is to flip the order in which we train.&lt;/li&gt;
&lt;/ul&gt;

&lt;h3 id=&quot;adaptation&quot;&gt;Adaptation&lt;/h3&gt;

&lt;p&gt;During this phase, only the speaker code is to be learned for the new speaker over the small amount of data available for adaptation.
This is one of the strong points of this work that the speaker code can be arbitrarily made small or large depending on how much data is available for adaptation.
We do the same, BP, but adaptNN weights are freezed as well.&lt;/p&gt;

&lt;h3 id=&quot;interesting-experiments&quot;&gt;Interesting experiments&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speaker-code-fig2.png&quot; alt=&quot;fig2&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The figure above shows the effect of number of examples used for estimating speaker code on performance.&lt;br /&gt;
The experiment was conducted on a 462 speaker training set and 24-speaker test set.
The test set each contain eight utterances per user.
The learning rate, context window are al fixed, hidden layer width (1000), speaker code size (50), 183 target class labels and feature vector dimension (??) are all fixed.&lt;/p&gt;

&lt;p&gt;Note&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;“Dummy” is when no speaker code, but only dummy layers – does not affect the performance meaning that the perf. improvement is not just increased model complexity.&lt;/li&gt;
  &lt;li&gt;using zero adaptation has some positve effect.&lt;/li&gt;
  &lt;li&gt;Even when exposed to one utterance, the perf. improvement is not bad.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;using-i-vector-inputs-to-improve-speaker-independence&quot;&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=6853591&quot; title=&quot;IMPROVING DNN SPEAKER INDEPENDENCE WITH I-VECTOR INPUTS&quot;&gt;Using I-Vector inputs to improve speaker independence&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Leveraging utterance-level features as inputs to DNN to facilitate speaker, channel and background normalization.&lt;/p&gt;

&lt;h3 id=&quot;i-vectors-or-identity-vectors&quot;&gt;i-Vectors or identity vectors&lt;/h3&gt;

&lt;p&gt;&lt;strong&gt;“i-vectors encode precisely those effects to which we want our ASR system to be invariant: speaker, channel and background noise.”&lt;/strong&gt;
These vectors are generally used in speaker recognition and verification&lt;/p&gt;

&lt;h3 id=&quot;adapting-with-i-vectors&quot;&gt;Adapting with i-vectors&lt;/h3&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/google-ivector-fig1.png&quot; alt=&quot;google-ivec-fig1&quot; /&gt;&lt;/p&gt;

&lt;p&gt;As shown in the image above, the idea is to provide the input with characterisation of the speaker, which could enable it to normalise the signal with respect to speaker specific nuances and thus leading to a better Speaker Independent model.&lt;/p&gt;

&lt;h3 id=&quot;experiments&quot;&gt;Experiments&lt;/h3&gt;

&lt;p&gt;The training and dev set performance differed when the input is compounded with the 300 dimensional i-vector. 
This could mean that the network is over-fitting the i-vectors or it could also be that the computing a 300-dim vector from short utterances is not relaiable.&lt;/p&gt;

&lt;p&gt;Reducing the i-vector dimension, to say 20, along with l2 regularization helped.&lt;/p&gt;

&lt;p&gt;The dataset contains 80 speakers with an equivalent of 10 minutes of utterance per user.
The input augmentation with 20-dimensional i-vector model along with re-training on the adaptation set with l2 reg. coeff of 0.01 improved the results further.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ivec-adapt-results.png&quot; alt=&quot;ivec-adapt-results&quot; /&gt;&lt;/p&gt;

&lt;p&gt;This work claims that when the network with input augmented with i-vectors is also adapted over the user-data, it can lead to better performance as shown in the figure above.
It is not mentioned if any weights are fixed while adapting, it is probably that none of the weights are fixed and are all jointly optimized over various passes on the user data.
The baseline model is a normal feed-forward neural network.&lt;/p&gt;

&lt;h2 id=&quot;speaker-adaptive-deep-neural-networks&quot;&gt;&lt;a href=&quot;https://www.cs.cmu.edu/~ymiao/pub/tasl_sat.pdf&quot; title=&quot;Towards Speaker Adaptive Training of Deep Neural Network Acoustic Models&quot;&gt;Speaker adaptive deep neural networks&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Two different model architectures are tried with: AdaptNN and iVecNN.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ivecNN.png&quot; alt=&quot;models&quot; /&gt;&lt;/p&gt;

&lt;p&gt;iVecNN works by producing a linear feature shift which is added to the original feature vector and is activated with a linear activation function.&lt;br /&gt;
&lt;script type=&quot;math/tex&quot;&gt;a_t = o_t+f(i_s)&lt;/script&gt;&lt;br /&gt;
The weights of iVecNN are estimated using BP while the weights of the initial DNN are estimated and fixed.&lt;/p&gt;

&lt;p&gt;The strong point of this method is its relevance to CNNs.&lt;/p&gt;

&lt;p&gt;In the figure above, &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt; is the element-wise sum of &lt;script type=&quot;math/tex&quot;&gt;o_t&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;y_s^{-1}&lt;/script&gt;.
For two speakers in the training set, two principal components from PCA are plotted as shown in the image below. 
Observe that the non-overlapping regions has shrunk for the case of &lt;script type=&quot;math/tex&quot;&gt;z_t&lt;/script&gt; when compared to &lt;script type=&quot;math/tex&quot;&gt;o_t&lt;/script&gt; implying that adding a linear shift to the original vector is actually making the speaker independent.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/speaker-indep-ivecNN.png&quot; alt=&quot;speaker independence&quot; /&gt;&lt;/p&gt;

&lt;p&gt;The training pipeline of the system is shown below.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/ivecNN-pipeline.png&quot; alt=&quot;pipeline&quot; /&gt;&lt;/p&gt;

&lt;p&gt;They did show that the model is better than DNN+i-vector, that is the augmented input, the first model in this article, and concluded that this model is better than the other.
However, the experiment was not set right, the i-vectors are not normalized and i-vector size is not varied.
I am not including results because I did not like them. (I have some issues with how the experiment was set-up)&lt;/p&gt;

&lt;h1 id=&quot;adaptation-in-the-context-of-hand-writing-recognition&quot;&gt;Adaptation in the context of hand writing recognition&lt;/h1&gt;

&lt;p&gt;I have not come across any work that adapts hand writing recognition to a user. 
There is some interest in recognizing what is called as unconstrained hand writing recognition task which is recognizing hand written characters with no restrictions imposed on their style, size, position and medium.&lt;/p&gt;

&lt;h2 id=&quot;generating-sequences-with-recurrent-neural-networksgen-hw&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1308.0850v5.pdf&quot; title=&quot;Generating Sequences With Recurrent Neural Networks&quot;&gt;Generating Sequences With Recurrent Neural Networks&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This work is an interesting read, although it does not explicitly make user modeling.
The model can generate hand writen sentences that resemble the ones written by human.
Also interesting is that it is possible to tune the style of the generated sentence instead of randomly choosing one.
This work demonstrates that it is possible to generate such writings one point at a time with RNNs that are also consistent with a style.&lt;/p&gt;

&lt;p&gt;Unlike others, no pre-processing of the data (online data) is made.
According to them, pre-processing will normalize and removes variance in the input and will lead an output that is more synthetic.&lt;/p&gt;

&lt;h2 id=&quot;a-novel-connectionist-system-for-unconstrained-handwriting-recognitiongarves-09&quot;&gt;&lt;a href=&quot;http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4531750&quot; title=&quot;A Novel Connectionist System for Unconstrained Handwriting Recognition&quot;&gt;A Novel Connectionist System for Unconstrained Handwriting Recognition&lt;/a&gt;&lt;/h2&gt;

&lt;h2 id=&quot;meta-learning-with-memory-augmented-neural-networksmemaug-one-shot&quot;&gt;&lt;a href=&quot;http://jmlr.org/proceedings/papers/v48/santoro16.pdf&quot; title=&quot;Meta-Learning with Memory-Augmented Neural Networks&quot;&gt;Meta-Learning with Memory-Augmented Neural Networks&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;Given a small amopunt of data, a straightforward gradient based solution is to completely relearn the parameters from the data available, which can lead to poor learning due to interference.
One-shot learning is quite hard because of the interference effects due to the training params learned over much larger data.
&lt;code class=&quot;highlighter-rouge&quot;&gt;Architectures with augmented memory capacities, such as Neural Turing Machines (NTMs), offer the ability to quickly encode and retrieve new information, and hence can potentially obviate the downsides of conventional models.&lt;/code&gt;&lt;/p&gt;

</description>
        <pubDate>Wed, 15 Mar 2017 00:00:00 +0530</pubDate>
        <link>http://localhost:4000/deep-learning/speaker-adaption/adaptive-neural-networks/2017/03/15/NN_adaptation.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep-learning/speaker-adaption/adaptive-neural-networks/2017/03/15/NN_adaptation.html</guid>
        
        
        <category>deep-learning</category>
        
        <category>speaker-adaption</category>
        
        <category>adaptive-neural-networks</category>
        
      </item>
    
      <item>
        <title>Transfer learning in the context of deep learning</title>
        <description>&lt;h1 id=&quot;transfer-learning-in-the-context-of-deep-learning&quot;&gt;Transfer learning in the context of deep learning&lt;/h1&gt;

&lt;h2 id=&quot;cnn-features-off-the-shelf-an-astounding-baseline-for-recognitionrit13&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1403.6382v3.pdf&quot;&gt;CNN Features off-the-shelf: an Astounding Baseline for Recognition&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This paper explores the potential of features generated by Convolutional Neural Network by using the features learned by Overfeat model for object classification task in ILSVRC13.&lt;br /&gt;
The features are appllied for various tasks such as &lt;em&gt;object classification, fine-grained recognition, scene recognition, attribute detection (for example attributes of pedestrians from a cam feed that is if they are wearing a hat, shoe, trouser, jeans etc.) and scene retrieval&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;object-recognition&quot;&gt;Object recognition&lt;/h3&gt;

&lt;p&gt;Is the task of assigning labels (possibly more than one) to a given image. 
Since the features used are learned over a similar task, better results are expected. 
They tried on PASCAL VOM and MIT indoor dataset both of which are considered harder than ImageNet.&lt;br /&gt;
The mean Average Precision(mAP) over all the classes on PASCAL VOM is 77.2, which is 7 point more than second best. 
Thing to note is that, Oquab et.al. fixes all the layers trained on ImageNet and then it adds and optimized two fully-connected layers on VOC dataset with &lt;strong&gt;77.7&lt;/strong&gt; mAP.&lt;/p&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;

&lt;p&gt;This is about recognising and also putting a box around the object.
With off-the-shelf features: &lt;strong&gt;46.2&lt;/strong&gt; mAP (Girshick et.al.) and when fine-tuned on PASCAL VOC it is &lt;strong&gt;53.1&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The graph below summarizes the contribution of this paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/feature-transfer13.png&quot; alt=&quot;Comparison of CNN features with other baselines&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decafdecaf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1310.1531v1.pdf&quot;&gt;DeCAF&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This work is quite similar to the above one. 
The features are extracted from the &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot; title=&quot;ImageNet classification with deep convolutional neural networks. &quot;&gt;AlexNet&lt;/a&gt; which contains 5 convolutional layers (not inluding pooling) followed b7y two fully connnected layers.&lt;br /&gt;
Features are extracted from the output of 5th, 6th and 7th layer; notice that 7th is last but one layer and 5th has only convolutional layers. 
AlexNet has dropout in layers 6 and 7. 
Experiments were carried on the tasks of Object detection, fine-grained recognition, domain adaptation and scene recognition.&lt;/p&gt;

&lt;h3 id=&quot;object-recognition-1&quot;&gt;Object Recognition&lt;/h3&gt;

&lt;p&gt;It is found in their experiments that logistic Regression or linear SVM on these features already beat methods that involve task-specific features with SVM using multiple kernels by around 2-3% on mean accuracy.
Things to note, the mean accuracy was around 33% when the training set contains only one example per class. 
&lt;em&gt;The performance of the system on 7th level features is slightly less (by 2%) when compared to 6th level feature.&lt;/em&gt;
Dropout on layers: 6/7 improves perf. by 0-2%.
This method beats two-layer convolution network trained on only the task-specific training data by 20%.&lt;/p&gt;

&lt;h3 id=&quot;domain-adaptation&quot;&gt;Domain Adaptation&lt;/h3&gt;

&lt;p&gt;This entire work is a kind of domain adapatation. 
This section explores domain apaptability of featutures extracted from Conv. layers.
In this task, the features are compared over their perf. on multi-class accuracy on ffice dataset collected from Amamzon, webcam and camera snapshots.
The ConvNet features beat SURF features by a large margin which means that this hand-engineered feature do not represent all the knowledge required to classify as well as convNet layers. 
The fact that logistic regression over source, target or combined data did not perform well and Daume III (next section) and SVM did well means that there are several features that are specific to a certain domain and feature reweightinghelps to increase the performance from &lt;strong&gt;75.30&lt;/strong&gt; for best performing log. regr. model to Daume III.&lt;/p&gt;

&lt;p&gt;The table below is an interesting read. 
Observe that the in the case of Dslr&lt;script type=&quot;math/tex&quot;&gt;\rightarrow&lt;/script&gt;webcam, the performance of log. regression model when trained on source data is better than when trained on target data. 
This indicates that the domains are not very different which is understandable since they only differ in resolution. 
Due to this, Daume III did not do any better than logistic regression on source and target data combined.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/decaf-domain-adapt.png&quot; alt=&quot;Domain Adaptation with AlexNet features&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-transferable-are-features-in-deep-neural-networkstransfer-bengio-14&quot;&gt;&lt;a href=&quot;https://arxiv.org/abs/1411.1792&quot;&gt;How transferable are features in deep neural networks?&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;Unlike other papers that measure transfer of features from ImageNet to various image-processing task, this work takes a different approach. 
Transfer learning is tested on mutually exludive splits of ImageNet data containing 1000 classes. 
The 1000 class collection is either split randomly that is both the splits still contain 1000 classes but the examples in each class will be shared equally between them.&lt;/p&gt;

&lt;p&gt;In a different experiment, the data is split to form two sets that are very different from other.
The 1000 classes are split into almost equal sets based on if it is man-made or natural, we will refer to this split as non-random split.&lt;/p&gt;

&lt;p&gt;The effect of coadaptation when using features from different layers is isolated, thet is when we are using features from say a 4th or 5th layer in a seven layer netweork then, the activations of this layer might have co-adapted with the next layer.&lt;/p&gt;

&lt;h2 id=&quot;frustratingly-easy-domain-adaptationeasyadapt&quot;&gt;&lt;a href=&quot;http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf&quot; title=&quot;Frustratingly Easy Domain Adaptation&quot;&gt;Frustratingly Easy Domain Adaptation&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This is a 2007 paper that propses a simple pre-processing trick for easy adaptation. 
The DeCAF paper reports best results in the task of domain adaptation from Amazon images -&amp;gt; webcam images using the features from AlexNet and this method. 
That interested me to study this further.
Let me start with the simple code.&lt;/p&gt;

&lt;div class=&quot;language-perl highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#source: http://hal3.name/easyadapt.pl.gz&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;@ARGV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ARGV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;&amp;lt;F&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;chomp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;@x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; *$_ $i$_&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;The snippet above has the following input-output characteristics.&lt;/p&gt;
&lt;div class=&quot;language-text highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;====File 1====
x a v f
y a c d
===File 2===
x d f
y g d f
===Output===
x *a 0a *v 0v *f 0f
y *a 0a *c 0c *d 0d
x *d 1d *f 1f
y *g 1g *d 1d *f 1f
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;p&gt;The output needs to be further processed before it is used, more specifically any symbol in input that starts with * is general and starting with ‘%d’ belongs to that number slot in the input.&lt;/p&gt;

&lt;p&gt;Consider the case where there is a loads of data in one domain but is different from the domain that we want to use it in. 
For example, we might want a PoS tagger trained on NewsWire to work on hardware blogs. 
That is, we have data from source domain and some amount of data from target domain; there are sevearal standard ways to adapt in such a case. 
According to the author, these approaches are surprisingly hard to beat.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SRCOnly: A single model is trained on only data from source data alone ignoring target data.&lt;/li&gt;
  &lt;li&gt;TGTOnly: Model trained only on target data alone&lt;/li&gt;
  &lt;li&gt;ALL: A model is trained on both the source and target domain. 
Can wash away the effects of target data since it is small relatively.
For this reason, the source data eaxamples are weighed down by that factor to bring balance. 
WEIGHT is our next baseline which chooses the scaling factor of source examples by CV.&lt;/li&gt;
  &lt;li&gt;PRED: Use the output of model trained on SRC data as feature for training on target domain.&lt;/li&gt;
  &lt;li&gt;LinINT: Linearly interpolate the predictions of src and target models while the interpolation params are from dev. data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two models are found to have beat these&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;PRIOR: train a model on target data with priors from model trained on source data. 
It is about just adding a regularization term like this: &lt;script type=&quot;math/tex&quot;&gt;\lambda{\lvert\lvert{w-w_s}\rvert\rvert _2}^2&lt;/script&gt;&lt;/li&gt;
  &lt;li&gt;By the very author, key idea is to learn three different models: one for each of source, target and general. 
The idea is very similar to the work presented in this paper, distinction is made for each example: if it is source specific or general or target-specific or general.
The EM algo. presented is quite slow, so this paper is redemtion of sorts.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;This work presents a simple augmentation of data that transforms source and target data differently with &lt;script type=&quot;math/tex&quot;&gt;\phi^s(\mathbf{x})&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\mathbf x,\mathbf x,0&gt; %]]&gt;&lt;/script&gt;, &lt;script type=&quot;math/tex&quot;&gt;\phi^t(\mathbf{x})&lt;/script&gt;=&lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
&lt;\mathbf x,0,\mathbf x&gt; %]]&gt;&lt;/script&gt;.
The intuition is that any by replicating the feature space thrice one for general, source-specific and target-specific, the features that are specific to say target or source will remain in their respective dimensions and common ones in the general dimension.&lt;/p&gt;

&lt;p&gt;An interesting takeaway is that things did not go south because of teh increased dimensionality of feature space. 
The methodfailed to perform in cases where the source and target domains did not differ by much.&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Oct 2016 16:42:25 +0530</pubDate>
        <link>http://localhost:4000/deep-learning/transfer-learning/2016/10/25/transfer-learning.html</link>
        <guid isPermaLink="true">http://localhost:4000/deep-learning/transfer-learning/2016/10/25/transfer-learning.html</guid>
        
        
        <category>deep-learning</category>
        
        <category>transfer-learning</category>
        
      </item>
    
      <item>
        <title>Notes from my independent study of deep learning</title>
        <description>&lt;h2 id=&quot;practical-aspects-of-training&quot;&gt;Practical aspects of Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1206.5533v2.pdf&quot; title=&quot;Practical Recommendations for Gradient-Based Training of Deep Architectures&quot;&gt;Recommendations by Y. Bengio&lt;/a&gt;&lt;br /&gt;
  The recommendations are not definitive and should be challenged.
  They can work as a good starting point.&lt;br /&gt;
  It has been shown that use of computer clusters for hyper-parameter selection can have an important effect on results. 
  The value of some hyper-parameters can be selected based on its performance on training data, but most cannot. For any hyper-parameter that affects the capacity of the learner, it makes more sense to use an out-of-sample data to select the params, hence the validation set.&lt;br /&gt;
  	&amp;gt; different researchers and research groups do not always agree on the practice of training neural networks&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Initial Learning rate (&lt;script type=&quot;math/tex&quot;&gt;\epsilon_0&lt;/script&gt;)&lt;/strong&gt; is the single most important hyp. param. according to him. &lt;code class=&quot;highlighter-rouge&quot;&gt;If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.&lt;/code&gt; Typical values for neural networks is: 1E-6 to 1 that is if the input is properly normalized to lie between 0 and 1. A default value of 0.01 typically works for multi-layer neural network.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Learning rate schedule&lt;/strong&gt;: The general strategy is to keep the learning rate constant until some iterations as shown in the equation below. In many cases choosing other than the default value for &lt;script type=&quot;math/tex&quot;&gt;\tau\rightarrow \infty&lt;/script&gt; has very little effect. 
There are suggestions to decrease the learning rate less steeply than linear after &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; time steps that is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
O(1/t^\alpha), \alpha&lt;1 %]]&gt;&lt;/script&gt; depending on the convex behaviour of the function being optimized.
Adaptive strategies for learning rate exist.&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_t=\frac{\epsilon_0 \tau}{max(t,\tau)}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mini-batch size (B)&lt;/strong&gt;: In theory, this hyp. param. should only affect the training time and not performance. Typically, B is a value between 1 and a few hundred. A value of 32 is good because it can take implementational advantages that matrix-matrix product are more efficient than matrix-vector and is not too high to require a long training time. 
B can be optimized independently of others. Once a value is choosen, it can be fixed and is better to re-optimize in the end since it weakly interacts with other hyp. params.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Number of Training iterations (T)&lt;/strong&gt;: To be choosen by early-stopping criteria. During the analysis stage where we try and compare different models, stopping early can have evening out effect. That is we cannot make-out between an over-fitting or under-fitting model. For this reason, it is best to turn it off during analysis. Another param called &lt;em&gt;patience&lt;/em&gt; is generally defined that tells how long the model should wait after it observed the minimum validation error, the param is defined in terms of minimum number of examples to be seen before stopping.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Momenum (&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;)&lt;/strong&gt;: &lt;script type=&quot;math/tex&quot;&gt;\hat{g}\leftarrow (1-\beta)\hat{g}+\beta g&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\hat{g}&lt;/script&gt; is the smoothed gradient. The default value of &lt;script type=&quot;math/tex&quot;&gt;\beta=1&lt;/script&gt; works for most cases, but is found to help in some cases of unsupervised learning. “The idea is that it removes some of the noise and oscillations that gradient descent has, in particular in the directions of high curvature of the loss function”
 In some rare cases, layer-specific hyper paramater optimization is employed. This makes sense when the number of hidden units vary large between layers. Generally employed in layer-wise unsupervised pre-training.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The parameters discussed above are related to SGD (Stochastic Gradient Descent), what follows are the params related to the neural network.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Number of hidden units &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt;&lt;/strong&gt; Because of early stopping and possiblty other regularizers such as weight decay, it is mostly important to choose &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt; large enough. In general, it is observed that seeting all the layers to equal number of units works better or the same as when setting the hidden unit widths in decreasing or increasing order (pyramidal or reverse pyramidal), but this could be data-dependent (&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/1111/jmlr10_larochelle.pdf&quot; title=&quot;Exploring Strategies for Training Deep Neural Networks&quot;&gt;Larochelle et.al. 2014&lt;/a&gt;). 
 An over-complete (larger than the input vector) is better than an under-complete one.
 A more validated observation is that optimal &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt; value when training with unsupervised pre-training in a supervised neural network. Typically from 100 to 1000. This could be because unsupervised pre-training could hold lot more information that is not relevant to the task and hence require large hidden layers to make sure relevant information is captured.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Weight decay:&lt;/strong&gt; This article makes a very inetersting note about thsi parameter. As we know, weight decay is used to avoid over-fitting by limiting capacity of the learner. L2 or L1 regularization correspond to the penalties: &lt;script type=&quot;math/tex&quot;&gt;\lambda \sum_i {\theta_i}^2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda \sum_i{\theta_i}&lt;/script&gt;, both the terms can be included. In the case of batch-wise handling of data, the param &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that we optimize is actually &lt;script type=&quot;math/tex&quot;&gt;\frac{\lambda'*B}{T}&lt;/script&gt; where B is batch size and T is the size of training data.
L2 regularization corresponds to a guassian prior (over weights) &lt;script type=&quot;math/tex&quot;&gt;\propto exp^{\frac{-1 \theta^2}{2 \sigma^2}}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\sigma^2=1/2\lambda&lt;/script&gt;.
&lt;em&gt;Note that there is a connection between L2 regularization and early stopping with one basically playing the same role as other&lt;/em&gt;. L1 regularization is different and sometimes act as feature selectors by making sure the parameters that are not really very useful go to 0. L1 corresponds to laplace density prior &lt;script type=&quot;math/tex&quot;&gt;\propto e^{-\frac{|\theta|}{s}}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;s=\frac{1}{\lambda}&lt;/script&gt;&lt;br /&gt;
 It is sufficient to regularize just the output weights in order to constrain the capacity. (we use the input weights and output weights to denote weights corresponding to the first and last layer. The input weights is also often referred to as &lt;em&gt;filters&lt;/em&gt; because of analogies with signal processing techniques)&lt;br /&gt;
 Using an L1 regularizer helps to make the input filters cleaner and easier to interpret. 
 We may draw that L1 cleans the input weights and L2 the output weights. 
 When we introduce both the penalties into our optimization, then it is required to tune the coeffs for L1 and L2 independently. In particluar, input and output weights are treated different.&lt;br /&gt;
 &lt;em&gt;In the limit case of the number of hidden layers going to infinity, L2 regularization corresponds to SVMs and L1 to Boosting (&lt;a href=&quot;https://papers.nips.cc/paper/2800-convex-neural-networks.pdf&quot; title=&quot;Convex Neural Networks&quot;&gt;Bengio et.al. 2006&lt;/a&gt;)&lt;/em&gt;&lt;br /&gt;
 One of the reason why we cannot rely only on early stopping criteria and treat input and output weights differently from hidden units is because they may be sparse. For example, some input features could be 0 more frequently and others non-zero more frequently. A similar situation may arise when target variable is sparse i.e. trying to predict a rare event. In both cases, the effective number of meaningful update (active feature or rare event) seen by these params is less than the actual number of updates. The parameters (weights outgoing from the corresponding input) of such sparse examples should be more regularized, that is to scale the reg. coeff. of these params by one over effective number of updated seen by the parameter. This presents an alteranaate way to deal with imbalanced data or anamolies(?).&lt;/li&gt;
      &lt;li&gt;There are several approaches that aim to minimize the sparsity of hidden layers (note that sparsity is very different from L1 norm)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Non-linear activation functions&lt;/strong&gt;: Popular choices are: sigmoid &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{1+e^{-a}}&lt;/script&gt;, the hyperbolic tangent &lt;script type=&quot;math/tex&quot;&gt;\frac{e^a-e^{-a}}{e^a+e^{-a}}&lt;/script&gt;, rectifier max; max(0,a) and hard tanh.&lt;br /&gt;
Sigmoid was shown to yield serious optimization difficulties when used as the top hidden layer of a deep supervised network without unsupervised pre-training. 
&lt;code class=&quot;highlighter-rouge&quot;&gt;For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &amp;lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error. For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &amp;lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error&lt;/code&gt; 
The general trick is to use linear output and squared error for Gaussian output model, cross-entropy and sigmoid output for binomial output model and log output[target class] with softmax outputs to correspond to multinomial output variables (that is take softmax for over all the neuron outpus and score by considering only target label required).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Weights initiazation and scaling coefficient&lt;/strong&gt;: The weights should be initialized randomly inorder to break symmetry and bias can be initialized to 0. 
If not all the neurons initially will produce the same output and hence receive the same gradient, wasting the capacity.
The scaling factor controls how small or big the initial weights are. Units with large input (fan-in of the unit) should have smaller weights (I have first-hand experience with problems that arise when this is not done with one of my NN assignments. The initialization that worked smaller number of hidden units just over-shooted due to explosive gradients. That is because the output diverged to a large value when I did this)&lt;br /&gt;
The recommendation made is either to sample &lt;em&gt;Uniform(-r,r)&lt;/em&gt;. 
Where r is &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\frac{6}{fan-in+fan-out}}&lt;/script&gt; for hyperbolic tangent units and  &lt;script type=&quot;math/tex&quot;&gt;4*\sqrt{\frac{6}{fan-in+fan-out}}&lt;/script&gt;. fan-in and fan-out are the input and output dimension of a hidden layer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;General advice on finding the best model.&lt;br /&gt;
  Numerical hyper-parameters need to be grid-searched in order to find one. 
  It is not sufficient to conclude the best value based on comparison with less than 5 other values. &lt;br /&gt;
  Scale of values considered is often an important decision to make, this is the starting interval in which the values will be looked up. 
  It makes more sense to sample values uniformly in the log space of such interval than to blindly evaluate at every value because the perf. at say 0.01 and 0.011 is likley to remain the same.&lt;br /&gt;
  Strategies for hyper-param selection: Coordinate descent and Multi-resolution search.
  Coordinate descent: Make changes to the each hyper-param one at a time, find the best value for the param and move on to the next one. 
  Mult-resolution search: There is no point in fine-tuning or high-resolution search over large intervals. Do a low-resolution search over several settings and then high-res search over best configurations.&lt;/p&gt;

&lt;h2 id=&quot;limitations-of-deep-learning&quot;&gt;Limitations of Deep Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.1897.pdf&quot;&gt;DNNs are easy to fool&lt;/a&gt; Surprisingly, DNNs can be easily fooled by adding adverse perturbations to an image. For example, adding noise that is imperceptable to humans to an image that looks like &lt;em&gt;panda&lt;/em&gt; and recognized as one with confidence of ~56% will lead to an image that is wrongly labeled but with very high confidence. This paper details about very interesting case-study of where the state-of-art AlexNet utterly fails. For code and images &lt;a href=&quot;http://www.evolvingai.org/fooling&quot; title=&quot;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images &quot;&gt;see&lt;/a&gt;. There has also been some effort at explaining this phenomena. In the paper: &lt;a href=&quot;https://arxiv.org/pdf/1412.6572v3.pdf&quot; title=&quot;Explaining and Harnessing Adversial Examples&quot;&gt;Explaining and Harnessing Adversial Examples&lt;/a&gt; by Ian Goodfellow et. al., they make a case that such a thing happens majorly because the DNNs are linear in nature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wisdom-from-random-sources&quot;&gt;Wisdom from random sources&lt;/h2&gt;

&lt;h3 id=&quot;hinton-at-stanford-can-the-brain-do-back-propagationgeoffrey-stanford&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VIRCybGgHts&quot; title=&quot;Can the Brain do back-propagation?&quot;&gt;Hinton at Stanford (Can the brain do back-propagation?)&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Big data is good (something that frequentist statisticians suggest)
    &lt;ul&gt;
      &lt;li&gt;For any given size of model, its better to have more data&lt;/li&gt;
      &lt;li&gt;But it’s a bad idea to try to make the data look big by making the model small&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Big models are good (Something that statisticians do not believe but true)
    &lt;ul&gt;
      &lt;li&gt;For any given size of data, the bigger the model, the better it generalized, provided you regularize it well.&lt;/li&gt;
      &lt;li&gt;This is obviously true id your model is an ensemble of smaller models. Adding extra models to the ensemble alwqays helps.&lt;/li&gt;
      &lt;li&gt;It’s a &lt;strong&gt;good idea&lt;/strong&gt; to try to make the data look small by using a big model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dropout enables the all the models in an ensemble to share knowledge. If there is only one layer in the network with with H, then the model with dropout is an ensemble of 2^H models and softmax over the output layer is geometric mean of all the models in ensemble.&lt;/li&gt;
  &lt;li&gt;Dropout can be seen as bernoulli noise, we do not change the expected value because a neuron either emits zero or twice the value. It is noted that any other kind of noise can work just as well. Gaussian noise and Possion noise are tested to give same performance if not better. In these cases a multiplicative noise with standard deviation equal to the activity. The point is that neurons do not share real values but spikes and that is a lot better than trhe actuaol values.&lt;/li&gt;
  &lt;li&gt;In this lecture, Hinton goes on lengths elaborating why brains cannot do exact back-propagation and explains possible other ways in which it could be learning the weights. He argues that neurons in a feed-back loop can do away with the need to back-propagate by considering the difference between the inout at this instance and previous one (plasticity of brain, Spike-time dependent plasticity)&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;debugging-in-tensorflow&quot;&gt;Debugging in Tensorflow&lt;/h2&gt;
&lt;p&gt;Debugging or even understanding a tensorflow code can be a daunting task. 
Here in this section, I will share some tricks of trade.&lt;/p&gt;

&lt;h3 id=&quot;a-practical-guide-to-debugging-tensorflow-codestf-debug&quot;&gt;&lt;a href=&quot;https://wookayin.github.io/TensorflowKR-2016-talk-debugging&quot; title=&quot;A practical guide to debugging tensorflow codes&quot;&gt;A practical guide to debugging tensorflow codes&lt;/a&gt;&lt;/h3&gt;
&lt;ul&gt;
  &lt;li&gt;Use &lt;code class=&quot;highlighter-rouge&quot;&gt;Session.run()&lt;/code&gt; on tensorflow variables.&lt;/li&gt;
  &lt;li&gt;Use off-the-shelf tensorboard to compute the variable summaries.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.print()&lt;/code&gt;: some control can be execrised through the paramaters of this method such as &lt;code class=&quot;highlighter-rouge&quot;&gt;first_n&lt;/code&gt;, number of lines to print before breaking, message and summarizer. Behaves like an identity operation with the side-effect of printing.&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.Assert()&lt;/code&gt; is also on the same lines, print when a condition is met. In the end, this need to be evaluated with &lt;code class=&quot;highlighter-rouge&quot;&gt;session.run()&lt;/code&gt;. One trick is to add all asserts to a collection with: &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.add_to_collection('Asserts', tf.Assert(...))&lt;/code&gt; and in the end: &lt;code class=&quot;highlighter-rouge&quot;&gt;assert_op = tf.group(*tf.get_collections('Asserts'))&lt;/code&gt; and &lt;code class=&quot;highlighter-rouge&quot;&gt;session.run(assert_op)&lt;/code&gt;.&lt;/li&gt;
  &lt;li&gt;How about python debuggers: &lt;code class=&quot;highlighter-rouge&quot;&gt;pdb&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;ipdb&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;pudb&lt;/code&gt;.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Some usefule Tensorflow APIs:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.get_default_graph()&lt;/code&gt; Get the current (default) graph&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;G.get_operations, G.get_operations_by_name(name),G.get_tensor_by_name(name), tf.get_collection(tf.GraphKeys.~~)&lt;/code&gt; gets all operations (sometimes by name) or tensors or collection&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tf.trainable_variables()&lt;/code&gt; list all the trainable variables and likewise &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.global_variables()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;[t = tf.verify_tensor_all_finite(t, msg)][tf-doc-controlflow]&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.add_check_numerics_ops()&lt;/code&gt;&lt;/li&gt;
  &lt;li&gt;Naming all the tensors and ops can help in pointing where the problem is when looking at stacktrace.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;strong&gt;Advanced&lt;/strong&gt;&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;It is possible to interpose any python code in computation graph by wrapping a tensorflow operation around the function with: &lt;code class=&quot;highlighter-rouge&quot;&gt;tf.py_func()&lt;/code&gt;. 
Consider the following example (https://wookayin.github.io/TensorflowKR-2016-talk-debugging/#57):&lt;/li&gt;
  &lt;li&gt;https://wookayin.github.io/TensorflowKR-2016-talk-debugging/#75&lt;/li&gt;
&lt;/ul&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;multilayer_perceptron&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fully_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fc1'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fully_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;256&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;nn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;relu&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'fc2'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;layers&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fully_connected&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;mi&quot;&gt;10&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;activation_fn&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;bp&quot;&gt;None&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;scope&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;_debug_print_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc2_val&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'FC1 : {}, FC2 : {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc2_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;shape&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;'min, max of FC2 = {}, {}'&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;format&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc2_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;min&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(),&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc2_val&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;max&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;())&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;bp&quot;&gt;False&lt;/span&gt;
    &lt;span class=&quot;n&quot;&gt;debug_print_op&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;py_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;_debug_print_func&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;fc1&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;fc2&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;],&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;bool&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;])&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;with&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;control_dependencies&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug_print_op&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;):&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;identity&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;'out'&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;return&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;out&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;ul&gt;
  &lt;li&gt;The &lt;a href=&quot;https://github.com/ericjang/tdb&quot;&gt;tdb&lt;/a&gt; library&lt;/li&gt;
  &lt;li&gt;&lt;code class=&quot;highlighter-rouge&quot;&gt;tfdbg&lt;/code&gt;: the official debugger from TF. [Dec. 2016]&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;tfdbgtfdbg&quot;&gt;&lt;a href=&quot;https://www.tensorflow.org/versions/master/how_tos/debugger/&quot; title=&quot;TensorFlow Debugger (tfdbg) Command-Line-Interface Tutorial: MNIST&quot;&gt;tfdbg&lt;/a&gt;&lt;/h2&gt;
&lt;p&gt;The debugger from tensorflow can be handy.&lt;/p&gt;

&lt;p&gt;It can be used to see the values of the nodes in the computation graph at runtime.
It can also be used to quicly figure out where a bug is: such as a variable not initialized, inf or nan values or shape mismatch in matrix multiplication with pre-fedined filters: &lt;code class=&quot;highlighter-rouge&quot;&gt;uninitialized_variable&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;has_inf_or_nan&lt;/code&gt;, &lt;code class=&quot;highlighter-rouge&quot;&gt;shape_filter&lt;/code&gt; respectively.&lt;/p&gt;

&lt;p&gt;It is possible to wrap the tensorflow session to debug along with filters to quicly figure where things are going wrong.
It works a gdb session and the interactive session supports: mouse clicks,  which can be turned off with &lt;code class=&quot;highlighter-rouge&quot;&gt;mouse off&lt;/code&gt;, history upon up-arrow and tab completions.&lt;/p&gt;

&lt;p&gt;Include the following linesr to enable interactive debugging (the tool is more helpful if all the ops and variables are properly named)&lt;/p&gt;

&lt;div class=&quot;language-python highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;kn&quot;&gt;from&lt;/span&gt; &lt;span class=&quot;nn&quot;&gt;tensorflow.python&lt;/span&gt; &lt;span class=&quot;kn&quot;&gt;import&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;as&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf_debug&lt;/span&gt;

&lt;span class=&quot;k&quot;&gt;if&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;FLAGS&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;debug&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;:&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf_debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;LocalCLIDebugWrapperSession&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
        &lt;span class=&quot;n&quot;&gt;session&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;add_tensor_filter&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s&quot;&gt;&quot;has_inf_or_nan&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;n&quot;&gt;tf_debug&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;.&lt;/span&gt;&lt;span class=&quot;n&quot;&gt;has_inf_or_nan&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;At the time of writing this, the module is experimental and buggy. 
For example, it failed on BasicLSTMCell with the error: &lt;code class=&quot;highlighter-rouge&quot;&gt;AttributeError: 'LSTMStateTuple' object has no attribute 'name'&lt;/code&gt;.
I don’t get it completely, but it looks like it is trying to find name attribute of LSTMCell which is not defined by default.&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Oct 2016 16:42:25 +0530</pubDate>
        <link>http://localhost:4000/jekyll/update/2016/10/25/study.html</link>
        <guid isPermaLink="true">http://localhost:4000/jekyll/update/2016/10/25/study.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
