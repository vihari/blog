<?xml version="1.0" encoding="UTF-8"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Your awesome title</title>
    <description>Write an awesome description for your new site here. You can edit this line in _config.yml. It will appear in your document head meta (for Google search results) and in your feed.xml site description.
</description>
    <link>http://yourdomain.com/</link>
    <atom:link href="http://yourdomain.com/feed.xml" rel="self" type="application/rss+xml"/>
    <pubDate>Fri, 11 Nov 2016 15:59:41 +0530</pubDate>
    <lastBuildDate>Fri, 11 Nov 2016 15:59:41 +0530</lastBuildDate>
    <generator>Jekyll v3.0.1</generator>
    
      <item>
        <title>Welcome to Jekyll!</title>
        <description>&lt;p&gt;You’ll find this post in your &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory. Go ahead and edit it and re-build the site to see your changes. You can rebuild the site in many different ways, but the most common way is to run &lt;code class=&quot;highlighter-rouge&quot;&gt;jekyll serve&lt;/code&gt;, which launches a web server and auto-regenerates your site when a file is updated.&lt;/p&gt;

&lt;p&gt;To add new posts, simply add a file in the &lt;code class=&quot;highlighter-rouge&quot;&gt;_posts&lt;/code&gt; directory that follows the convention &lt;code class=&quot;highlighter-rouge&quot;&gt;YYYY-MM-DD-name-of-post.ext&lt;/code&gt; and includes the necessary front matter. Take a look at the source for this post to get an idea about how it works.&lt;/p&gt;

&lt;p&gt;Jekyll also offers powerful support for code snippets:&lt;/p&gt;

&lt;figure class=&quot;highlight&quot;&gt;&lt;pre&gt;&lt;code class=&quot;language-ruby&quot; data-lang=&quot;ruby&quot;&gt;&lt;span class=&quot;k&quot;&gt;def&lt;/span&gt; &lt;span class=&quot;nf&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
  &lt;span class=&quot;nb&quot;&gt;puts&lt;/span&gt; &lt;span class=&quot;s2&quot;&gt;&quot;Hi, &lt;/span&gt;&lt;span class=&quot;si&quot;&gt;#{&lt;/span&gt;&lt;span class=&quot;nb&quot;&gt;name&lt;/span&gt;&lt;span class=&quot;si&quot;&gt;}&lt;/span&gt;&lt;span class=&quot;s2&quot;&gt;&quot;&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;end&lt;/span&gt;
&lt;span class=&quot;n&quot;&gt;print_hi&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;s1&quot;&gt;&#39;Tom&#39;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt;
&lt;span class=&quot;c1&quot;&gt;#=&amp;gt; prints &#39;Hi, Tom&#39; to STDOUT.&lt;/span&gt;&lt;/code&gt;&lt;/pre&gt;&lt;/figure&gt;

&lt;p&gt;Check out the &lt;a href=&quot;http://jekyllrb.com/docs/home&quot;&gt;Jekyll docs&lt;/a&gt; for more info on how to get the most out of Jekyll. File all bugs/feature requests at &lt;a href=&quot;https://github.com/jekyll/jekyll&quot;&gt;Jekyll’s GitHub repo&lt;/a&gt;. If you have questions, you can ask them on &lt;a href=&quot;https://talk.jekyllrb.com/&quot;&gt;Jekyll Talk&lt;/a&gt;.&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Oct 2016 16:42:25 +0530</pubDate>
        <link>http://yourdomain.com/jekyll/update/2016/10/25/welcome-to-jekyll.html</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/2016/10/25/welcome-to-jekyll.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
      <item>
        <title>Transfer learning in the context of deep learning</title>
        <description>&lt;h1 id=&quot;transfer-learning-in-the-context-of-deep-learning&quot;&gt;Transfer learning in the context of deep learning&lt;/h1&gt;

&lt;h2 id=&quot;cnn-features-off-the-shelf-an-astounding-baseline-for-recognitionrit13&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1403.6382v3.pdf&quot;&gt;CNN Features off-the-shelf: an Astounding Baseline for Recognition&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This paper explores the potential of features generated by Convolutional Neural Network by using the features learned by Overfeat model for object classification task in ILSVRC13.  &lt;br /&gt;
The features are appllied for various tasks such as &lt;em&gt;object classification, fine-grained recognition, scene recognition, attribute detection (for example attributes of pedestrians from a cam feed that is if they are wearing a hat, shoe, trouser, jeans etc.) and scene retrieval&lt;/em&gt;.&lt;/p&gt;

&lt;h3 id=&quot;object-recognition&quot;&gt;Object recognition&lt;/h3&gt;

&lt;p&gt;Is the task of assigning labels (possibly more than one) to a given image. &lt;br /&gt;
Since the features used are learned over a similar task, better results are expected. &lt;br /&gt;
They tried on PASCAL VOM and MIT indoor dataset both of which are considered harder than ImageNet.  &lt;br /&gt;
The mean Average Precision(mAP) over all the classes on PASCAL VOM is 77.2, which is 7 point more than second best. &lt;br /&gt;
Thing to note is that, Oquab et.al. fixes all the layers trained on ImageNet and then it adds and optimized two fully-connected layers on VOC dataset with &lt;strong&gt;77.7&lt;/strong&gt; mAP.&lt;/p&gt;

&lt;h3 id=&quot;object-detection&quot;&gt;Object Detection&lt;/h3&gt;

&lt;p&gt;This is about recognising and also putting a box around the object.&lt;br /&gt;
With off-the-shelf features: &lt;strong&gt;46.2&lt;/strong&gt; mAP (Girshick et.al.) and when fine-tuned on PASCAL VOC it is &lt;strong&gt;53.1&lt;/strong&gt;.&lt;/p&gt;

&lt;p&gt;The graph below summarizes the contribution of this paper.&lt;/p&gt;

&lt;p&gt;&lt;img src=&quot;/assets/images/feature-transfer13.png&quot; alt=&quot;Comparison of CNN features with other baselines&quot; /&gt;&lt;/p&gt;

&lt;h2 id=&quot;decafdecaf&quot;&gt;&lt;a href=&quot;https://arxiv.org/pdf/1310.1531v1.pdf&quot;&gt;DeCAF&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This work is quite similar to the above one. &lt;br /&gt;
The features are extracted from the &lt;a href=&quot;https://papers.nips.cc/paper/4824-imagenet-classification-with-deep-convolutional-neural-networks.pdf&quot; title=&quot;ImageNet classification with deep convolutional neural networks. &quot;&gt;AlexNet&lt;/a&gt; which contains 5 convolutional layers (not inluding pooling) followed b7y two fully connnected layers.  &lt;br /&gt;
Features are extracted from the output of 5th, 6th and 7th layer; notice that 7th is last but one layer and 5th has only convolutional layers. &lt;br /&gt;
AlexNet has dropout in layers 6 and 7. &lt;br /&gt;
Experiments were carried on the tasks of Object detection, fine-grained recognition, domain adaptation and scene recognition.&lt;/p&gt;

&lt;h3 id=&quot;object-recognition-1&quot;&gt;Object Recognition&lt;/h3&gt;

&lt;p&gt;It is found in their experiments that logistic Regression or linear SVM on these features already beat methods that involve task-specific features with SVM using multiple kernels by around 2-3% on mean accuracy.&lt;br /&gt;
Things to note, the mean accuracy was around 33% when the training set contains only one example per class. &lt;br /&gt;
&lt;em&gt;The performance of the system on 7th level features is slightly less (by 2%) when compared to 6th level feature.&lt;/em&gt;&lt;br /&gt;
Dropout on layers: 6/7 improves perf. by 0-2%.&lt;br /&gt;
This method beats two-layer convolution network trained on only the task-specific training data by 20%.&lt;/p&gt;

&lt;h2 id=&quot;frustratingly-easy-domain-adaptationeasyadapt&quot;&gt;&lt;a href=&quot;http://www.umiacs.umd.edu/~hal/docs/daume07easyadapt.pdf&quot; title=&quot;Frustratingly Easy Domain Adaptation&quot;&gt;Frustratingly Easy Domain Adaptation&lt;/a&gt;&lt;/h2&gt;

&lt;p&gt;This is a 2007 paper that propses a simple pre-processing trick for easy adaptation. &lt;br /&gt;
The DeCAF paper reports best results in the task of domain adaptation from Amazon images -&amp;gt; webcam images using the features from AlexNet and this method. &lt;br /&gt;
That interested me to study this further.&lt;br /&gt;
Let me start with the simple code.&lt;/p&gt;

&lt;div class=&quot;highlighter-rouge&quot;&gt;&lt;pre class=&quot;highlight&quot;&gt;&lt;code&gt;&lt;span class=&quot;c1&quot;&gt;#source: http://hal3.name/easyadapt.pl.gz&lt;/span&gt;
&lt;span class=&quot;k&quot;&gt;for&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;=&lt;/span&gt;&lt;span class=&quot;mi&quot;&gt;0&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;&amp;lt;&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;@ARGV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;o&quot;&gt;++&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
    &lt;span class=&quot;nb&quot;&gt;open&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;F&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$ARGV&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;[&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$i&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;];&lt;/span&gt;
    &lt;span class=&quot;k&quot;&gt;while&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;sr&quot;&gt;&amp;lt;F&amp;gt;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;chomp&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;p&quot;&gt;(&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;$y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;,&lt;/span&gt;&lt;span class=&quot;nv&quot;&gt;@x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;)&lt;/span&gt; &lt;span class=&quot;o&quot;&gt;=&lt;/span&gt; &lt;span class=&quot;nb&quot;&gt;split&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;$y&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;nb&quot;&gt;map&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;{&lt;/span&gt; &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot; *$_ $i$_&quot;&lt;/span&gt; &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt; &lt;span class=&quot;nv&quot;&gt;@x&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
        &lt;span class=&quot;k&quot;&gt;print&lt;/span&gt; &lt;span class=&quot;s&quot;&gt;&quot;\n&quot;&lt;/span&gt;&lt;span class=&quot;p&quot;&gt;;&lt;/span&gt;
    &lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;span class=&quot;p&quot;&gt;}&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;

&lt;p&gt;Consider the case where there is a loads of data in one domain but is different from the domain that we want to use it in. &lt;br /&gt;
For example, we might want a PoS tagger trained on NewsWire to work on hardware blogs. &lt;br /&gt;
That is, we have data from source domain and some amount of data from target domain; there are sevearal standard ways to adapt in such a case. &lt;br /&gt;
According to the author, these approaches are surprisingly hard to beat.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;SRCOnly: A single model is trained on only data from source data alone ignoring target data.&lt;/li&gt;
  &lt;li&gt;TGTOnly: Model trained only on target data alone&lt;/li&gt;
  &lt;li&gt;ALL: A model is trained on both the source and target domain. &lt;br /&gt;
Can wash away the effects of target data since it is small relatively.&lt;br /&gt;
For this reason, the source data eaxamples are weighed down by that factor to bring balance. &lt;br /&gt;
WEIGHT is our next baseline which chooses the scaling factor of source examples by CV.&lt;/li&gt;
  &lt;li&gt;PRED: Use the output of model trained on SRC data as feature for training on target domain.&lt;/li&gt;
  &lt;li&gt;LinINT: Linearly interpolate the predictions of src and target models while the interpolation params are from dev. data.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Two models are found to have beat these:&lt;/p&gt;

</description>
        <pubDate>Tue, 25 Oct 2016 16:42:25 +0530</pubDate>
        <link>http://yourdomain.com/deep-learning/transfer-learning/2016/10/25/transfer-learning.html</link>
        <guid isPermaLink="true">http://yourdomain.com/deep-learning/transfer-learning/2016/10/25/transfer-learning.html</guid>
        
        
        <category>deep-learning</category>
        
        <category>transfer-learning</category>
        
      </item>
    
      <item>
        <title>Notes from my independent study of deep learning</title>
        <description>&lt;h2 id=&quot;practical-aspects-of-training&quot;&gt;Practical aspects of Training&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;&lt;a href=&quot;https://arxiv.org/pdf/1206.5533v2.pdf&quot; title=&quot;Practical Recommendations for Gradient-Based Training of Deep Architectures&quot;&gt;Recommendations by Y. Bengio&lt;/a&gt;  &lt;br /&gt;
  The recommendations are not definitive and should be challenged.&lt;br /&gt;
  They can work as a good starting point.  &lt;br /&gt;
  It has been shown that use of computer clusters for hyper-parameter selection can have an important effect on results. &lt;br /&gt;
  The value of sopme hyper-parameters can be selected based on its performance on training data, but most cannot. For any hyper-parameter that affects the capacity of the learner, it makes more sense to use an out-of-sample data to select the params, hence the validation set.  &lt;br /&gt;
  	&amp;gt; different researchers and research groups do not always agree on the practice of training neural networks&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Initial Learning rate (&lt;script type=&quot;math/tex&quot;&gt;\epsilon_0&lt;/script&gt;)&lt;/strong&gt; is the single most important hyp. param. according to him. &lt;code class=&quot;highlighter-rouge&quot;&gt;If there is only time to optimize one hyper-parameter and one uses stochastic gradient descent, then this is the hyper-parameter that is worth tuning.&lt;/code&gt; Typical values for neural networks is: 1E-6 to 1 that is if the input is properly normalized to lie between 0 and 1. A default value of 0.01 typically works for multi-layer neural network.&lt;/li&gt;
      &lt;li&gt;
        &lt;p&gt;&lt;strong&gt;Learning rate schedule&lt;/strong&gt;: The general strategy is to keep the learning rate constant until some iterations as shown in the equation below. In many cases choosing other than the default value for &lt;script type=&quot;math/tex&quot;&gt;\tau\rightarrow \infty&lt;/script&gt; has very little effect. &lt;br /&gt;
There are suggestions to decrease the learning rate less steeply than linear after &lt;script type=&quot;math/tex&quot;&gt;\tau&lt;/script&gt; time steps that is &lt;script type=&quot;math/tex&quot;&gt;% &lt;![CDATA[
O(1/t^\alpha), \alpha&lt;1 %]]&gt;&lt;/script&gt; depending on the convex behaviour of the function being optimized.&lt;br /&gt;
Adaptive strategies for learning rate exist.&lt;/p&gt;

        &lt;script type=&quot;math/tex; mode=display&quot;&gt;\epsilon_t=\frac{\epsilon_0 \tau}{max(t,\tau)}&lt;/script&gt;
      &lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Mini-batch size (B)&lt;/strong&gt;: In theory, this hyp. param. should only affect the training time and not performance. Typically, B is a value between 1 and a few hundred. A value of 32 is good because it can take implementational advantages that matrix-matrix product are more efficient than matrix-vector and is not too high to require a long training time. &lt;br /&gt;
B can be optimized independently of others. Once a value is choosen, it can be fixed and is better to re-optimize in the end since it weakly interacts with other hyp. params.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Number of Training iterations (T)&lt;/strong&gt;: To be choosen by early-stopping criteria. During the analysis stage where we try and compare different models, stopping early can have evening out effect. That is we cannot make-out between an over-fitting or under-fitting model. For this reason, it is best to turn it off during analysis. Another param called &lt;em&gt;patience&lt;/em&gt; is generally defined that tells how long the model should wait after it observed the minimum validation error, the param is defined in terms of minimum number of examples to be seen before stopping.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Momenum (&lt;script type=&quot;math/tex&quot;&gt;\beta&lt;/script&gt;)&lt;/strong&gt;: &lt;script type=&quot;math/tex&quot;&gt;\hat{g}\leftarrow (1-\beta)\hat{g}+\beta g&lt;/script&gt; where &lt;script type=&quot;math/tex&quot;&gt;\hat{g}&lt;/script&gt; is the smoothed gradient. The default value of &lt;script type=&quot;math/tex&quot;&gt;\beta=1&lt;/script&gt; works for most cases, but is found to help in some cases of unsupervised learning. “The idea is that it removes some of the noise and oscillations that gradient descent has, in particular in the directions of high curvature of the loss function”&lt;br /&gt;
 In some rare cases, layer-specific hyper paramater optimization is employed. This makes sense when the number of hidden units vary large between layers. Generally employed in layer-wise unsupervised pre-training.&lt;/li&gt;
    &lt;/ul&gt;

    &lt;p&gt;The parameters discussed above are related to SGD (Stochastic Gradient Descent), what follows are the params related to the neural network.&lt;/p&gt;

    &lt;ul&gt;
      &lt;li&gt;&lt;strong&gt;Number of hidden units &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt;&lt;/strong&gt; Because of early stopping and possiblty other regularizers such as weight decay, it is mostly important to choose &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt; large enough. In general, it is observed that seeting all the layers to equal number of units works better or the same as when setting the hidden unit widths in decreasing or increasing order (pyramidal or reverse pyramidal), but this could be data-dependent (&lt;a href=&quot;http://deeplearning.cs.cmu.edu/pdfs/1111/jmlr10_larochelle.pdf&quot; title=&quot;Exploring Strategies for Training Deep Neural Networks&quot;&gt;Larochelle et.al. 2014&lt;/a&gt;). &lt;br /&gt;
 An over-complete (larger than the input vector) is better than an under-complete one.&lt;br /&gt;
 A more validated observation is that optimal &lt;script type=&quot;math/tex&quot;&gt;n_h&lt;/script&gt; value when training with unsupervised pre-training in a supervised neural network. Typically from 100 to 1000. This could be because unsupervised pre-training could hold lot more information that is not relevant to the task and hence require large hidden layers to make sure relevant information is captured.&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Weight decay:&lt;/strong&gt; This article makes a very inetersting note about thsi parameter. As we know, weight decay is used to avoid over-fitting by limiting capacity of the learner. L2 or L1 regularization correspond to the penalties: &lt;script type=&quot;math/tex&quot;&gt;\lambda \sum_i {\theta_i}^2&lt;/script&gt; and &lt;script type=&quot;math/tex&quot;&gt;\lambda \sum_i{\theta_i}&lt;/script&gt;, both the terms can be included. In the case of batch-wise handling of data, the param &lt;script type=&quot;math/tex&quot;&gt;\lambda&lt;/script&gt; that we optimize is actually &lt;script type=&quot;math/tex&quot;&gt;\frac{\lambda&#39;*B}{T}&lt;/script&gt; where B is batch size and T is the size of training data.&lt;br /&gt;
L2 regularization corresponds to a guassian prior (over weights) &lt;script type=&quot;math/tex&quot;&gt;\propto exp^{\frac{-1 \theta^2}{2 \sigma^2}}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;\sigma^2=1/2\lambda&lt;/script&gt;.&lt;br /&gt;
&lt;em&gt;Note that there is a connection between L2 regularization and early stopping with one basically playing the same role as other&lt;/em&gt;. L1 regularization is different and sometimes act as feature selectors by making sure the parameters that are not really very useful go to 0. L1 corresponds to laplace density prior &lt;script type=&quot;math/tex&quot;&gt;\propto e^{-\frac{|\theta|}{s}}&lt;/script&gt; with &lt;script type=&quot;math/tex&quot;&gt;s=\frac{1}{\lambda}&lt;/script&gt;  &lt;br /&gt;
 It is sufficient to regularize just the output weights in order to constrain the capacity. (we use the input weights and output weights to denote weights corresponding to the first and last layer. The input weights is also often referred to as &lt;em&gt;filters&lt;/em&gt; because of analogies with signal processing techniques)  &lt;br /&gt;
 Using an L1 regularizer helps to make the input filters cleaner and easier to interpret. &lt;br /&gt;
 We may draw that L1 cleans the input weights and L2 the output weights. &lt;br /&gt;
 When we introduce both the penalties into our optimization, then it is required to tune the coeffs for L1 and L2 independently. In particluar, input and output weights are treated different.  &lt;br /&gt;
 &lt;em&gt;In the limit case of the number of hidden layers going to infinity, L2 regularization corresponds to SVMs and L1 to Boosting (&lt;a href=&quot;https://papers.nips.cc/paper/2800-convex-neural-networks.pdf&quot; title=&quot;Convex Neural Networks&quot;&gt;Bengio et.al. 2006&lt;/a&gt;)&lt;/em&gt;  &lt;br /&gt;
 One of the reason why we cannot rely only on early stopping criteria and treat input and output weights differently from hidden units is because they may be sparse. For example, some input features could be 0 more frequently and others non-zero more frequently. A similar situation may arise when target variable is sparse i.e. trying to predict a rare event. In both cases, the effective number of meaningful update (active feature or rare event) seen by these params is less than the actual number of updates. The parameters (weights outgoing from the corresponding input) of such sparse examples should be more regularized, that is to scale the reg. coeff. of these params by one over effective number of updated seen by the parameter. This presents an alteranaate way to deal with imbalanced data or anamolies(?).&lt;/li&gt;
      &lt;li&gt;There are several approaches that aim to minimize the sparsity of hidden layers (note that sparsity is very different from L1 norm)&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Non-linear activation functions&lt;/strong&gt;: Popular choices are: sigmoid &lt;script type=&quot;math/tex&quot;&gt;\frac{1}{1+e^{-a}}&lt;/script&gt;, the hyperbolic tangent &lt;script type=&quot;math/tex&quot;&gt;\frac{e^a-e^{-a}}{e^a+e^{-a}}&lt;/script&gt;, rectifier max; max(0,a) and hard tanh.  &lt;br /&gt;
Sigmoid was shown to yield serious optimization difficulties when used as the top hidden layer of a deep supervised network without unsupervised pre-training. &lt;br /&gt;
&lt;code class=&quot;highlighter-rouge&quot;&gt;For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &amp;lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error. For output (or reconstruction) units, hard neuron non-linearities like the rectifier do not make sense because when the unit is saturated (e.g. a &amp;lt; 0 for the rectifier) and associated with a loss, no gradient is propagated inside the network, i.e., there is no chance to correct the error&lt;/code&gt; &lt;br /&gt;
The general trick is to use linear output and squared error for Gaussian output model, cross-entropy and sigmoid output for binomial output model and log output[target class] with softmax outputs to correspond to multinomial output variables (that is take softmax for over all the neuron outpus and score by considering only target label required).&lt;/li&gt;
      &lt;li&gt;&lt;strong&gt;Weights initiazation and scaling coefficient&lt;/strong&gt;: The weights should be initialized randomly inorder to break symmetry and bias can be initialized to 0. &lt;br /&gt;
If not all the neurons initially will produce the same output and hence receive the same gradient, wasting the capacity.&lt;br /&gt;
The scaling factor controls how small or big the initial weights are. Units with large input (fan-in of the unit) should have smaller weights (I have first-hand experience with problems that arise when this is not done with one of my NN assignments. The initialization that worked smaller number of hidden units just over-shooted due to explosive gradients. That is because the output diverged to a large value when I did this)  &lt;br /&gt;
The recommendation made is either to sample &lt;em&gt;Uniform(-r,r)&lt;/em&gt;. &lt;br /&gt;
Where r is &lt;script type=&quot;math/tex&quot;&gt;\sqrt{\frac{6}{fan-in+fan-out}}&lt;/script&gt; for hyperbolic tangent units and  &lt;script type=&quot;math/tex&quot;&gt;4*\sqrt{\frac{6}{fan-in+fan-out}}&lt;/script&gt;. fan-in and fan-out are the input and output dimension of a hidden layer.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;General advice on finding the best model.  &lt;br /&gt;
  Numerical hyper-parameters need to be grid-searched in order to find one. &lt;br /&gt;
  It is not sufficient to conclude the best value based on comparison with less than 5 other values.   &lt;br /&gt;
  Scale of values considered is often an important decision to make, this is the starting interval in which the values will be looked up. &lt;br /&gt;
  It makes more sense to sample values uniformly in the log space of such interval than to blindly evaluate at every value because the perf. at say 0.01 and 0.011 is likley to remain the same.  &lt;br /&gt;
  Strategies for hyper-param selection: Coordinate descent and Multi-resolution search.&lt;br /&gt;
  Coordinate descent: Make changes to the each hyper-param one at a time, find the best value for the param and move on to the next one. &lt;br /&gt;
  Mult-resolution search: There is no point in fine-tuning or high-resolution search over large intervals. Do a low-resolution search over several settings and then high-res search over best configurations.&lt;/p&gt;

&lt;h2 id=&quot;limitations-of-deep-learning&quot;&gt;Limitations of Deep Learning&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;a href=&quot;https://arxiv.org/pdf/1412.1897.pdf&quot;&gt;DNNs are easy to fool&lt;/a&gt; Surprisingly, DNNs can be easily fooled by adding adverse perturbations to an image. For example, adding noise that is imperceptable to humans to an image that looks like &lt;em&gt;panda&lt;/em&gt; and recognized as one with confidence of ~56% will lead to an image that is wrongly labeled but with very high confidence. This paper details about very interesting case-study of where the state-of-art AlexNet utterly fails. For code and images &lt;a href=&quot;http://www.evolvingai.org/fooling&quot; title=&quot;Deep neural networks are easily fooled: High confidence predictions for unrecognizable images &quot;&gt;see&lt;/a&gt;. There has also been some effort at explaining this phenomena. In the paper: &lt;a href=&quot;https://arxiv.org/pdf/1412.6572v3.pdf&quot; title=&quot;Explaining and Harnessing Adversial Examples&quot;&gt;Explaining and Harnessing Adversial Examples&lt;/a&gt; by Ian Goodfellow et. al., they make a case that such a thing happens majorly because the DNNs are linear in nature.&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;wisdom-from-random-sources&quot;&gt;Wisdom from random sources&lt;/h2&gt;

&lt;h3 id=&quot;hinton-at-stanfordgeoffrey-stanford&quot;&gt;&lt;a href=&quot;https://www.youtube.com/watch?v=VIRCybGgHts&quot; title=&quot;Can the Brain do back-propagation?&quot;&gt;Hinton at Stanford&lt;/a&gt;&lt;/h3&gt;

&lt;ul&gt;
  &lt;li&gt;Big data is good (something that frequentist statisticians suggest)
    &lt;ul&gt;
      &lt;li&gt;For any given size of model, its better to have more data&lt;/li&gt;
      &lt;li&gt;But it’s a bad idea to try to make the data look big by making the model small&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Big models are good (Something that statisticians do not believe but true)
    &lt;ul&gt;
      &lt;li&gt;For any given size of data, the bigger the model, the better it generalized, provided you regularize it well.&lt;/li&gt;
      &lt;li&gt;This is obviously true id your model is an ensemble of smaller models. Adding extra models to the ensemble alwqays helps.&lt;/li&gt;
      &lt;li&gt;It’s a &lt;strong&gt;good idea&lt;/strong&gt; to try to make the data look small by using a big model.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Dropout enables the all the models in an ensemble to share knowledge. If there is only one layer in the network with with H, then the model with dropout is an ensemble of 2^H models and softmax over the output layer is geometric mean of all the models in ensemble.&lt;/li&gt;
  &lt;li&gt;Dropout can be seen as bernoulli noise, we do not change the expected value because a neuron either emits zero or twice the value. It is noted that any other kind of noise can work just as well. Gaussian noise and Possion noise are tested to give same performance if not better. In these cases a multiplicative noise with standard deviation equal to the activity. The point is that neurons do not share real values but spikes and that is a lot better than trhe actuaol values.&lt;/li&gt;
  &lt;li&gt;In this lecture, Hinton goes on lengths elaborating why brains cannot do exact back-propagation and explains possible other ways in which it could be learning the weights. He argues that neurons in a feed-back loop can do away with the need to back-propagate by considering the difference between the inout at this instance and previous one (plasticity of brain, Spike-time dependent plasticity)&lt;/li&gt;
&lt;/ul&gt;

</description>
        <pubDate>Tue, 25 Oct 2016 16:42:25 +0530</pubDate>
        <link>http://yourdomain.com/jekyll/update/2016/10/25/study.html</link>
        <guid isPermaLink="true">http://yourdomain.com/jekyll/update/2016/10/25/study.html</guid>
        
        
        <category>jekyll</category>
        
        <category>update</category>
        
      </item>
    
  </channel>
</rss>
