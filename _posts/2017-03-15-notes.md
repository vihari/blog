---
layout: default
title:  Notes from Geoffrey Hinton's Neural Networks course on Coursera
categories: deep-learning notes coursera
---

# My notes from Geoffrey Hinton's Neural Networks course on Coursera

* This will become a table of contents (this text will be scraped).
{:toc}

# Week 2 (An overview of the main types of neural network architecture) #

## Perceptron learning ##

In a perceptron learning case, the updates to the weight vector are as follows:
  * If the current weight vector correctly classifies a vector **x**, then it is passed on
  * when misclassified then update the weight with $$\eta * (t-y)*x$$ where t and y are expected and found output, $$\eta$$ is the learning rate. 
  
Few points to note: 
  * Because, we look at each example at a time, the update can throw the learning procedure off and hence may lead to more number of classification errors. 
    This can happen irrespective of if the problem is linearly separable or not.
	A smoother transition can be obtained by a smaller learning rate.
  * The similarity of the weight vector with generously feasible weight vector should continuosly increase and infact it can be proved that perceptron algorithm works because it gets closer to the desired vector.
  
One way to make sense of the update equation is to analytically see that the update is gradient of distance between current weight vector and feasible vectors. 
(This is what makes the difference between Perceptron learning and general delta-rule, although they may both lead to same update equation in the case of single linear neuron)
This can also be seen geometrically, see the images below:

Update in positive case:  
![Update in positive case](/assets/images/nn-notes/positive-perceptron.png)

Update in negative case:  
![Update in negative case](/assets/images/nn-notes/negative-perceptron.png)  

# Week 3 (The backpropagation learning proccedure) #

![Backprop slide](/assets/images/nn-notes/back-prop.png)

The image above captures everything you need to know about back-prop. Observe that there are two things which are almost separate. One is the recurrence relation of error with y for each layer (the first two equations). Other is weight update calculated by the outer product of error differential with respect to input of next layer and output of this layer.

Stacking several linear neuron layers does not add to capacity. In the end, the entire stack can be replaced by a single linear neuron.

> A feed forward neural network is defined as a network with no cycles and with no other added constraints. For example, an input layer can have connections to any layer of the subsequent layers as long as they do not form a loop. It is not necessary for every neuron to connect to only the next layer. To enforce the constraint of no cycle, every neuron should send output to layer above and receive input from layer below.

## Multi-layer learning ##

In perceptron learning procedure, the weights got closer to true weights with every iteration, but in NN we try to get the outputs closer to expected ones.

It's not hard to see that perceptrons are specialization of learning through back-propagation. 
The update for weights in that case is just cross product of error in the last layer and output of the input layer, which means adding proportion of input to the weights every time.
The perceptron algorithm cannot directly be extended to neural networks. 
In perceptron space, the average of two good solutions is again a solution (convex).  
The delta rule for weight update with linear activation is $$\sum_{n}{\eta_i * x_n * (t_n-y_n)}$$
  
  * By making the error rate small enough we can get as close as we want to the best answer.
  * **How quickly do weights converge to their correct values?** 
    It can be very slow if two input dimensions are highly correlated. 
	An analogy to weight learning is explained with the example: if you were not told about the prices of each of the commodities that you buy but only the final price, then after some purchases you can figure out the price. 
	If you always buy same quantity of two things then it will take a long time to figure their individual prices.
  
Cases of Over-fitting:
  
  * Target values may be unreliable (bayesian error). This is usually a minor worry.
  * Sampling error: There will be accidental regularities just because of particular training cases that were chosen. 
    For example, in the hand-writing case, we collected data from a person who write 'A' very differently. 
	If the model is very flexible it can model the sampling error really well which can be disastrous.

Although, back propagation wins by a huge margin in finding weights, there are other possible ways to find the weights (although none of the following beats backprop in performance or exactness, it's good to take note).
  
  * Change the weights randomly to see how they effect the performance. 
    This is not scalable as there can be a lot of params and for each change a full forward pass has to be made inorder to calculate.
  * Randomly perturb all the weights to find the best set of weight values that seem to do the best. 
	Again requires a large number of trials.
  * Randomly change the activations instead of weights and then figure out how the weights should change. 
	Better because lesser neurons than weights but worse than backprop (wins by factor of number of neurons).

## Reading assignment (Learning internal representations by Error Propagation) [link][1980backprop]##

Neural networks are feature estimators on steroids. 
They work by trying and adding more features to the existing features. 
For example, consider XOR problem.  

| input | output |
|-------|--------|
| 00    | 1      |
| 01    | 0      |
| 10    | 0      |
| 11    | 1      |

The input patterns that differ the most are the ones that have the same output and hence it is hard to learn. 
By adding an additional feature to the input, we can hope to a better job at classification.  

| input | output |
|-------|--------|
| 000   | 1      |
| 010   | 0      |
| 100   | 0      |
| 111   | 1      |

The feature added is the disjunction of the first two dimensions and now we can see a pattern on input that maps to the same output. 

Minsky and Pappert (1969) apart from showing the limits of perceptron learning, they also showed that an input if when recoded with an internal representation (hidden layer) then an input-output mapping can be learned which could not have been possible without recoding. 
They also pointed that there is no general algorithm to learn internal representation at that time. 
Note that the genaeralized delta rule does not work with activation functions that are not differentiable.

This chpater of a book discusses various other interesting problems that explain how neural networks work. 

### Parity Problem ###

This problem is discussed in Minky and Pappert and is solved here using generalized delta rule.
This class of problems are very similar to the XOR problems. 
The input patterns that look least similar share an output. 
One such problem is mapping of one for inputs with even/odd number of ones in it.  
The networks learned by NN with backprop look like the image shown.

![parity-solution](/assets/images/nn-notes/parity-solution.png)

As can be seen, the internal encoding contain bits that turn on if any input is on in the input, at least two inputs are on in the input and so on... The weights from hidden to output are varying in signs (positive, negative). In the image shown solid lines are positive and dotted are negative. This allows the network to recognize inputs with odd number of ones.

### Symmetry Problem ###

The problem is to recognize if a string of fixed size is symmetric around the mid-point.

![symmetry-solution](/assets/images/nn-notes/symmetry-solution.png)

The network is beautiful that it barely needs an explanation. 
Observe that the weights for the above and below hidden units are mirror images (lateral inversion). 
The weights for the same hidden unit are alternate positive and negative. 
Also observe that the weights for the same hidden unit are in the ratio of 1:2:4 this helps to make sure that the correct bits are being compared. 
Finally, if you are wondering if a single hidden unit could solve the problem, it is not possible, at least not with the binary threshold activation function used. 
For example, for input: 101100 only the below unit responds and for "001101" the unit above responds.

### Addition Problem ###

Given two 2-bit numbers, the output is a 3-bit number which is the addition of the two. 
The minimal network contains two hidden units that map to three output units. 
Each of hidden units function as carry-on bits; in general, for adding two n bit numbers that map to a n+1 bit output, n hidden units are required.
Learning over network with only two hidden units seldom leads (reliably) to a local minima solution that fails one or more cases. 
The authors attribute this to the latent ordering over the input which is that the output of middle bit depends on the carry on from the previous bits.
The second output bit depends on the first output bit and hence follows learning of first bit which means it does not influence the first bit learning. 
Because of which middle bit gets messed up more often due to loosing of carry on information.
The case of local solution occurs when the network untilises the hidden unit corresponding to higher bits when adding lower-order bits which makes the lower order hidden unit ignorant of carry-on. 
This causes error like "11"+"11"="100" instead of "110"

![minimal-add](/assets/images/nn-notes/minimal-add.png)

The problem can be avoided by using three units instead, but one looses the interpretability of the hidden output.

### Negation ###

Transformation of input such that all the bits are inverted if the sign bit is negative and will remain the same if the sign bit is positive. 
"011"->"11"; "111"->"00"
The minimal solution of the problem should adopt XOR for every bit with the sign bit and the solution acheives that.

### T-C classification ###

The task is to classify the shapes T and C.
This problem is again from Minsky and Pappert, the two shapes only differ in one cell and cannot be distinguished by only considering pairs of cells and requires consideration of triplets hence are of order 3.

The text goes on to describe convolution networks, weight sharing concept, translational invariance (30 year old paper?! So ahead of its time).
Various detectors learned by the model are presented which exploit the compactness of C and protrusion difference between the networks. 

We really have waited for 30 years just so that computers can handle the computation.

## In the case of recurrent nets ##

The chapter further discusses the relevance in the case of recurrent nets. 
Minsky and Pappert (again) showed that for any recurrent network there is an equivalent feed-forward network that has the same behavior in a finite amount of time.
In RNets, supervision is through periodic comparison of the outputs to the desired values.

They have tried the weight learning on 
  
  * Shift Registers: where the input should be shifted by two units in the register after two time units. 
	Let us say the input is a 8 bit vector with few bits on and it is expected that the on bits are moved by two units after two time units. 
	Observe that the intermediate representation is not specified. 
	For this reason, the intial biases are all set to 0 so that the intermediate transitions are not complex.
    The system does learns to move input by one unit every time step and showed a strong affliation with left neighbour although initially connections between any of the output units is allowed.
  * Learning to predict next in the sequence: the task is to predict the next four numbers given first two characters. 
	All the characters are correlated to codes they appear with, so a character deterministically gives the two digit numeric code. 
	The specific test has 5 chars and 3 numbers. 
	one of the two chars in the begin are shown hence input dimension: 5, output dimension: 3 and 30 hidden layers. 
	They could successfully train RNets to do well over this task.

This heavily cited paper was an answer to the Minsky and Pappert critic. 

[1980backprop]: http://www.cnbc.cmu.edu/~plaut/IntroPDP/papers/RumelhartETAL86.backprop.pdf

# Week 4 (Learning to predict the next word) #

Q: *Why do we adopt one-hot encoding given that it is O(N) storage, but not a binary encoding which only takes log N bits?*  
A: Because then subset of input is linearly separable from any other disjoint set over the input.
Another reason is that binary coding mis-represents the input space and puts some input closer than others while in reality we have not apriori information.

----
An interesting application of relational learning is discussed in the lecture which is to infer new or unspecified relations given a family tree. 
For example if the family tree mentions two people as the children with the same parents then they are also *brothers* which is unspecified. 
It is hard to infer such relations through deduction over several rules.
Neural networks to rescue, if we recast the problem as trying to predict person2 given person1 and relationship.
In practice, we predict the distributional encoding of person2 given such encoding for person1 and relationship.  
Learning of distributional encoding is where neural nets come in.
They learn to encode objects so that they represent useful features such as *nationality, generation, branch of the family tree*.

A large scale example: given a large database of facts (such tuples) find the ones that are unlikely.

-----

**Debate on how concept is encoded in the brain (Cognitive Science)**  
**Feature Theory** A concept is encoded as vector of semantic features  
**Structuralist Theory** The meaning of a concept lies in its relationships to other concepts.  
Geff argues that a feature of semantic vector representation can be used to implement a relational graph. 
**The right way to implement relational knowledge in a neural net is still an open problem.**
Typically, a neauron can be involved in many concepts and every concept involved many neurons.

-----

Say we are dealing with logistic units with sequared error measure, in that case if the actual output is 1 in a million and desired output is 1, there is hardly any gradient because $$\frac{\partial E}{\partial z}=y*(1-y)*$$(finite number).
We are depriving the network of the world knowledge that the outputs should sum to one hence the softmax.

$$\frac{\partial y_i}{\partial z_i} = y_i*(1-y_i)$$

This problem can be avoided by using right cost function: cross entropy $$-\sum_{j}{t_j*log y_j}$$

$$\frac{\partial C}{\partial z_i}=\sum_j \frac{\partial C}{\partial y_j}\frac{\partial y_j}{\partial z_i} = y_i-t_i$$

--------

## Reading Assignment (Y. Bengio's Neural Language Model) [link][bengio-language] ##

A neural language model is presented that jointly learns to predict next word in the sequence with distributional representation of words. 
The insight is that the problem of unseen sequence is acute and can only be handled if a sentence that is already looked at can be generalized to exponentially many similar sentences.
"The cat got squashed in the garden on friday" is equivalent to "The dog flattened in the yard on monday"

In this paper, a neural network with one hidden layer and skip layer connection to output from input that formulates a representation for each word such that it predicts the next word.
Softmax is used in the last layer to output probabilities which means summing over the entire vocabulary.
This is the reason why the system works best with tri-gram models so that the probabilities are only summed over candidates proposed by the n-gram model.

Another method that is worth mentioning is Collobert and Weston, 2008 where they try to distinguish if a middle word looks right or random.
By just classifying if a word is right or random they have reformulated the problem as a binary classification one which is much easier to deal with.

[bengio-language]: http://www.jmlr.org/papers/volume3/bengio03a/bengio03a.pdf "A Neural Probabilistic Language Model"

# Week 5 (Why object recognition is difficult.)#
## Reading Assignment - 1 (Gradient based learning applied to document recognition) [link][lecun-docrec] [Yet to be read]##

The performance difference between the train and test sets can be expressed as

$$E_{test} - E_{train} = k(h/P)^\alpha$$

where P is the number of training samples, h is the effective capacity and $$\alpha$$ is a real number between 0.5 and 1; k is a constant.
Note that 'h' is the effective capacity and when a network with a given capacity is trained with a regularizer, its effectove capactity increases smoothly.

## Reading Assignment - 2 (Convolutional Networks for Image, Speech and Time-Series) [link][conv-nets] ##

Convolutional networks show some degree of shift and distortion invariance by local receptive fields.
They enable weight sharing (weight replication), and spatial or temporal sub-sampling.

>With local receptive fields, neurons can extract elementary visual features such as corners, edges, end-points.

Fixed size convolutional networks that share weights along a single temporal dimension are known as Time-Delay Neural Networks used in phoneme, spoken word recognition and online hand writing task.

[lecun-docrec]: http://yann.lecun.com/exdb/publis/pdf/lecun-98.pdf "Gradient based learning applied to document recognition"

[conv-nets]: http://yann.lecun.com/exdb/publis/pdf/lecun-bengio-95a.pdf "Convolutional Networks for Image, Speech and Time-Series"

## Slides ##

Object recognition is hard because there can be viewpoint changes, object in the image can be transformed in non-affine ways, pixel intensities may change and finally, viewpoint changes (translational variance); change in viewpoint makes the object appear in different locations

Viewpoint variation can be handled either through 
  
  * invariant features -- features that are redundant under transformations. (SIFT, SURF and so on...)
  * Put a box around the object -- normalization approach.
  * Feature sharing followed by averaging. Convolution + Pooling

Convolution nets can also be replicated across scale and orientation (not just position) but that is more complicated.

**Replicated feature detectors do not achieve translational invariance but equivariance.**
We are acheiving equivariance in neural activity and invariance in knowledge. 
That is the neural activity is same no matter where in the image the object appears (equivariance) and the feature is detected irrespective of where it appeared (knowledge invariance)

We can get a small amount of translational invariance by averaging over four neighbouring replicated detectors.
That is, the neural activity or the feature map will remain the same in the entire region of pooling/averaging.
Because of pooling we loose precise spatial relationships between high-level parts, that is we may recognize that the image contains few eyes, nose and mouth etc. but to recognize whose face, precise spatial locations of the individual objects is required.

Ways of including prior knowledge:
  * Neural connectivity
  * Weight constraints
  * Neuron activation functions
  
Forces the network to learn what we have in mind (prejudice); Alternatively we can use our prior knowledge to create whole lot more training data. (Hofman & Tresp '93).
Lenet uses local connectivity, weight sharing and pooling to inform knowledge about invariance. 
LeNet 80 errors on 10,000; Ranzatop 2008 40 errors by including lot of transforms over input.
Ciresan et.al. 2010 by lots of synthetic data => can use larger network on GPU without over-fitting; 35 errors. with model averaging 25 errors.  
McNemar test for comparing image classification methods.

AlexNet entry for ImageNet 2012 had a much smaller error rate 16% compared to the next best, 26%. 
The rest of the entries in ImageNet came from Computer Vision labs.
AlexNet had 7 layers (not including convolution layers), the early layers were all convolutional.
The last two layers were globally connected. 
The last layers look for combination of features generated in earlier layers and because there are combinatorially many such, they contain a large number of parameters.
It used ReLU activation units -- train much faster and more expressive than logistic.

Competitive Normalization is another technique that is generally used to help with variations in intensity.
Suppresses hidden activities when nearby units have stronger activities. 
For example, a blurry edge in the midst of high intensity - feature rich neighbourhood. 
Trained on 224\*224 patches in 256\*256 images, 4 corner patches+central patch + patches from left-right reflection => 10 patches per example. 
Dropout regularization.

Dimension hopping is why we need conv. layers. 
A dataset is said to be doing this if the input or the features hop the sub-space and still have the same input. 
For example, consider the case where we want to predict the risk of heart attack given age, weight, family history etc. 
If the weight and age fields are swapped for some input entries, we don't expect them to have same label as the entries that are not swapped (?!)  
Consider if we are trying to recognize a pattern in an image or wave-form, then the ink bloat or wave amplitudes can shift/translate and still have the same label. 
This is dimension hopping.

## Good questions from quiz ##

Brian mangled the digits dataset and cannot be repaired, but the damage is caused with the same pattern on both the train and test. 
For example, pixel i swapped with pixel j (and for several other pixels) in both.
This puts Conv. Nets with small weight windows at disadvantage because they may not be able to capture any regularities such as loop or stroke or edge when the data is mangled like this.
Note that conv. nets that span the entire image-size or fully connected layers may stll solve the problem.

## Programming assignment ##

  * Do not assume that when the initial weights are all set to zero, then they are going to remian there.
	They will remain all equal but not zero. 
	The gradients will flow through bias one iteration at a time.
	In the first iteration, the bias in the last layer will move away from 0 because it is just the sum of error derviative in the next layer over the entire batch.
	That makes the state of last but one layer non-zero in the next iteration which will make the gradients move.
   * ~~The model trained in this assignment unlike CBOW or skip-gram, does not put words in similar context closer~~
     Two words are put closer if one can be replaced by other and still form a valid 4-gram.
     Tail words that appear less than 10 times, say, barely get any update and hence will be closer to their initialized values. This may lead to wrong inference that they are similar. 

With 50 dimensional embedding and 200 width hidden layer, after 5 epochs

| momentum | train CE | validation CE | test CE |
|----------+----------+---------------+---------|
|        0 |    3.986 |         3.944 |   3.947 |
|----------+----------+---------------+---------|
|      0.5 |    3.328 |         3.254 |   3.252 |
|----------+----------+---------------+---------|
|      0.9 |    2.714 |         2.716 |   2.725 |
|----------+----------+---------------+---------|

After 10 epochs

| embed size | hidden width | train CE | val CE | test CE | time(s) |
|------------+--------------+----------+--------+---------+---------|
|          5 |          100 |    2.811 |  2.829 |   2.839 |     775 |
|------------+--------------+----------+--------+---------+---------|
|         50 |           10 |    3.014 |  3.027 |   3.024 |     644 |
|------------+--------------+----------+--------+---------+---------|
|         50 |          200 |    2.535 |  2.604 |   2.611 |  27,953 |
|------------+--------------+----------+--------+---------+---------|
|        100 |            5 |    3.231 |  3.236 |   3.232 |     708 |
|------------+--------------+----------+--------+---------+---------|

Momentum=0.5, after 1 epoch; 50, 200 hidden layer widths

| learning rate | train CE | valCE | test CE |
|---------------+----------+-------+---------|
|         0.001 |    5.296 | 5.089 |   5.092 |
|           0.1 |    4.436 | 4.385 |   4.392 |
|            10 |    3.544 | 3.416 |   3.413 |

Momenum=0.5, after 10 epochs; 50, 200 hidden layer widths

| learning rate | train CE | valCE | test CE |
|---------------+----------+-------+---------|
|         0.001 |    4.380 | 4.382 |   4.388 |
|           0.1 |    2.934 | 2.924 |   2.923 |
|            10 |    3.358 | 3.311 |   3.318 |

# Week 6 (Mini-batch gradient descent and learning rate)#

The desirable descent or ascent in the case of gradient descent is that we want to move quickly in the directons with low gradient and slow in the directions with high gradient (going faster in this direction can change the sign of gradient)
I can't help but mention it is just like mountain climbing.

Stchastic gradient descent (online learning) and mini-batch learning are of utility when there is a lot of data redundancy.
More efficient if mini-batches are balanced for classes. 
If one batch contains one class only and the next the other then that would cause updates to slosh unnecessarily.

An indicator that the learning rate can be turned down is when the validation error stops decreasing consistently.

Tricks and recommendations for mini-batch gradient descent:

  * Initialisation: If a hidden unit has big fan-in, then small changes on many of its incomming weights can cause the learning to overshoot. 
	We generally want small incomming weights when fan-in is big, so initialize $$\propto$$ sqrt(fan-in)
  * Shifting the input: transforming the input such that it is zero mean helps a big way.
	It simplifies the error surface.  
	![Error surface when we shift](/assets/images/nn-notes/error-shen-shift.png)  
	As shown in the figure, for the two inputs to be satisfied when not shifted have the parabolic troughs lying on $$(w_1*101+w_2*102)$$ and $$(w_1*101+w2*99)$$ whose intersection we are interested.
	The two lines which are almost parallel look a lot different when a constant is subtracted from the input such that the inputs sum to one.  
	In this respect, hyperbolic tangent is better than logistic because it produces outputs that roughly sum to 0.
	But, logistic is better in other aspects, it gives you "rug to sweep things under" by being robust to fluctuations in the input because of saturation whereas for hyperbolic you have to go to the end of its plateu before it can ignore anything.
  * Scaling the inputs: this has the same effect of making the error surface circular. 
	One way of doing this is to make the variance of each of the input components one. 
	More sensible thing to do is, decorrelate the inputs (correlated inputs can also increase the training time) by doing PCA, remove components that correspond to low eigen values and divide the rest by the square root of eigen value.  
	**In the case of circular error surface, the gradient points straight towards the minimum**
  * **Confusing plateu with local minimum:** Both the cases below can be confused with local minimum.
      * When we start with very big learning rate then the weights will either become big and negative or +ve leading to saturation. 
		When saturated, the error derivatives go to zero
      * In a multi-layer networks, the network quickly learns to set the outputs to equal the proportion of times it should be one and may take much longer to improve on it. 
		Which may look as if the objective function has plateaued
  * Do not turn down the learning rate too soon because that may smooth random fluctations between the batches too much that it can plateau after some time.
  
## Tricks to speed up training ##
  
  * Momentum trick: Most common trick for training large neural nets. 
	In this metod, we change the velocity instead of position.
	The intuition is that often the gradient has two components: one that goes down the ravine and other that goes along. 
	It is not desired to go across the ravine and momentum method helps build velocity along the ravine while cancelling the changes across.   
	$$v(t)= \alpha * v(t-1) - \epsilon*\frac{\partial E}{\partial w}(t)$$  
	$$\delta w(t) = v(t)$$  
	$$\alpha$$ is really viscocity but we call it momentum.
	When the surface is a tilted plane then it reaches a terminal velocity where slow down due to viscosity will balance the pull downward.
	A momentum that is close to one can multiply the speed by a large amount, the expression for momentum increase is: $$\frac{1}{1-\alpha}$$.  
	Using just a large learning rate alone may just cause divergent oscillations. 
	Momentum on the other hand have larger velocity in directions of consistent gradient.  
	Nestorev ('83) method to improves the momentum method.
	In the momentum method, we measure the gradient at this location and add it to the accumulated gradient. 
	Nesterov method: make a big jump in the direction of accumulated gradient, measure the gradient at the new location and then move in that direction.
  * Jacobs in 1980, idea is each connection in net has its own adaptive learning rate which is set empirically.
	Gradients in the initial layers are much smaller than the later ones, especially if the initial weights are small.
	If a layer has large fan-in, the chances of over-shoot are high by changing the values of incoming weights.  
	$$\delta w_{ij} = - \epsilon g_{ij}\frac{\partial E}{\partial w_{ij}}$$  
	if the previous and the current gradient are of same sign then  
	$$g_{ij}(t) = g_{ij}(t-1)+.05$$ else  
	$$g_{ij}(t) = g_{ij}(t-1)*0.95$$
	
	The gains should all be started with one, which ensures that when gradients alternate in signs randomly then the value hovers at one instead of converging or diverging.
	It is a good idea to cap gains between lower and upper bounds.  
	Better use it with full-batch so that fluctuations of mini-batch does not mis-lead.
  * rprop and rmsprop: extension of Jacobs for mini-batch learning. 
	rmsprop is Hintons favorite.  
	rprop: The magnitude of gradient can be very different for different weights and can change during learning, what if w eonly look at signs and make a step in the correct direction based on that.
	Step size for each weight needs to be updated for each weight.
	Increase/decrease the step size multiplicatively by 1.2 or 0.5 depending on if the last two gradients agree in sign or not.
	Advice: limit the step sizes: [50, 1E6]
	This has the same ill effects of fluctuating gradients, stochastic gradient descent works by averaging over many updates but this method does not.  
  * Enters rmsprop which is the mini-batch version of rprop.
	This is a lot different from rprop actually, does not have adaptive learning rate for each weight or not worrying about the gradient value.
	It works by keeping a moving average of gradient at every iteration.
	The problem with mini-batch is that we divide by a different number for each mini-batch, why not make the number we divide with be close for successive batches.  
	$$MeanSquare(w,t)=0.9*MeanSquare(w,t-1)+0.1*(\frac{\partial E}{\partial w}(t)^2)$$  
	Then dividing the gradient by root mean square makes learning much better.
	It works as well as gradient descent with momentum  
	Susketeer (2012) unpublished work combined rmsprop with Nestorev method to good results.  
	Yan Lecun's: "No more pesky learning rates"
	
	![Useful summary](/assets/images/nn-notes/slide-lat-lecture6.png)

# Week 7 (Training Recurrent Neural Networks) #
Neural Networks for sequences. 
Teaching signal is through trying to predict the next input and blurs the difference between supervised and unsupervised learning. 
The target is the input advanced by one step.
Below  we will summarize existing models for sequences

* Auto regressive models (Memory-less): predict the next input from k previous inputs. 
  In Linear auto-regressor, it is just weighted average of previous inputs.
* Feed forward nets: generalize above by introducing hidden units and input to which is from the k previous inputs.

We move beyond memory-less models by having the hidden state encode some information about what is seen already.
There are two widely used models of this sort

  * Linear Dynamical systems (popular with Engineers): Remember the vehicle localzation technique based on measurements from various noisy sources with Kalman Filters.
	It is assumed that the vehicle is moving with constant velocity (without acceleration) in a certain direction between measurements. 
	The hidden state in this case is the actual position of the vehicle which has (assumed) linear dynamics. 
	A linearly transformed Gaussian is a Gaussian again, so the distribution over hidden state given the data so far is Gaussian.
  * Hidden Markove Model on the other hand have discrete one of n states. It is popular with computer scientists probably due to the discreteness of states. 
	The transitions between the states is probabilistic and decided by transition matrix.
	Given an output, we cannot say which hidden state produced it and can only comment on distribution of possible states hence the name hidden.
	To predict the next output, we need to infer the prob. distribution over the hidden states.  
  
Linear Dynamical systems and HMM are stochastic. 
Recall that the transition from HMM state A to B is a draw from transition matrix. 
The posterior probability on the states that generated an output is deterministic, though.
RNN models are deterministic, one can think the hidden states of RNN to be equivalent of deterministic probability distribution over hidden states ina linear dynamical system or hidden markov model.

**Limitation of HMM** one of the hidden state should be selected to generate the output. 
	If there are N hidden states then information equivalent of log(N) bits is coded about what is generated so far. 
    For example, there are several bits of information that are to be stored about a speaker when we are trying to transcribe a sentence.
	We shouild make sure that the various attributes such as intonation, semantics, syntactics etc. match between the first and second half. 
	If there are 300 possible syntactic forms, 100,000 semantic types and 1,000 different voice type+intonation combinations possible, then we need 300\*100000\*1000 hidden states.
	
RNNs to the rescue
  * Unlike HMM, they have can several active hidden states, which mean several states can remember several bits of information at the same time.
  * Unlike Linear Dynamic systems, they allow hidden states to update in complicated ways.
  
It is required in the case of RNNs the initial states of all the hidden and output states need to be specified. 
There is an alternative, though, if some or none of the states are specified then it is possible to backprop and learn the initial states as well.
  
Feeding input to RNN (this is tricky, make sure you understand)  
Imagine a grid of states with inter-connections between layers. 
These are several clones of the network, one can provide input in one of these ways:  
  * initial states of all the nodes are specified.
  * initial state of some subset of the nodes are specified
  * state of a subset of nodes can be specified at every time step. This is more natural way in the case of sequence data.

Similarly, there are ways of providing target.  
  * Specify desired final activities of all the units 
  * Specify desired activities of all units for the last few steps -- good for learning attractors
  * Specify the desired activity of a subset of the units. 

If we are trying to predict the next in the sequence, then initial states of subset of nodes can be specified and desired activation of the rest can be specified to the target.
Specifying initial states is like feeding a feed-forward network with the input.

## Training RNN -- backprop ##
RNN and feed forward nets are equivalent.
RNN is just a layered net that keeps using the same weights. 
We can just do normal backprop as in feed-forward and sum all the gradients to keep the weights same.

This is followed by a discussion how input and target be specified to RNN which I could not follow. 

**Toy Example** is choosen such that it is hard to deal with feed-forward and is good only with RNN.
We can try FFN which takes as input two binary numbers and produce the output, but
  * maximum length of the binary vector should be pre-defined and 
  * learning does not generalize from one position in the vector to the other.
  
The RNN employed to solve this problem has a hidden layer with three nodes and an output layer. 
The input at any instant are the two bits corresponding to the operands.
The output of RNN is the result of addition of the input two time steps ago, the lag of two steps: one time step for input to hidden and another from hidden to output.  
Finite state automata for this problem will have 4 states: cross of 2 possible output (1,0) and carry and no-carry. 
FSAs are constrained to be in one state at every time step.  
Our RNN will have 4 different activation pattern corresponding to 4 states of FSA.
With N hidden neurons, there are $$2^N$$ possible binary activity vectors, but there are only N^2 weights (which limit the representational capactiy)

## Why is training RNN difficult ##
Backpropagation in general irrespective of the activation function behaves like a linear system.
Once we fix the output values and consider their gradient the output is no longer bounded.
This leads to explosion or lead to dying of the weights especially is we are back-prop-ing across many layers.
Another difficulty with RNN is to capture teh long range dependencies, how do you output now depends on input k time steps ago.
4 ways of dealing with this.
  * LSTM -- Make RNNs ouyt of memory modules that are designed to remember for long times.
  * Hessian free optimization
  * Echo state networks and
  * Good initialization with momentum.
  
## Long term short term memory ##
Hochreiter & Schmidhuber (1997) demonstrated the remembrance for 100 steps.
	
![Useful summary](/assets/images/nn-notes/lstm.png)

Whenever the data is read from the memory cell and error that is caused by that is back-propagated all the way to time step where the cell is written to.

## Interesting questions from Quiz ## 

How many bits of information can be modeled by the vector of hidden activities (at a specific time) of a Recurrent Neural Network (RNN) with 16 logistic hidden units?  
Initially, I just gave my answer as 16, but there's more to it. 
If there are only two possible asctivation values: 1 and 0 say, then the answer is correct. 
Since it is a logistic hidden unit, let us say there are three values: close to 1, close to 0 and close to 0.5 -- in which case the answer would be log(16*log3), where the base is 2. 
In reality, the values are continuous and the amount of information is much more.

Consider the network below:

![Trouble-network](/assets/images/nn-notes/quiz7-q5.png)

This is the case where the gadient problem occurs in RNN.
Let us say is inputs at t=1,2,3,4 are 1,0,0,0 and output is seen only at t=4. 
The gradient with respect to $$W_{xh}$$ includes the product like $$h_1*(1-h1)$$ 4 times for each of the hidden units, that is eight terms less than 1 which leads to a diminishing gradient. 

# Week 8 (More RNNs) #

In this lecture, we see how RNN can be used to modeling text.  
Modeling on character level is better than on word level for several reasons. 
A learning method that is powerful enough should understand what strings make up words.
Dealing with words can have the problems with: (1) Morphemes, variations of one word (2) Words that should go together like New York (3) Some languages like Finnish can contain several morphemes in one single word which makes canonicalization hard.
Also, it is a lot easier to predict one of 86 characters than 100,000 words.

Just like any other RNN, each of the hidden units takes as input the activations in the previous time steps and the current input (character).
To understand the power of RNN in language modeling, imagine you are trying to predict the next character based on a tree traversal as shown in the image below.

![tree-like-model](/assets/images/nn-notes/lec8-tree.png)

The traversal is dependant on characters seen so far or in a time window and the current character.
The tree grows exponentially with branching factor of 86 (# chars).
In an RNN or any NN for that matter, each of the node is equivalent to hidden state vector.
This is a game changer because it reduces the complexity of representation and inference and because of distributed representation, different nodes can share structure.
For example, in the image above if it the tree learns that *fixin* is *fixi* is the current state because *fix* is a verb and it is likely to have an *ing* form.
This knowledge is not shared across all the verbs; which is not the case when we adopt a distributed representation.  
Since we require the hidden-to-hidden weights to depend on the character input, one way of doing it is to use 86 different transition matrices which leads to 86x1500x1500 parameters (1500 is the number of hidden nodes). 
To avoid the explosion in number of parameters, we need a different transition matrix that also takes the current input into consideration.
Also, we want to share parameters amoung character inputs that are similar that is transition matrices for characters 9 and 8 would be very similar.  
For this reason, we use factor model.
The idea is that there are several factors and each of them take two inputs: the previous hidden state vector and the current input. 
The weighted sum of both the inputs is computed and multiplied with the outbound weights that contribute to the hidden state vector in the next time step.

$$ c = \sum_{f} {(b^T w_f)(u_f * v_f^T)a}$$

where c is the hidden state vector in the next time step and a the state vector in this time step, b is the current input.
$$u_f, v_f, w_f$$ are the weights connecting current state, next state and the current input respectively.  
The number of parameters in such a model is F*(two times the number of hidden nodes + size of the input vocabulary), so things are more manageable.

The reading assignment of this lecture is about a work that made an RNN that predicts the next character.
The RNN model trained on 5 million strings with 100 chars each from Wikipedia on multiple GPUs for 5 days and optimized with HF optimizer.
For each string, it starts predicting from the 11th character.  
The model does a great job at:
  * producing the correct words almost all the time and when it produces a wrong word, it is more or less sensible.
  * it can match quotes, brackets etc. infact it can count and keep track of them.
  * it has a very good syntactic knowledge, but hard to pin how the knowledge is represented.
  * knows awful lot about semantic associations, for example: cabbage and vegetable, milk and cow. 
	One problem, is that it produces sentences with such associations, but they does not always make sense. 
	That it milk after cow does not always make sense. 
  * knows a lot about proper names, dates and numbers.
	To the extent that it generated a lot of proper names not seen in the training data.

When promped for next character in *Sheila thrunge*, it gives *s* and for *People thrunge*, it gives *<space>* meaning that it can differentiate bewtween singular and plural, although thrunge is not even an english word.  
It knows about capitalization: *Shiela, Thrungelini del Rey*

Tomas Mikolov and co are working on predicting the next word with embeddings and RNN (which do better than feed-foward networks)  
"RNNs require much less training data to reach the same level of performance as other models."
Other models as in n-gram models.
According to him, this will make them hard to beat.


## Echo state networks ##
I do not completely understand these, on one level they make no sense to me. 
The echo state networks (ESN) have a carefully set input->hidden and hidden->hidden weights. 
The hidden->hidden weights are set such that the length of the activity vectors stays about the same after each iteration. 
This allows the input to echo around the network for a long time.  
Use sparse connectivity: creates a lot of loosely coupled oscillators.  
The scale of input->hidden weights is set carefully such that "They need to drive the loosely coupled oscillators without wiping out the information from the past that they already contain. "

ESNs are fast because they just need to set the hidden->output weights which is linear and can do impressive modelling of one dimensional time series. 
For example, given a signal of the frequency wave, it can generate the sine wave with the input frequency.

Ilya Sutskever (2012) using rmsprop with momentum showed that if and when ESNs can be used to also back-prop to the hidden->hidden weights they can be trained very effectively.


## Reading Assignment (Generating Text with Recurrent Neural Networks) ##

RNNs have unstable relationship between dynamics of hidden states and weights which lead exploding or diminishing gradient.
This led to surprisingly low interest in RNNs.
Hessian-free optimization in the case of deep neural networks provided a solution to this problem which is extended to the case of RNNs in [Martens & Sutskever 11][hf-rnn] with a novel damping mechanism.

The goal of this work is to train a character level language model so as to predict the next character and in better compression of text.
Better compression beyond a ceretain point is possible only through deeper understanding of text's meaning.
The final model learned demonstrated deep knowledge about the meaning of text as discussed in the lecture.

Few ways to combat the back-prop fiasco:
  * LSTM
  * HF optimization
  * Echo state networks.

In a typical RNN, the sum of weighted sum of previous hidden state and weighted sum of input generates the input to the next state. 
This model did not perform well because the hidden transition matrix is independent of input.
They proposed: Multiplicative RNN with new temporal arch. 

**The diffculty in learning weights**
The effective weight $$W_{ij}^{(c)}$$ the weight from hidden layer i to j given the input c is given by $$\sum_f{W_{if}W_{fc}W_{fj}}$$.
If for example, $$W_{if}$$ is very small and $$W_{fj}$$ very small then we may have large gradient for small value and small gradient for large value.
This can be handled by HF optimizers with second order derivatives.
This work uses them for this reason.

The character level language model seems unnecessary since we know that morphemes are appropriate unites to make semantic and syntactic predictions.
Converting a large text with words into smaller chunks of morphemes is non-trivial.

## Hessian-free optimization (optional lecture material)

How far to move in the direction ogf gradient before the vaklue starts rising again. 
It depends on the curvature, we generally assume uniform curvature (quadratic surface).  
A good direction to move in is one with a high ratio of gradient to curvature, even if the gradient itself is small.

The direction of gradient is fine on  cicular cross section so lets remove the curvature by Newton's trick.
Newton's method of finding such directions is multiplication with inverse of curvature matrix.
On a real quadratic surface it jumps to the minimum in one step. 
Unfortunately, with only a million weights, the curvature matrix has a trillion terms and it is totally infeasible to invert it.

The off-diagonal terms in curvature matrix correspond to twists in error surface.
There are tricks to avoid inverting large hessian matrix: 
  * What if we just use the terms on the main diagonal i.e. self interactions -- Le Cun
  * Approximation methods: Hessian-free methods and LBFGS
  
Hessian free methods use a minimization technique called: conjugate descent.
We do not make one-shot minimization, but minimize iteratively.  
"Use a sequence of steps each of which finds the minimum alongone direction. "  
"Make sure that each new direction is “conjugate” to the previous directions so you do not mess up the minimization you already did."


[hf-rnn]: http://machinelearning.wustl.edu/mlpapers/paper_files/ICML2011Martens_532.pdf "Learning Recurrent Neural Networks with Hessian-Free Optimization"

## Regularization in RNNs ##
There has been considerable amount of effort put in to making RNNs learn long range dependencies.  
To handle the vanishing and exploding gradients in RNN, tensorflow uses the technique of `clip by global norm` proposed by T. Mikolov and co. in http://arxiv.org/pdf/1211.5063.pdf.
The technique itself is quite simple: `t_list[i] * clip_norm / max(global_norm, clip_norm)` where t_list is the tensor supplied.
`clip_norm` is a supplied parameter and global norm is the computed norm of t_list.

# Week 9 (Ways to make neural networks generalize better) #

Overfitting happens because there can be sample error in the training
sample due to the way the data is sampled. This will reduce generalization.
## Overview of ways to improve generalizaton
### Get more data
A best bet if more data is available.
### Use a model that has the right capacity
### Average many different models
Models with different forms will make different mistakes making the
average better.  
Train the model on different subsets of training data called
"bagging".  
### Bayesisan approach
Use a single neural network architecture, but average the predictions
made by many different weight vectors.  
Bayesian approach can come in handy when there is very less training
data and basically does it by introducing priors.

**Limiting model capacity**

  * Architecture: limit number of hidden layers or their width
  * Early stopping: Start with small weights and stop learning before
    overfits
  * Weight decay: Penalize large weights with L2 or L1 penality. It is
    referred to as weight penality because the penality acts like a
    force that is pulling the weights towards zero.
  * Noise: Add noise to the weights or activity.
  
Imagine a neural network starting with small weights and logistic
hidden units, when the weights are near 0, the inputs function like a
linear unit. This reduces the effective capacity of the model. 
As training happens, more hidden units evolve towards non-linear. 
The model capacity smoothly transitions from low to high as it is
learned.
The network with smaller weights is simpler than the one with larger
one is the basis for Early stopping and weight decay.

### Weight decay ###
L2 weights improve generalization a lot because it prevents the
network from using the weights that are not needed. 
It creates a smoother model in which the output changes more slowly
with the input. 
**If the network has two similar inputs, then it tends to put equal
weight on them rather than unsymmetrical w,0 like.**  
L1 penality sets can make many weights equal to zero which helps in
interpretation.  
Sometimes it is better if we only try to pull small weights but have negligible effect on large weights. 

**Weight penality vs Weight constraint**  
Weight constrainsts are a way of saying that the weights should not
explode beyond certain threshold. 
It does hold on to weights from growing and avoids explosion.
It is easier to set a sensible value for it. 
When the weights exceed the limit, then all the weights are scaled
determined by the big gradient.  
This is more effective than a fixed penalty at pushing the irrelevant
weights to zero.

**Setting parameters on training set alone is insufficient**  
For this reason, we divide the data in to training, validation and test set. 
Meta-params are set on validation data and final performance is
reported on test set.
It is possible that we are over-fitting on the validation data.  

We cannot afford to do it when the training data is too small. 
Cross-validation lets training on entire data by rotating the
validataion data subset.

### Using noise as regularizer ###
We can add noise to either input or activations and achieve
regularization.

Consider a simple network with linear units, if we add guassian noise
to the input with variance sigma^2 then the output of the hidden node
will have an additive noise of w_i^2*sigma^2.
Minimizing the sum y_i plus this noise is equivalent to L2 penality
over the weights with the penality strength sigma^2.

Adding noise to the weights of multilayer non-linear neural net is not
exactly equivalent to using L2 penality.
It may work better especially in RNN; A. Graves RNN that recognizes
hand-writing works significatly better when noise is added to the
weights.

We can instead add noise to the hidden unit activations. 
For example, in a logistic unit, we compute the activation and treat
like the probability of the output unit to be one. 
The unit behaves like a binary stochatic unit, where:  
$$P(s=1) = \frac{1}{1+e^{-z}}$$  
But, during the backpropagation we compute as if we had done the
forwrd pass properly.

### Bayesian approach to regularization ### 
Wenever you see a squared error being minimized, you can find a
probabilistic equivalent that is to maximize the log likelihood. 
This comes from the assumption that the output is a mean of the
gaussian and likelihood of the target value is given by the guassian
with mean given by the output.

In the bayesian fremework we try to maximize P(W\|D) which is
P(D\|W)*P(W) (ignoring normalization term that does not depend on W).
log P(D\|W) will give a term like:

$$\frac{\sum{(y_c-t_c)^2}}{(\sigma_D)^2}$$ 

and log P(W) will give

$$\frac{w^2}{\sigma_w^2}$$

Assuming that w is a guassian prior with zero mean. 
The regularization prior term is given by the ratio of $$\sigma_D$$
and $$\sigma_w$$ and is not a random term in this framework.

**MacKay's quick and dirty way of fixing weight costs**  
This allows for different weight penalities for different subset of
weights in the network because it does not use validation sets. 
MacKay used it win several competitions.

The summary/method is:  
* Start with some initial guess for ratio of noise variance and weight
  variance
* DONE In a loop:
    * Do some learning using the ratio as weight penality coeff.
    * Set the noise variance to the variance of the residual errors
    * Set he weight prior variance to variance of distribution of
      actual learned weights: $$\sum_j{(w_ij-0)^2}$$ (I am not very
      clear about this.)
	  
## Good questions from quiz ##
L2 regularization will penalize larger weights than the smaller ones
because the function is of second order. 
If you want to penalize smaller weights more than the larger weights,
you should use f(x)=x^0.5 which looks like an inverted banana peel.

`Different regularization methods have different effects on the
learning process. For example L2 regularization penalizes high weight
values. L1 regularization penalizes weight values that do not equal
zero. Adding noise to the weights during learning ensures that the
learned hidden representations take extreme values. Sampling the
hidden representations regularizes the network by pushing the hidden
representation to be binary during the forward pass which limits the
modeling capacity of the network.`

![activation histogram](/assets/images/nn-notes/activation-hist.png)

An activation histogram that looks like this could mean that
activation sampling and adding noise to the weights is used as
regularizers. 
Sampling leads to non-continuous outputs leading to either firmly on
(+1) or firmly off (-1) states of the previous layer.  
It cannot be L1 or L2 because in that case the middle region where it
is closer to zero cannot be as empty as here.

![weight histogram](/assets/images/nn-notes/weight-hist.png)

The histogram above corresponds to L2 regularization because the
frequency of the large weights is seriously limited.

## Programming Assignment ##

| Learning rate | Momentum | Training Loss | Validation Loss | Test Loss |
|---------------+----------+---------------+-----------------+-----------|
|         0.002 |        0 |      2.304283 |                 |           |
|          0.01 |        0 |      2.302117 |                 |           |
|          0.05 |        0 |      2.292967 |                 |           |
|           0.2 |        0 |      2.228969 |                 |           |
|           1.0 |        0 |      1.598844 |                 |           |
|           5.0 |        0 |      2.301322 |                 |           |
|          20.0 |        0 |      2.302585 |                 |           |
|         0.002 |      0.9 |      2.300135 |                 |           |
|          0.01 |      0.9 |      2.284022 |                 |           |
|          0.05 |      0.9 |      2.008606 |        2.018598 |  2.008179 |
|           0.2 |      0.9 |      1.083429 |        1.122502 |  1.097623 |
|           1.0 |      0.9 |      2.018723 |        2.041323 |  2.038473 |
|           5.0 |      0.9 |      2.302585 |        2.302585 |  2.302585 |
|          20.0 |      0.9 |      2.302585 |        2.302585 |  2.302585 |

With 200 hidden units, 0 weight decay and learning rate of 0.3, momentum 0.95, mini-batch size of 100 and 1000 iterations lead to
`a3(0, 200, 1000, 0.35, 0.9, false, 100)`
The loss on the training data is 0.002614
The classification error rate on the training data is 0.000000

The loss on the validation data is 0.430185
The classification error rate on the validation data is 0.087000

The loss on the test data is 0.464988
The classification error rate on the test data is 0.093778
With early stopping: 0.334505

Effect of weight decay on loss

| Wd_param | Training loss | Validation loss | test loss |
|----------+---------------+-----------------+-----------|
|        0 |      0.002614 |        0.430185 |  0.464988 |
|   0.0001 |      0.007561 |        0.348294 |  0.369097 |
|    0.001 |      0.070793 |        0.287910 |  0.289973 |
|     0.01 |      0.442156 |        0.509763 |  0.511233 |
|        1 |      2.302585 |        2.302585 |  2.302585 |
|        5 |      2.302585 |        2.302585 |  2.302585 |

Effect of model capacity

| Width of hidden layer | Training loss | Validation Loss | Test Loss |
|-----------------------+---------------+-----------------+-----------|
|                    10 |      0.011050 |        0.421705 |  0.389471 |
|                    30 |      0.004042 |        0.317077 |  0.364651 |
|                   100 |      0.002849 |        0.368593 |  0.408845 |
|                   130 |      0.002715 |        0.397597 |  0.418396 |
|                   200 |      0.002614 |        0.430185 |  0.464988 |

Effect of model capacity with early stopping

| Width of hidden layer | Training loss | Val. Loss | Test loss |
|-----------------------+---------------+-----------+-----------|
|                    18 |      0.037047 |  0.306083 |  0.284525 |
|                    37 |      0.284525 |  0.265165 |  0.282510 |
|                    83 |      0.059285 |  0.311244 |  0.337624 |
|                   113 |      0.064678 |  0.313749 |  0.347098 |
|                   236 |      0.076253 |  0.343841 |  0.339124 |

# Week 10 (Combining multiple neural networks to improve generalization) #
When you average across many models, then you can expect to do better than any single model.
We can reduce the over-fitting in the case where we have small amount of training data.
It helps most when the models make very different predictions.

The squared error is sum of bias and variance, for a complex model the bias is reduced and also the evariance can be reduced by averaging over many models.
The variance in the case of when we consider the output as the average over all the models is less than that of when any of the model outputs is randomly choosen by the variance of model outputs.

when $$\bar{y}=<y_i>_i=\frac{1}{N}\sum_{i=1}^{N}{y_i}$$

$$<(t-y_i)^2>_i = (t-\bar{y})^2 + <(y-\bar{y})^2>_i$$

This trick of averaging will work only for convex or concave functions depending on if you are minimizing or maximizing.  
We can train different models that make different predictions by
  * re-running and hoping that the learning algo. will each time gets stuck in different local optima.
  * Use lots of differnt models, does not matter if they are neural nets.
  * Diff Neural nets by varying:
      * NUmber of hidden layers, Number of hidden units, activation functions
      * weight penality (L1, L2)
      * learning algorithm: mini-batch, full-batch
Another trick is to train on different subsets of data: bagging (pick data subsets with replacement). 
Boosting: train several low capacity models and weight them such that much computational resources are not spent on instances that some other classifier already did a good job on.

## Mixture of experts (developed in 1990s) ##
This can make very good use of extremely large datasets
We train models that learn on subsets of data making them an experts in the data they have seen.

One one hand we have models like K-nearest neighbours that are very local and make a decision depending on local context, on the other hand we have global models which try to fit one model to all the input, output pairs. 
Mixture of experts falls somewhere in between.
We will have number of experts less than that of KNN and also we are not interested in clustering over input alone in the case of Mixture of Experts, but rather interested in grouping of inputs such that their outputs can be better modeled.

E = $$<(t-\bar{y})^2>_i$$

will train the mixture of cooperative models.
The models each will try to reduce the error caused by other models whether or not that will bring them closer or away from target.

E = $$<p_i(t-y_i)^2>_i$$

The weights $$p_i$$ are determined by the manager or the gated network.
The update equation for this network are such that the correction to the weights of an expert is the product of $$p_i$$ and the error that it made. 
The update equation for variable that results in gate output for a certain gate is based on if the error of this expert is more or less than the average error.

## Full Bayesian learning revisited ##

Unlike MLE, we do not estimate one set of parameter that do well over the data, but full posterior distribution over all possible parameter settings.
In this setting, there is no case of over-fitting. 
When you keep track all the parameter setting, each of the parameters will have blunt probability distribution, but together can perform well; As we get more data the probability distr. of parameters get sharper.

In the case of neural networks, it is not practical to do full bayesian learning. 
Imagine a case where we have 6 parameters and allow each of them to take 9 different values then the grid is 9^6 large.(not feasible for large nets)

$$P(t_{test}/input_{test}) = \sum_{i}{P(W_i/D)*P(t_{test}/input_{test},W_i)}$$

## Making it practical with MCMC ##
We start with random vectors in the weight space and update the weights with some noise so the weights will never converge, but keep wandering.
We can use mini-batch gradients such that the noise we require can be supplied by sampling noise of mini-batch.


```
 If we use just the right amount of noise, and if we let the
weight vector wander around for long enough before we take a sample, we
will get an unbiased sample from the true posterior over weight vectors. 
```

## Dropout -- Averaging without using several models ##
In the case of single hidden layer and dropout, we omit the output of one of the hidden units with some probability. 
Which means that there are 2^H different architectures possible, some of them may not even get trained explicitly. 
All the different networks share weights which works like a strong regularizer.
The weight sharing is a much better regularizer than L2 or L1 penalities which just pull the weights towards zero while in dropout the weights are pulled towards the correct value.

In the case of single hidden layer, dropout exactly computes the geometric mean of predictions of 2^H models.
When more than one hidden layer: dropout is a good approximation to averaging separate dropped out models.

We can also use dropout in input layer, typically with higher retention probability and is in use: "denoising autoencoders" P. Vincent.

Dropout prevent complex co-adaptations. 
These co-adaptations can go wrong over the new data.
When a hidden unit is forced to work with othe units which are unpredictable since there are many, it is more likely to do something that is individually useful and not useful when used by unit that is fed into.

## Reading Assignment 1 (Adaptive Mixtures of Local Experts) [link][expert-mixture] ##
Some points worth noticing and not discussed in the lecture are:

$$E^c = \|d^c-\sum_i{p_i^co_i^c}\|^2$$

A loss function computed like above used to be the standard before this one and it is bad for the following reasons.
The prediction of the system is contributed by all the experts, so in order to reduce the loss the experts cooperate and co-adapt so that one reduces the residue left by the other.
This is bad because we are using several experts for each case and it will be slow to train and as interference effects shown by a normal model.
Instead

$$E^c=\sum_i{p_i^c\|d^c-o_i\|^2}$$

It is also observed that a slightly different variant is quick to converge

$$E^c=-log{\sum_i{p_i^ce^{-\frac{1}{2}\|d^c-o_i^c\|^2}}}$$

This is better because, the expression $$\frac{\partial{E^c}}{\partial{o_i^c}}$$ contains sum over all experts in the denominator and hence helps to quickly pass the information that some expert is already doing well over a case, c.

Finally, the gating network can be a feed-forward network that receives the same input as the experts and outputs a softmax score over which expert to choose.
Then, a stochastic one-out-of-n selector will select an expert based on the expert responsibility probabilities above.

[expert-mixture]: http://www.cs.toronto.edu/~fritz/absps/jjnh91.pdf "Adaptive Mixtures of Local Experts"

## Reading Assignment 2 (Improving neural networks by preventing co-adaptation of feature detectors) [link][dropout] ##

The paper does not much detail that is not already covered in the lecture.

A few interesting points are:
  * dropout can pretty much be applied to any layer, be it input or any of the hidden layers. In the article, at least, the dropout on the input is reported to have lesser effect on performance 20% compared to 50% improvement.
  * For fully onnected layers, droput in all hidden layers works better than droput in only one hidden layer and more extreme probabilities tend to be worse, which is dropout of 0.5 is generally used.
  * "For datasets in which the required input-output mapping has a number of fairly different regimes, performance can probably be further improved by making the dropout probabilities be a learned function of the input, thus creating a statistically efficient “mixture of experts” (13) in which there are combinatorially many experts, but each parameter gets adapted on a large fraction of the training data."
  * Dropout can be seen as a Bayesian model, but instead of weighting the output of each model by their coditional on the data, Droput assumes that all the models are equally likely.
  * "Dropout can be seen as an extreme form of bagging in which each model is trained on a single case and each parameter of the model is very strongly regularized by sharing it with the corresponding parameter in all the other models. This is a much better regularizer than the standard method of shrinking parameters towards zero."

[dropout]: https://arxiv.org/pdf/1207.0580.pdf "Improving neural networks by preventing co-adaptation of feature detectors"

# Week 11 (Hopfield Nets and Boltzmann machines)
Hopfield Nets is composed of binary threshold units with recurrent connections between them.  
These are generally hard to train, but if the connections are symmetric, there is a good energy function.  
The energy function for the ho. Net with symmetric connections is:

$$E = -\sum_{i}{s_i*b_i} - \sum_{i,j}{s_i s_j w_i w_j}$$  
$$\delta{E_i} = E(s_i=0)-E(s_i=1)$$

The states of the system is set to a value based on the energy gap defined above, but it may not be the deepest possible.  
We only make sequential update, else the energy may not go down or can get stuck in oscillations.

A paper published in 1982 put forward the use case in applications of memory retrieval and content-addressable memory.  
* The network can restore the neural network weights from "corrupted data" by running the conf. to minimum energy.
* It is possible to access content with partially specified bits.
* A system can be made robust if the other components converge to a better state when one of the components have failed.

To store memory in such network, we update the weight of the connection between two nodes as $$\delta{w_{ij}}=s_i*s_j$$.
Which correspond to the least energy state and hence can correct if it deviates from it.

## The memory capacity of hopfield nets and methods to improve it

In a Hopfield net with N memory units, the state of the bits of each of the states are not completely uncorrelated, because it should be possible to retrieve memory without ambiguity.
This limits the capacity, and according to hopfield, it is effectively 0.15N^2 for the N^2 total connections in the network.
The capacity is almost useless because of the storage required for N^2 weights and biases which is N^2log(2M+1) where M is the  number of memory units; log(M) because we either add or subtract one each time we make an update to w, which means $$w \in [-M, M]$$

To explain a little more on limited capacity of the HN, when the energy of two state space are very close then they may fall in the vicinity of another state configuration that has even smaller energy, i.e. they are no longer the local minima.
This means that we cannot use those state values.

There is an interesting description of this spurious minima can be avoided with "unlearning" and a theory put forth by Crick and Mitchinson that we are possibly doing unlearning to correct the spurious minima while dreaming (REMM state).
That is our brains do some unlearning and fit a MLE model on how well your NN is able to explain the things it has seen in the day.

The solution to improving capacity is to use perceptron convergence procedure and instead iterate over the data several times.

## Using Hopfield energy states for a good interpretation of inputs with the underlying hidden states

Can we use them such that low energy correspond to a good interpretation?

For example, although we see the world in 2D, projected on retina, we constantly interpret the world in 3D.
The example provided in lecture is our interpretation of structure based on observing a few bunch of 2D lines.
If we have one neuron for every possible 2D line that we see in the world, then the activated neurons each will activate hidden states that activate the neurons that corresponds to 3D edges and common-sensical knowledge that, for example edges generally connect orthongonally in real world and that two intersecting edges should have same depth at intersection, all lead to added constraints. 
If you consider necker cube (image below), then it has two possible interpretations which have same low energy states.

![necker cube](https://upload.wikimedia.org/wikipedia/commons/thumb/e/e7/Necker_cube.svg/220px-Necker_cube.svg.png)

## Simulated annealing and thermal equilibrium
One way to avoid local minima is to add noise to the threshold states initially and calm the noise down from there on (Simulated annealing)

$$P(s_i=1) = \frac{1}{1+e^{\delta{E_i}/T}}$$

where T is temperature and $$\delta{E_i}$$ is the energy gap.

After running the system for long enough time, the probability distribution of the configurations stabilise and is refered to as "thermal equilibrium"
Each of the configurations may not have stabilised, but the switching of one confguration to other is such that their dist. remains the same.

These two concepts play a role in Boltzmann machines.
Boltzmann machines make a stochastic update with an assumed constant temperature of one. 
Hopfield nets update weights deterministicaly that is as if T=0.

There is a stark similarity between activation of a binary threshold unit in BM and in a neural network.
In a BM with temp=1:

$$P(s_i=1) = \frac{1}{1+e^{-\delta{E}/T}}$$

$$\delta{E} = E(s_i=0)-E(s_i=1)$$

The energy gap is exactly equal to the weighted sum of nodes connected to the node being updated plus bias.

## Boltzmann Machines aka stochastic hopfield networks
A causal generative model generates by sampling the hidden state from their probability distribution and generating the visible states conditional on the hidden states.
BM on the other hand, everything is defined as energy.
$$P(v,h) \propto e^{-E(v,h)}$$
The expression for E contains 5 terms that is, the bias terms for visible and hidden states, weights connecting visible, hidden and between.
Since for the computation of P(v,h) requires the partition function, hence MCMC.
MCMC is used to sample (v,h) and also to sample posterior, distribution over h.
For sampling over posterior, the visible states are clamped to obeserved and only the hidden states are updated when we stochastically pick a node and update it based on energy gap.
# Week 12 (Restricted Boltzmann Machines -- RBMs)
## Boltzmann Machine Learning algorithm
We have seen how BMs can be used to model probability distributions of a given data vectors, we shall see how they are learned.

Unlike in the supervised setting, where we have noth data and labels and we back-propagate to learn the weights; In BMs there are no labels.
The goal of learning is to maximize the product of probabilites that BM assigns to the binary vectors in the training sets.

Learning BMs can be hard because the updates to weight between any two states requires the information of other weights.
For example, in figure below: for the training data, (1,0) and (0,1), we want the visible units to be negatively correlated. 
If all the weights are +ve or -ve then the inputs are +ve correlated, hence to update weight w1, we need info about w3.

![Why learning BM difficult](/assets/images/nn-notes/BM-learning-difficult.png)

Given the complex interaction between weights, it is surprising that local information can cater to the weight update.

$$\frac{\partial{P(v)}}{\partial{w_{ij}}} = <s_i*s_j>_{data}-<s_i*s_j>_{model}$$

The change to a weight is proportional to difference between correlation between $$s_i$$ and $$s_j$$ when the inputs are clamped to the visible vectors and the network is allowed to settle to a thermal equilibrium and correlation when there is no clamping.
The negative term to the right gets rid of the spurious minimum. It corresponds to the normalizing term in the denominator.

The process of settling to the thermal equilibrium itself propagates information about the weights -- don't need backpropagation.

To collect statistics over positive and negative phase, we start with some initial configuration and let the system reach thermal equilibrium with or without the visible unit clamping.
When we start with global configs (-ve phase), we expect that a small fraction of states corresponding to the data have low energy and the rest high energy, things get complex when the underlying model is multi-modal and hence can have different states with low energy.
These problems are fixed with thechniques discussed in next section.

## More efficient ways to train BMs

## Restricted Boltzmann Machines (RBMs)
Makes learning simpler by removing conections between the hidden states, also no connections between visible units.

In an RBM, it only takes one step to reach thermal equilibrium when the visible units are clamped.

$$P(h_j=1)=\frac{1}{1+e^{-(b_j+\sum_{i\in vis}{v_iw_{ij}})}}$$

As can be seen, the hidden state is active depending on the input from all the visible states (alone).

An efficient mini-batch learning procedure -- Persistent CD (PCD) by Tieleman, 2008. 
The basic idea od to use single markov state, which has a persistent state.
See the deeplearning.net resource below.

Contrastive divergence procedure of learning weights.
This is what we do when we wait for thermal equilibrium to update weights

![Slide23](/assets/images/nn-notes/rbm-slide23.png)

When we clamp the visible states to the training data and update the hidden states at time 0 (with the logistic activation above), we then reconstruct the visible state from hidden state at time step 1 and so on. 
The visible state at the end when the updates have stabilised is the *fantasy*.
Obviously, that will take several updates.

Instead, we can just ...

![Slide24](/assets/images/nn-notes/rbm-slide24.png)

correct for the confabulations rather than wandering a long way into the update.
This can have the effect of increasing the energy of alternate reconstructions surrounding the data point, creating a trough at the point itself.
This procedure may fail to recognize regions of data-space that the model likes, but are far from any data.
These low energy holes cause the normalization terms to bloat, but this procedure would fail to sense it.

A good summary of all the procedures of learning can be found [here][deeplearnin.net]

[deeplearning.net]: http://deeplearning.net/tutorial/rbm.html#sampling-in-an-rbm "Restricted Boltzmann Machines (RBM)"

## RBMs for collaborative filtering

Given that RBMs are good at filling the missing data, collaborative filtering which is filling of value in user-item matric is a natural problem.

The Netflix challenge has 0.5M users that rated 18,000 movies in total with a rating from 1-5. 
The task is to predict the rating for a user and movie pair in the held-out dataset. 
One way to see the problem is to model the user, movie, rating triplet.
If we use the language model, we would come up with a feature vector for user and movie and in the end feed the two vectors in say a network with hidden layer to predict the rating (Hinto explained though that, computation just the scalar product itself did equllay good). 
This approach is similar to the alternative, Matrix Factorization.

Using RBMs for this task requires re-formulation of the problem.
Every user is treated as an 18,000 vector of movies while the movie dimension can take 5 values. 
Since the data for one user could be sparse, the user vector is represented by only the movies rated by that user.
We use a separate RBM for each user and share the hidden to movie weights between them.

For more, look at [Salakhutdinov et al. 2007][netflix-07]

[netflix-07]: http://www.machinelearning.org/proceedings/icml2007/papers/407.pdf "Restricted Boltzmann Machines for Collaborative Filtering"

## Good questions from quiz

The weight update term: $$w_{ij}$$ that can be obtained from averge of product of states: $$s_i$$ and $$s_j$$ is only an approximation because in the case of BM models because computing $$<s_is_j>$$ over model or data is hard in general because it involves sampling from the distribution and several steps of update.
On the other hand, the computation of $$<s_is_j>_{data}$$ can be computed exactly because the system reaches thermal equilibrium in just one step.

If we ignore the former or the latter term in the weight update equation given by the difference in average over data and model, it does not lead to a good solution.
If we omit the former term, then we are not making use of data and if we omit the latter term then we are reducing the energy of the observed data point which could also be reducing the energy of other points that are not seen.

PCD can also be applied to BM.

![BM-activation](/assets/images/nn-notes/bm-activation.png)

# Week 13 (Belief nets) #

## A brief history of backpropagation

Backpropagation is proposed during 1970-80.
It was abandoned by many machine learning for reasons (actual according to Hinton): less computational power, less data, deep network weights not initialized properly, as a result, the gradients tended to die.

Backprop was abondoned for SVM which have a much fancier theory and easy to condition on a small data. 
He discusses two different views of SVMs: both views look at SVMs as clever reincarnation of perceptron.
* View 1: SVMs traforms the input into very large layer of non-linear non-adaptive features which is followed by one layer of adaptive weights. They have clever way of learning weights so as to avoid over-fitting through tarining of max-margin classifier.
* View 2: Use each input vector in training set to define a non-adaptive feature, and given a test input, a match is computed between the test and the train instance. The learning algo. learns to select the right feature(input) and weight them.  
Major drawback of SVM is that they cannot be trained to learn multiple layers of representations.

## Belief nets ##
The need for alternative or why back-propagation does not always work:
  * Requires lot of labeled data (Supervised learning)
  * learning time is very large probably because the weights are not properly initialized.
  * local optima problem.
  
He mentioned how probability was abhored by the AI community. 
According to them, computers are all about processing discrete symbols and introducing probability will infest everything.

Belief nets are directed acyclic graphs, typically sparsely connected.
The edges are the dependencies between variables.
Early graphical models used experts to define the graph structure and the conditional probabilities (such as the probability tables).
They only focused on inference (figuring out distribution of hidden states given the visible state) and not learning (because the graph is given by the experts).

Boltzmann machine is a binary siochastic neurons connected with symmetric weights (undirected) which are hard to learn as seen before, but the learning can be slightly made better by restricting (but RBMs are only one layer though)

Sigmoid belief nets are easier to learn with an appropriate algorithm.
This is because it is hard to infer the posterior distribution given a datavector due to explaining away property.
Consider the case where the two causes: *truck hits house* and *earthquake* can cause the effect: *house jumps*.
Although, the two causes are independent, they get dependent when the effect is observed.

| truck hits house | earthquake | probability |
|------------------+------------+-------------|
| 1                | 1          | 0.0001      |
| 1                | 0          | 0.4999      |
| 0                | 1          | 0.4999      |
| 0                | 0          | 0.0001      |

## Wake-sleep algorithm ##

To evade the *explaining away* problem, we draw posterior from a wrong distribution and yet make the learning work.

The poserior distribution is wrongly assumed to be a factorial diatribution, that is they are assumed to be independent even in the posterior just as in the prior.
A general probability distribition over N variables has 2^N-1 degrees of freedom (one less because they shopuld sum upto one).
A factorial distribution on the other hand has N degrees of freedom.

This is the algorithm that led to variational learning methods.

![Wake-sleep algorithm](/assets/images/nn-notes/wake-sleep.png)

The slide above summarizes the wake-sleep algorithm.

The wake sleep algorithm leads to incorrect mode averaging due to the factorial distribution assumption.
For example, in the example above of house jumping, since both (1,0) and (0,1) over the two causes are equally likely.
That is, when we generate from the model half the instances of a 1 at data layer will be caused by a (1,0) and other by (0,1).
Hence, the recognition weights (the weights that go from visibe state to hidden) will be 0.5 for both states which puts equal mass even on unlikely states: (1,1) and (0,0).

**Expression for gradient on weight in SBN**
Let $$w_{ij}$$ denote the weight from parent $$s_j$$ to the child $$s_i$$.
The probability of certain state of the network be denoted by *S*.

$$log(P(S)) = log(R)+log(Pn)+log(P(s_i/Pn))$$

where Pn is the parent nodes of $$s_i$$, R is the rest of the network. 
$$\frac{\partial log(P(S))}{\partial w_ij}$$ depends only on the last term in the above expression.
Making use of the fact that the network uses sigmoid activations, we have the gradient equal to:

$$\frac{\partial log(P(S))}{\partial w_{ij}} = s_j * (s_i-p(s_i))$$ 

which should be straightforward to derive.

## Reading Assignment 1 ([The wake-sleep algorithm for unsupervised neural networks][wake-sleep]) ##
The aim of learning is to minimize the "description length" which is the total number of bits that would be required to communicate the input vectors in this way.
By forcing the network to learn an economical representation of the data can help in realizing any regularities in the data.

> The neural network has two quite different sets of connections. The bottom-up “recognition” connections are used to convert the input vector into a representation in one or more layers of hidden units. The top-down "generative" connections are then used to reconstruct an approximation to the input vector from its underlying representation.

The activation of the stochastic neuron is given by:

$$P(s_v=1) = \frac{1}{1+exp(-b_v-\sum_{u}{s_uw_{uv}})}$$



## Programming assignment ##
The valiudation cross entropy loss for different classification learning rate
with rbm learning rate of 0.02, 300 hidden units and 1000 iterations over an RBM
is as shown in the table.
Also, the RBM input to hidden weights are not learned when training for classification.

| classification learning rate | validation xentropy loss |
|------------------------------+--------------------------|
|                       0.0001 |                 1.820011 |
|                       0.0005 |                 0.968603 |
|                         .001 |                 0.673805 |
|                         .005 |                 0.322890 |
|                          .01 |                 0.259159 |
|                          .05 |                 0.202482 |
|                         0.08 |                 0.198855 |
|                          0.1 |                 0.198644 |
|                          0.2 |                 0.203790 |
|                          0.5 |                 0.224757 |
|                            1 |                 0.272533 |

One of the questions in the programming assignments is about estimating the partition function when there are 10 hidden states and 256 visible states, which means that a brute force summation over all the terms would need sum over $$O(2^{266})$$ terms.
Instead it is possible to calculate the summation in $$O(2^{10}256)$$ calculations. That is, we iterate over all possible hidden states and for a given hidden state and the weight matrix, the summation over all the visible states is just $$\prod_{i}{1+e^{prob_v(i)}} $$ where $$prob_v$$ is $$h*rbm_w$$

## Cheat sheet ##
### BM ###
Most of the formalae in this section are also applicatble to RBMs since they are just BMs with only one hidden layer and visible layer and with some additional constraints.

$$E(v,h) = -\sum_{i}{a_iv_i}-\sum_{j}{b_jh_j}-\sum_{i,j}{v_ih_jW_{ij}}$$

This leads to 

$$P(h_j=1|v)=\frac{1}{1+exp(-\sum_{i}{W_{ij}v_i}-b_j)}$$

**Partition function** Z is given by $$\sum_{v}\sum_{h}e^{-E(v,h)}$$

**Goodness** is just the negative if the energy.

### RBM ### 

*CD1 update expression*
In the CD1 update, which can readily be extended to CDn in general, we start with a visible state and with the given hidden to weight vector.

$$P(h_1/v) = logistic(rbm_w*v)$$

$$h_1 \sim P(h_1/v)$$

$$P(v_2/h_1) = logistic(h*rbm_w)$$

$$v_2 \sim P(v_2/h_1)$$

$$P(h_2/v_2) = logistic(rbm*v_2)$$

$$h_2 \sim P(h_2/v_2)$$

Loss = $$-E(v, h_1) + E(v_2, h_2)$$

The $$v_2$$ term above is often referred to as "reconstruction" visible state.
The first term in the loss is to make sure that the visible state is more likely according to the model and the second term is so that the reconstructed weights have high energy.

[wake-sleep]: http://www.gatsby.ucl.ac.uk/~Dayan/papers/hdfn95.pdf "The wake-sleep algorithm for unsupervised neural networks"

# Week 14 (Deep neural nets with generative pre-training) #
## Layers of features by stacking RBMs##
Given that RBMs are easy to learn, can we stack RBM spo as to learn layers of features effectively?

When you stack several RBMs and learn multi-layer features, you expect that the resulting model would be equivalent to a BM, but instead it behaves like a SBN. 
Hence, it provides an interesting way of training the SBN.

>It can be proved that each time we add another layer of features we improve a variational lower bound on the log probability of generating the training data. (based on equivalence between RBM and infinitely deep belief nets.)

Deep belief nets are a hybrid of boltzmann machines (undirected connections) and SBN (directed connection)

Composing two RBMs make a DBN
![RBMs make a DBN](/assets/images/nn-notes/rbns-make-dbn.png)

The DBN shown in the diagram above has top two layers that make an RBM and the bottom two layers make a sigmoid belief net.
The reason for how and why is more complicated.

**Training a DBN with a variant of wake-sleep algorithm:**
  * Do stochastic bottom up pass and fine-tune the top down weights to reconstruct the features in the bottom layers
  * Do a few iterations of sampling in the top level RBM using CD (contrastive divergence)
  * Do a stochastic top-down pass and adjust bottom-up weights

We have seen a variant of wake-sleep for learning stack of RBMs to make them better at generation of inputs. 
We can use backprop to make this network better at discrimination.

Backprop works better when we pre-train weights because of **optimization and generalization reasons**. 
We do not start backprop until we have sensible feature detectors from pre-training which means that the gradients make more sense and backprop does not have to do a global search, but only a local search around the existing weights.

Because, most of the training happens on the unlabeled data and we use fine-tuning of the labels in the final stages, we do not require as much trainng data for fine-tuning.
In the fine-tuning stage, we are not learning to produce any new features, but just figuring out how to use them.

Hinton demonstrated with reults on permutation invariant MNIST task (where the pixels in MNIST images are randomly permutated) that learning a generative model of unlabelled digits followed by gentle backpropagation does better than generative model of joint density of images and labels, SVM, and backprop based optimizations in this order.

![RBMs make a DBN](/assets/images/nn-notes/pretraining-affect-learning-space.png)

The image above shows the t-SNE visualization of output mappiongs of various models in 2D space with and without pre-training.
Each blob in the image above correspond to a model, the cluster on the top is without pre-training and the bottom one is with pre-training.
The color of the blob indicates the epoch, blue->yellow as the # epochs increase.
The picture above is generated by concatenating of all the outputs on a test suite.
Things to note:
  * The cluster at the top is more scattered meaning that the outputs learn a local minimum that is sensitive to the initial state. While in the bottom one, the solutions are not as scattered.
  * There is no overlap between the clusters meaning that the solutions obtained with and without pre-training are qualitatively different.

**Why unsupervised pre-training works**

![Why pretrain?](/assets/images/nn-notes/why-pretrain.png)
As shown in the image above, image(high bandwidth) captures lot more information for the world than a label (low bandwidth).
That is when an image is labeled with "cow", we do not know if the image contains any other animals or if the cow is dead or where in the image the cow is.
Hence, follows the justification for the text provided in the image above.

## Modeling of real-valued data ## 
We can only go to some extent with logistic neaurons (actually they are referred to as "mean field logistic neurons", I don't quite understand what they are).
Consider the case of hand-writing images where a pixel can have varying intensity levels depending on the pressure applied by the user. 
The intermediate values (i.e. between 0 and 1) can be treated as the probability that a pixel is inked, but it won't be able to capture a fact that a pixel is almost always a mean of the neighbouring pixels, that is a pixel with intensity 0.69 is less likely to be 0.67 or 0.71.
The confidence interval (variance) is missing.
Hence, we use a linear unit with Gaussian noise.

## Guassian Binary RBM ##

$$E(v,h) = \sum_{i\in vis}{\frac{(v_i-b_i)^2}{2\sigma_i^2}}-\sum_{j\in hid}{b_jh_j} - \sum_{i,j}{\frac{v_i}{\sigma_i}h_jw*{ij}}$$

THe visible state $$v_i$$ is now approximated by the bias term $$b_i$$ and scaled with $$\sigma_i$$.
The first term in the expresssion is a parabola with minimum at $$b_i$$ while the last term depends on $$v_i$$ linearly.

It is hard to train these to obtain tight variance bounds because the top-down (from hidden to visible) effects are multiplied with $$\sigma_i$$ while the bottom up effects are divide by $$\sigma_i$$. 
As a result, the hidden units get saturated and either turn "on" or "off" all the time.
Can avoid this effect by employing lot more hidden units than visible units which multiplies the top-down effect.

This idea is applied with a clever technique of *Stepped Sigmoid Units*.
In a stepped sigmoid unit, we have various copies of a stochastic binary unit.
All copies have same weight and same adaptive bias, *b*, but they have different fixed offsets to the bias: *b-0.5, b-1.5, b-2.5 ...*

![Stepped sigmoid unit](/assets/images/nn-notes/stepped-sigmoid.png)

As *x* increases, the number of units that gets activated increases.
Which means as the variance of the visible units decreases, it increases the bottom-up values leading to more number of hidden units being activated.
Since there are more number of activated hidden units, top-down reconstruction is taken care of.

**Approximation**

$$\langle y \rangle = \sum_{n=1}^{n=\infty}{\sigma(x+0.5-n)} \approx log(1+e^x) \approx max(0, x+noise)$$

There comes the rectified linear units.

## Equivalence of RBM and sigmoid belief net ##
> An RBM is actually just an infinitely deep sigmoid belief net with a lot of weight sharing. 

![Infinite SBN](/assets/images/nn-notes/infinite-sbn.png)

The weights from any hidden to visible layer is W.
The effect of "explaining away" is taken care of by inifinite sigmoid net because when we compute bottom up value of hidden state then we are multiplying with the likelihood term (the weight matrix) and the prior term (complementary prior) itself.

The inifinite SBN is learned by first sharing all the weights which is equivalent to the case of learning in RBM.
Once the weight between the last two layers is learned, they are fixed and the aggregate posterior distribution over h0 is used as data for the new layer of RBM.
Although, because of weights learned can differ from the frozen weights, the gain from better approximation of aggregated posterior distribution outweights the loss due to less accurate inference.

In this section, it is explained why CDn works

# Week 15 (Auto-encoders and semantic hashing -- Modeling hierarchical structure with neural nets)#

## From PCA to Autoencoder ##
By finding how we can do PCA with neural networks, we can represent non-linear manifolds.

We can use backprop to implement PCA inefficiently.
If we have a input layer that maps to a code which then maps to an output layer, such that the input and output layer of the same size is a simple auto-encoder.
If we the input to hidden and hidden to output layers are linear, then the network minimizes the squared reconstruction error exactly like PCA.
The M hidden units will span the same space as the first M components found by PCA.
  * Their M vectors may not be orthogonal
  * They will tend to have equal variance  
That is, when the data points in XYZ are scattered on an XY plane, then the M vectors found be the encoder are in the XY plane only, but not exactly the same.

**Deep Auto-encoders** Now, we can switch the input to hidden and hidden to output to non-linear and hope to do better than PCA.
This is more appealing because the training time in linear in number of training cases and during the test time all that needs to be done is matrix multiplication(s) to find the code.
They happened to be hard to train because the gradients tend to die with small initialization weights.

They can be either repaired by careful initialization (like Echo State Networks) or pre-training.
They were shown to do well with stacked RBM layers from input to code and code to output such that the weights from code to outputs is the transpose of the input.

Deep auto-encoders on documents do better than Latent Semantic Analysis (LSA).
LSA implements PCA over word vectors.
It is shown that Deep auto-encoders with 10D code do better than LSA with 50D. 
There are a few tricks that went into the training of the auto-encoder:
  * Since the word count vectors are not real value unlike images, they are normalized by dividing with N, the total number of words in the document. The vector then behaves like a probability vector, probability of picking the word from the document.
  * The input to hidden weights in the RBM are multiplied by N, since N samples of the distribution (not really sure what that means!). Nothing like that for hidden to output, though.
  
## Semantic Hashing ##
Arrange stuff like in a super-market. 
Things close by are more similar.
The entities are hashed to a value that puts them closer to other similar.
The difference from auto-encoders is that the code is a binary vector unlike real values in the other case.

Architecture to generate this is very similar to auto-encoders except that the code units are now logistic units.
Since the logistic units can also generate values between 0 and 1, we add noise to the inputs to the code units to force the logistic units to either go to 1 or 0, since it cannot rely on between values when the input is noisy.
Instead, it is possible to just use stochastic binary units which activates stochastically based on the input from logistic unit and while backprop, we pretend that we have passed the real values of the logistic unit so that we get smooth gradients.

In the case of image retrival where we may not be able to encode all the information in the short code, we use a smaller code for semantic hashing which gives a list of "good" set of candidate images and can then use sequential search for most closest one with larger code.

In the case of images, a clever trick is employed to make the network focus more on the objects in image rather than the pixels in itself.
For example, we want the nertwork to consider several images of elephant in different poses to be the same although the pixel intensity distributions can be quite different.
For this reason, a model like AlexNet is used and the hidden activities in the last layer are what is encoded.

## Using Auto-encoders for Pre-training ##
The auto-encoders when trained on unlabeled data can discover a code which is basically a collection of good features over the data.
It is possible to use these features further in discriminative training. 
Especially when the labeled data is limited, it is possible to find the features with auto-encoder which can then be reweighted using the limited training data.

RBMs can be seen as autoencoders because they minimize the distance between the input and the reconstruction (with CD1 learning).
If we train RBM with ML, if the noise is fed to an autoencoder, it will try to reproduce the noise, whereas RBM ignores the input and learns only the bias param.
However, it is limited because the hidden states can only be binary.

Denoising and Contractive encoders do better job at pre-training.

**Denoising autoencoder** adds noise to the input by setting many of its c0omponents zero (like dropout on inputs). 
That way, the network is forced to reconstruct the input and hence forced to learn the correlations between the components.
This avoids the risk that a hidden state can just copy the value of one of its component.

**Contractive encoder** presents an alternative of penalizing the squared gradient of each hidden activity wrt the inputs.
The codes tend to have the property that only a small subset of teh hidden units are sensitive to the changes in input.

**Pre-training was the first good way to initialize the weights for deep nets, but now there are better ways**


# Good questions from final quiz #
  
  * The contrastive divergence (CD 10) has advantage over CD1 that the reconstruction generated by CD10 are closer to the model distribution than in the case of CD1. 
    In the case of CD1, it is easy to compute the gradients and the computed gradients have low variance.
  * In an RBM, there are no connections between hidden states and between visible states, the hidden states are independent given the visible states.
	SBNs on the other hand suffer from the explainining away property because the posterior distributons are no longer independent given the visible states.
  * The sole purpose of momentum is to speed up learning, but not to provide any qualitative improvements.
